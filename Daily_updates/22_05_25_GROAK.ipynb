{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc32b01-dd24-479d-8d20-0734fc7bea4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5cf141-234d-4c80-aa8d-7a4218ae6bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Searching for images in C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Found 4793 images across 5 classes.\n",
      "Classes: ['Hatchback', 'Other', 'Pickup', 'Seden', 'SUV']\n",
      "Epoch 1/3000, Beta: 0.00, Total Loss: 1234.7925, Recon Loss: 2469.1533, KL Loss: 51425368689866216.0000, Percep Loss: 0.2159\n",
      "Epoch 2/3000, Beta: 0.01, Total Loss: 248366.8210, Recon Loss: 678.2645, KL Loss: 24802751.1481, Percep Loss: 0.1792\n",
      "Epoch 3/3000, Beta: 0.02, Total Loss: 199.4490, Recon Loss: 396.2839, KL Loss: 57.2738, Percep Loss: 0.1616\n",
      "Epoch 4/3000, Beta: 0.03, Total Loss: 138.1492, Recon Loss: 273.6968, KL Loss: 38.4055, Percep Loss: 0.1487\n",
      "Epoch 5/3000, Beta: 0.04, Total Loss: 115.7033, Recon Loss: 228.4818, KL Loss: 33.0613, Percep Loss: 0.1399\n",
      "Epoch 6/3000, Beta: 0.05, Total Loss: 89.1474, Recon Loss: 175.5803, KL Loss: 24.6396, Percep Loss: 0.1253\n",
      "Epoch 7/3000, Beta: 0.06, Total Loss: 78.5140, Recon Loss: 155.2085, KL Loss: 13.1766, Percep Loss: 0.1192\n",
      "Epoch 8/3000, Beta: 0.07, Total Loss: 66.9028, Recon Loss: 131.6792, KL Loss: 13.6409, Percep Loss: 0.1083\n",
      "Epoch 9/3000, Beta: 0.08, Total Loss: 60.0648, Recon Loss: 116.5050, KL Loss: 21.3236, Percep Loss: 0.1064\n",
      "Epoch 10/3000, Beta: 0.09, Total Loss: 50.8789, Recon Loss: 99.7998, KL Loss: 9.8752, Percep Loss: 0.0902\n",
      "Epoch 11/3000, Beta: 0.10, Total Loss: 50.5444, Recon Loss: 99.6104, KL Loss: 6.4712, Percep Loss: 0.0921\n",
      "Epoch 12/3000, Beta: 0.11, Total Loss: 49.7759, Recon Loss: 90.6498, KL Loss: 39.6610, Percep Loss: 0.0883\n",
      "Epoch 13/3000, Beta: 0.12, Total Loss: 41.0891, Recon Loss: 81.1146, KL Loss: 3.7692, Percep Loss: 0.0795\n",
      "Epoch 14/3000, Beta: 0.13, Total Loss: 38.8359, Recon Loss: 76.5206, KL Loss: 3.8342, Percep Loss: 0.0771\n",
      "Epoch 15/3000, Beta: 0.14, Total Loss: 36.3163, Recon Loss: 70.6367, KL Loss: 6.6235, Percep Loss: 0.0706\n",
      "Epoch 16/3000, Beta: 0.15, Total Loss: 35.3912, Recon Loss: 69.4824, KL Loss: 3.9032, Percep Loss: 0.0645\n",
      "Epoch 17/3000, Beta: 0.16, Total Loss: 32.5729, Recon Loss: 63.9602, KL Loss: 3.2695, Percep Loss: 0.0696\n",
      "Epoch 18/3000, Beta: 0.17, Total Loss: 29.8732, Recon Loss: 55.6937, KL Loss: 11.5706, Percep Loss: 0.0594\n",
      "Epoch 19/3000, Beta: 0.18, Total Loss: 28.0185, Recon Loss: 53.7342, KL Loss: 6.0708, Percep Loss: 0.0587\n",
      "Epoch 20/3000, Beta: 0.19, Total Loss: 26.7058, Recon Loss: 52.3733, KL Loss: 2.4435, Percep Loss: 0.0549\n",
      "Epoch 21/3000, Beta: 0.20, Total Loss: 27.7007, Recon Loss: 54.3145, KL Loss: 2.4542, Percep Loss: 0.0526\n",
      "Epoch 22/3000, Beta: 0.21, Total Loss: 25.7506, Recon Loss: 50.5685, KL Loss: 1.9827, Percep Loss: 0.0500\n",
      "Epoch 23/3000, Beta: 0.22, Total Loss: 23.6876, Recon Loss: 46.5138, KL Loss: 1.7307, Percep Loss: 0.0500\n",
      "Epoch 24/3000, Beta: 0.23, Total Loss: 23.5958, Recon Loss: 46.0337, KL Loss: 2.3103, Percep Loss: 0.0476\n",
      "Epoch 25/3000, Beta: 0.24, Total Loss: 25.8489, Recon Loss: 50.6950, KL Loss: 1.8834, Percep Loss: 0.0494\n",
      "Epoch 26/3000, Beta: 0.25, Total Loss: 23.0945, Recon Loss: 44.5359, KL Loss: 3.1345, Percep Loss: 0.0429\n",
      "Epoch 27/3000, Beta: 0.26, Total Loss: 23.5964, Recon Loss: 45.7898, KL Loss: 2.4880, Percep Loss: 0.0546\n",
      "Epoch 28/3000, Beta: 0.27, Total Loss: 22.5517, Recon Loss: 43.6049, KL Loss: 2.6180, Percep Loss: 0.0424\n",
      "Epoch 29/3000, Beta: 0.28, Total Loss: 20.8679, Recon Loss: 40.5510, KL Loss: 1.9683, Percep Loss: 0.0413\n",
      "Epoch 30/3000, Beta: 0.29, Total Loss: 20.1084, Recon Loss: 39.1375, KL Loss: 1.7036, Percep Loss: 0.0456\n",
      "Epoch 31/3000, Beta: 0.30, Total Loss: 20.4695, Recon Loss: 40.0123, KL Loss: 1.3868, Percep Loss: 0.0473\n",
      "Epoch 32/3000, Beta: 0.31, Total Loss: 20.5098, Recon Loss: 40.1486, KL Loss: 1.2594, Percep Loss: 0.0451\n",
      "Epoch 33/3000, Beta: 0.32, Total Loss: 20.4785, Recon Loss: 39.9912, KL Loss: 1.3685, Percep Loss: 0.0450\n",
      "Epoch 34/3000, Beta: 0.33, Total Loss: 19.4172, Recon Loss: 37.7126, KL Loss: 1.5726, Percep Loss: 0.0419\n",
      "Epoch 35/3000, Beta: 0.34, Total Loss: 18.3909, Recon Loss: 35.4153, KL Loss: 1.8965, Percep Loss: 0.0384\n",
      "Epoch 36/3000, Beta: 0.35, Total Loss: 20.2061, Recon Loss: 39.5104, KL Loss: 1.1860, Percep Loss: 0.0358\n",
      "Epoch 37/3000, Beta: 0.36, Total Loss: 18.6853, Recon Loss: 35.1112, KL Loss: 3.0312, Percep Loss: 0.0384\n",
      "Epoch 38/3000, Beta: 0.37, Total Loss: 19.8792, Recon Loss: 39.0806, KL Loss: 0.8243, Percep Loss: 0.0339\n",
      "Epoch 39/3000, Beta: 0.38, Total Loss: 18.6037, Recon Loss: 34.5247, KL Loss: 3.4335, Percep Loss: 0.0366\n",
      "Epoch 40/3000, Beta: 0.39, Total Loss: 20.3931, Recon Loss: 36.8922, KL Loss: 4.8994, Percep Loss: 0.0362\n",
      "Epoch 41/3000, Beta: 0.40, Total Loss: 17.0685, Recon Loss: 33.0894, KL Loss: 1.2287, Percep Loss: 0.0323\n",
      "Epoch 42/3000, Beta: 0.41, Total Loss: 21.4365, Recon Loss: 37.0205, KL Loss: 7.0489, Percep Loss: 0.0362\n",
      "Epoch 43/3000, Beta: 0.42, Total Loss: 14.9921, Recon Loss: 29.1437, KL Loss: 0.9333, Percep Loss: 0.0282\n",
      "Epoch 44/3000, Beta: 0.43, Total Loss: 15.9669, Recon Loss: 31.1987, KL Loss: 0.7896, Percep Loss: 0.0280\n",
      "Epoch 45/3000, Beta: 0.44, Total Loss: 15.2296, Recon Loss: 29.8602, KL Loss: 0.6070, Percep Loss: 0.0324\n",
      "Epoch 46/3000, Beta: 0.45, Total Loss: 15.3624, Recon Loss: 30.0438, KL Loss: 0.6908, Percep Loss: 0.0297\n",
      "Epoch 47/3000, Beta: 0.46, Total Loss: 14.1591, Recon Loss: 27.6538, KL Loss: 0.6690, Percep Loss: 0.0244\n",
      "Epoch 48/3000, Beta: 0.47, Total Loss: 15.8517, Recon Loss: 30.9928, KL Loss: 0.6919, Percep Loss: 0.0301\n",
      "Epoch 49/3000, Beta: 0.48, Total Loss: 15.6840, Recon Loss: 30.6909, KL Loss: 0.6459, Percep Loss: 0.0286\n",
      "Epoch 50/3000, Beta: 0.49, Total Loss: 14.6283, Recon Loss: 28.5881, KL Loss: 0.6281, Percep Loss: 0.0265\n",
      "Saved checkpoint at epoch 50 to FL_CVAE\\cvae_vehicle_epoch_50.pth\n",
      "Epoch 50: Generated 500 samples for Class 0 (Hatchback).\n",
      "Epoch 50: Generated 500 samples for Class 1 (Other).\n",
      "Epoch 50: Generated 500 samples for Class 2 (Pickup).\n",
      "Epoch 50: Generated 500 samples for Class 3 (Seden).\n",
      "Epoch 50: Generated 500 samples for Class 4 (SUV).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.models as models\n",
    "from torchvision.models import VGG16_Weights\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import kagglehub\n",
    "from PIL import Image\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.device_count() > 1 else 'cuda')\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 256\n",
    "intermediate_dim = 1024\n",
    "num_classes = 5\n",
    "batch_size = 64\n",
    "epochs = 3000\n",
    "learning_rate = 3e-4\n",
    "image_size = 128\n",
    "channels = 3\n",
    "output_dir = \"FL_CVAE\"\n",
    "beta_max = 1.0\n",
    "annealing_epochs = 100\n",
    "recon_weight = 0.5\n",
    "perceptual_weight = 5.0\n",
    "save_freq = 50\n",
    "weight_decay = 1e-5  # Add weight decay for regularization\n",
    "patience = 50  # Early stopping patience\n",
    "min_delta = 0.001  # Minimum improvement for early stopping\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Download Vehicle Type Image Dataset from Kaggle\n",
    "try:\n",
    "    path = kagglehub.dataset_download(\"sujaykapadnis/vehicle-type-image-dataset\")\n",
    "    print(\"Path to dataset files:\", path)\n",
    "    dataset_path = path\n",
    "except Exception as e:\n",
    "    print(f\"Failed to download dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Define the VehicleTypeDataset class\n",
    "class VehicleTypeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_names = []\n",
    "        self.class_to_idx = {}\n",
    "\n",
    "        print(f\"Searching for images in {root_dir}\")\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            image_files = [f for f in files if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "            if image_files:\n",
    "                class_name = os.path.basename(root)\n",
    "                if class_name not in self.class_to_idx:\n",
    "                    self.class_names.append(class_name)\n",
    "                    self.class_to_idx[class_name] = len(self.class_names) - 1\n",
    "                for img_file in image_files:\n",
    "                    img_path = os.path.join(root, img_file)\n",
    "                    self.images.append(img_path)\n",
    "                    self.labels.append(self.class_to_idx[class_name])\n",
    "\n",
    "        if not self.images:\n",
    "            raise ValueError(\n",
    "                f\"No images found in {root_dir}. \"\n",
    "                \"Expected class folders containing .jpg, .png, or .jpeg images.\"\n",
    "            )\n",
    "\n",
    "        print(f\"Found {len(self.images)} images across {len(self.class_names)} classes.\")\n",
    "        print(f\"Classes: {self.class_names}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = VehicleTypeDataset(root_dir=dataset_path, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the Enhanced Encoder network\n",
    "class EnhancedEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim, intermediate_dim, num_classes):\n",
    "        super(EnhancedEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels + num_classes, 64, kernel_size=4, stride=2, padding=1)  # 64x64x64\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)  # 128x32x32\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)  # 256x16x16\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)  # 512x8x8\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.conv5 = nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1)  # 512x4x4\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        self.conv6 = nn.Conv2d(512, 1024, kernel_size=4, stride=2, padding=1)  # 1024x2x2\n",
    "        self.bn6 = nn.BatchNorm2d(1024)\n",
    "        \n",
    "        self.fc_intermediate = nn.Linear(1024 * 2 * 2, intermediate_dim)\n",
    "        self.bn_intermediate = nn.BatchNorm1d(intermediate_dim)\n",
    "        \n",
    "        self.fc_mean = nn.Linear(intermediate_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(intermediate_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        y = F.one_hot(y, num_classes=num_classes).float()\n",
    "        y = y.unsqueeze(-1).unsqueeze(-1)\n",
    "        y = y.expand(-1, -1, x.size(2), x.size(3))\n",
    "        x_with_y = torch.cat([x, y], dim=1)\n",
    "        \n",
    "        h1 = F.leaky_relu(self.bn1(self.conv1(x_with_y)), negative_slope=0.2)\n",
    "        h2 = F.leaky_relu(self.bn2(self.conv2(h1)), negative_slope=0.2)\n",
    "        h3 = F.leaky_relu(self.bn3(self.conv3(h2)), negative_slope=0.2)\n",
    "        h4 = F.leaky_relu(self.bn4(self.conv4(h3)), negative_slope=0.2)\n",
    "        h5 = F.leaky_relu(self.bn5(self.conv5(h4)), negative_slope=0.2)\n",
    "        h6 = F.leaky_relu(self.bn6(self.conv6(h5)), negative_slope=0.2)\n",
    "        \n",
    "        h = h6.view(h6.size(0), -1)  # Fixed: Use h6.size(0)\n",
    "        h = F.leaky_relu(self.bn_intermediate(self.fc_intermediate(h)), negative_slope=0.2)\n",
    "        \n",
    "        z_mean = self.fc_mean(h)\n",
    "        z_logvar = self.fc_logvar(h)\n",
    "        return z_mean, z_logvar, (h1, h2, h3, h4, h5, h6)\n",
    "\n",
    "# Define the Enhanced Decoder network\n",
    "class EnhancedDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, intermediate_dim, num_classes):\n",
    "        super(EnhancedDecoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim + num_classes, intermediate_dim)\n",
    "        self.bn_fc = nn.BatchNorm1d(intermediate_dim)\n",
    "        self.fc_to_features = nn.Linear(intermediate_dim, 1024 * 2 * 2)\n",
    "        \n",
    "        self.deconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1)  # 512x4x4\n",
    "        self.bn1 = nn.BatchNorm2d(512)\n",
    "        self.deconv2 = nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1)  # 512x8x8\n",
    "        self.bn2 = nn.BatchNorm2d(512)\n",
    "        self.deconv3 = nn.ConvTranspose2d(1024, 256, kernel_size=4, stride=2, padding=1)  # 256x16x16\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.deconv4 = nn.ConvTranspose2d(512, 128, kernel_size=4, stride=2, padding=1)  # 128x32x32\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.deconv5 = nn.ConvTranspose2d(256, 64, kernel_size=4, stride=2, padding=1)  # 64x64x64\n",
    "        self.bn5 = nn.BatchNorm2d(64)\n",
    "        self.deconv6 = nn.ConvTranspose2d(128, channels, kernel_size=4, stride=2, padding=1)  # 3x128x128\n",
    "\n",
    "    def forward(self, z, y, skip_connections):\n",
    "        h1, h2, h3, h4, h5, h6 = skip_connections\n",
    "        y = F.one_hot(y, num_classes=num_classes).float()\n",
    "        z_with_y = torch.cat([z, y], dim=-1)\n",
    "        \n",
    "        h = F.leaky_relu(self.bn_fc(self.fc(z_with_y)), negative_slope=0.2)\n",
    "        h = self.fc_to_features(h)\n",
    "        h = h.view(h.size(0), 1024, 2, 2)\n",
    "        \n",
    "        h = F.leaky_relu(self.bn1(self.deconv1(h)), negative_slope=0.2)\n",
    "        h = torch.cat([h, h5], dim=1)\n",
    "        h = F.leaky_relu(self.bn2(self.deconv2(h)), negative_slope=0.2)\n",
    "        h = torch.cat([h, h4], dim=1)\n",
    "        h = F.leaky_relu(self.bn3(self.deconv3(h)), negative_slope=0.2)\n",
    "        h = torch.cat([h, h3], dim=1)\n",
    "        h = F.leaky_relu(self.bn4(self.deconv4(h)), negative_slope=0.2)\n",
    "        h = torch.cat([h, h2], dim=1)\n",
    "        h = F.leaky_relu(self.bn5(self.deconv5(h)), negative_slope=0.2)\n",
    "        h = torch.cat([h, h1], dim=1)\n",
    "        x_reconstructed = self.deconv6(h)\n",
    "        return torch.clamp(x_reconstructed, 0, 1)\n",
    "\n",
    "# Conditional VAE model\n",
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def reparameterize(self, z_mean, z_logvar):\n",
    "        std = torch.exp(0.5 * z_logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return z_mean + eps * std\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        z_mean, z_logvar, skip_connections = self.encoder(x, y)\n",
    "        z = self.reparameterize(z_mean, z_logvar)\n",
    "        x_reconstructed = self.decoder(z, y, skip_connections)\n",
    "        return x_reconstructed, z_mean, z_logvar\n",
    "\n",
    "# Load pretrained VGG16 for perceptual loss\n",
    "vgg = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features.to(device).eval()\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "def perceptual_loss(x, x_reconstructed):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(x.device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(x.device)\n",
    "    x_normalized = (x - mean) / std\n",
    "    x_reconstructed_normalized = (x_reconstructed - mean) / std\n",
    "    x_features = vgg(x_normalized)\n",
    "    x_recon_features = vgg(x_reconstructed_normalized)\n",
    "    return F.mse_loss(x_features, x_recon_features)\n",
    "\n",
    "# Instantiate the model\n",
    "encoder = EnhancedEncoder(latent_dim, intermediate_dim, num_classes).to(device)\n",
    "decoder = EnhancedDecoder(latent_dim, intermediate_dim, num_classes).to(device)\n",
    "cvae = ConditionalVAE(encoder, decoder).to(device)\n",
    "\n",
    "# Define optimizer and scheduler\n",
    "optimizer = optim.Adam(cvae.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=20, factor=0.5)\n",
    "\n",
    "# Define loss function\n",
    "def cvae_loss(x, x_reconstructed, z_mean, z_logvar, beta=1.0, recon_weight=1.0, perceptual_weight=1.0):\n",
    "    recon_loss = F.mse_loss(x_reconstructed, x, reduction='sum')\n",
    "    kl_loss = -0.5 * torch.sum(1 + z_logvar - z_mean.pow(2) - z_logvar.exp()) + 1e-6  # Add small constant\n",
    "    percep_loss = perceptual_loss(x, x_reconstructed) * perceptual_weight\n",
    "    total_loss = recon_weight * recon_loss + beta * kl_loss + percep_loss\n",
    "    return total_loss, recon_loss, kl_loss, percep_loss\n",
    "\n",
    "# Generate samples for all classes\n",
    "def generate_samples_labelwise(cvae, num_samples, classes_to_generate, base_dir, latent_dim, device, epoch):\n",
    "    cvae.eval()\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    with torch.no_grad():\n",
    "        for class_label in classes_to_generate:\n",
    "            label_tensor = torch.tensor([class_label]).repeat(num_samples).to(device)\n",
    "            z = torch.randn(num_samples, latent_dim).to(device)\n",
    "            dummy_skips = [\n",
    "                torch.randn(num_samples, 64, 64, 64).to(device) * 0.1,\n",
    "                torch.randn(num_samples, 128, 32, 32).to(device) * 0.1,\n",
    "                torch.randn(num_samples, 256, 16, 16).to(device) * 0.1,\n",
    "                torch.randn(num_samples, 512, 8, 8).to(device) * 0.1,\n",
    "                torch.randn(num_samples, 512, 4, 4).to(device) * 0.1,\n",
    "                torch.randn(num_samples, 1024, 2, 2).to(device) * 0.1\n",
    "            ]\n",
    "            generated_samples = cvae.decoder(z, label_tensor, dummy_skips)\n",
    "            \n",
    "            class_dir = os.path.join(base_dir, f\"class_{class_label}\")\n",
    "            os.makedirs(class_dir, exist_ok=True)\n",
    "            for idx, sample in enumerate(generated_samples):\n",
    "                save_image(sample, os.path.join(class_dir, f\"sample_{idx}_epoch_{epoch}.png\"))\n",
    "            print(f\"Epoch {epoch}: Generated {num_samples} samples for Class {class_label} ({dataset.class_names[class_label]}).\")\n",
    "\n",
    "# Plot random samples for all classes\n",
    "def plot_random_samples(base_dir, classes_to_generate, num_images_per_class, epoch):\n",
    "    fig, axs = plt.subplots(len(classes_to_generate), num_images_per_class, figsize=(20, len(classes_to_generate) * 2))\n",
    "    for row, class_label in enumerate(classes_to_generate):\n",
    "        class_dir = os.path.join(base_dir, f\"class_{class_label}\")\n",
    "        sample_files = [f for f in os.listdir(class_dir) if f.endswith(f\"epoch_{epoch}.png\")]\n",
    "        if len(sample_files) < num_images_per_class:\n",
    "            continue\n",
    "        random_samples = np.random.choice(sample_files, num_images_per_class, replace=False)\n",
    "        \n",
    "        for col, sample_file in enumerate(random_samples):\n",
    "            sample_path = os.path.join(class_dir, sample_file)\n",
    "            sample_image = plt.imread(sample_path)\n",
    "            ax = axs[row, col] if len(classes_to_generate) > 1 else axs[col]\n",
    "            ax.imshow(sample_image)\n",
    "            ax.axis('off')\n",
    "            if col == 0:\n",
    "                ax.set_ylabel(dataset.class_names[class_label], rotation=90, labelpad=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(output_dir, f\"synthetic_samples_epoch_{epoch}.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    print(f\"Saved synthetic samples plot for epoch {epoch} to {plot_path}\")\n",
    "\n",
    "# Training loop with early stopping\n",
    "cvae.train()\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "for epoch in range(epochs):\n",
    "    if epoch < annealing_epochs:\n",
    "        beta = beta_max * (epoch / annealing_epochs)\n",
    "    else:\n",
    "        beta = beta_max\n",
    "\n",
    "    train_loss = 0\n",
    "    train_recon_loss = 0\n",
    "    train_kl_loss = 0\n",
    "    train_percep_loss = 0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        x_reconstructed, z_mean, z_logvar = cvae(data, labels)\n",
    "        total_loss, recon_loss, kl_loss, percep_loss = cvae_loss(\n",
    "            data, x_reconstructed, z_mean, z_logvar, \n",
    "            beta=beta, recon_weight=recon_weight, perceptual_weight=perceptual_weight\n",
    "        )\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(cvae.parameters(), max_norm=1.0)\n",
    "        train_loss += total_loss.item()\n",
    "        train_recon_loss += recon_loss.item()\n",
    "        train_kl_loss += kl_loss.item()\n",
    "        train_percep_loss += percep_loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    scheduler.step(avg_loss)\n",
    "    avg_recon_loss = train_recon_loss / len(train_loader.dataset)\n",
    "    avg_kl_loss = train_kl_loss / len(train_loader.dataset)\n",
    "    avg_percep_loss = train_percep_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Beta: {beta:.2f}, Total Loss: {avg_loss:.4f}, '\n",
    "          f'Recon Loss: {avg_recon_loss:.4f}, KL Loss: {avg_kl_loss:.4f}, Percep Loss: {avg_percep_loss:.4f}')\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_loss < best_loss - min_delta:\n",
    "        best_loss = avg_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1} due to no improvement in loss.\")\n",
    "            break\n",
    "\n",
    "    # Save checkpoint and generate samples every 50 epochs\n",
    "    if (epoch + 1) % save_freq == 0:\n",
    "        checkpoint_path = os.path.join(output_dir, f\"cvae_vehicle_epoch_{epoch + 1}.pth\")\n",
    "        torch.save(cvae.state_dict(), checkpoint_path)\n",
    "        print(f\"Saved checkpoint at epoch {epoch + 1} to {checkpoint_path}\")\n",
    "\n",
    "        base_dir = os.path.join(output_dir, f\"generated_samples_epoch_{epoch + 1}\")\n",
    "        classes_to_generate = list(range(num_classes))\n",
    "        generate_samples_labelwise(cvae, num_samples=500, classes_to_generate=classes_to_generate, \n",
    "                                 base_dir=base_dir, latent_dim=latent_dim, device=device, epoch=epoch + 1)\n",
    "        \n",
    "        plot_random_samples(base_dir=base_dir, classes_to_generate=classes_to_generate, \n",
    "                           num_images_per_class=10, epoch=epoch + 1)\n",
    "\n",
    "# Save the final model\n",
    "model_path = os.path.join(output_dir, \"cvae_vehicle_final.pth\")\n",
    "torch.save(cvae.state_dict(), model_path)\n",
    "print(f\"Saved final CVAE model to {model_path}\")\n",
    "\n",
    "# Final generation and plotting\n",
    "base_dir = os.path.join(output_dir, \"generated_samples_final\")\n",
    "generate_samples_labelwise(cvae, num_samples=500, classes_to_generate=classes_to_generate, \n",
    "                         base_dir=base_dir, latent_dim=latent_dim, device=device, epoch=epochs)\n",
    "plot_random_samples(base_dir=base_dir, classes_to_generate=classes_to_generate, \n",
    "                   num_images_per_class=10, epoch=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e553b8c2-e24a-469a-b444-a47163a0a030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Searching for images in C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Found 4793 images across 5 classes.\n",
      "Classes: ['Hatchback', 'Other', 'Pickup', 'Seden', 'SUV']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_25932\\3951705115.py:227: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cvae.load_state_dict(torch.load(checkpoint_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from FL_CVAE\\cvae_vehicle_epoch_50.pth\n",
      "Epoch 51/3000, Beta: 0.50, Total Loss: 20.6018, Recon Loss: 39.8786, KL Loss: 1.2381, Percep Loss: 0.0434\n",
      "Epoch 52/3000, Beta: 0.51, Total Loss: 19.7890, Recon Loss: 36.2003, KL Loss: 3.2417, Percep Loss: 0.0356\n",
      "Epoch 53/3000, Beta: 0.52, Total Loss: 16.5824, Recon Loss: 31.5419, KL Loss: 1.4915, Percep Loss: 0.0358\n",
      "Epoch 54/3000, Beta: 0.53, Total Loss: 14.6445, Recon Loss: 28.5190, KL Loss: 0.6714, Percep Loss: 0.0292\n",
      "Epoch 55/3000, Beta: 0.54, Total Loss: 19.5544, Recon Loss: 36.0875, KL Loss: 2.7309, Percep Loss: 0.0360\n",
      "Epoch 56/3000, Beta: 0.55, Total Loss: 16.9137, Recon Loss: 32.2648, KL Loss: 1.3578, Percep Loss: 0.0345\n",
      "Epoch 57/3000, Beta: 0.56, Total Loss: 13.5545, Recon Loss: 26.3678, KL Loss: 0.6119, Percep Loss: 0.0279\n",
      "Epoch 58/3000, Beta: 0.57, Total Loss: 17.3054, Recon Loss: 31.8745, KL Loss: 2.3420, Percep Loss: 0.0332\n",
      "Epoch 59/3000, Beta: 0.58, Total Loss: 14.9173, Recon Loss: 29.1466, KL Loss: 0.5508, Percep Loss: 0.0246\n",
      "Epoch 60/3000, Beta: 0.59, Total Loss: 13.1450, Recon Loss: 25.8404, KL Loss: 0.3498, Percep Loss: 0.0184\n",
      "Epoch 61/3000, Beta: 0.60, Total Loss: 13.3876, Recon Loss: 26.2200, KL Loss: 0.4230, Percep Loss: 0.0239\n",
      "Epoch 62/3000, Beta: 0.61, Total Loss: 14.0277, Recon Loss: 27.2124, KL Loss: 0.6517, Percep Loss: 0.0240\n",
      "Epoch 63/3000, Beta: 0.62, Total Loss: 13.0645, Recon Loss: 25.4319, KL Loss: 0.5259, Percep Loss: 0.0225\n",
      "Epoch 64/3000, Beta: 0.63, Total Loss: 13.5210, Recon Loss: 26.6596, KL Loss: 0.2573, Percep Loss: 0.0291\n",
      "Epoch 65/3000, Beta: 0.64, Total Loss: 12.2108, Recon Loss: 23.8234, KL Loss: 0.4349, Percep Loss: 0.0208\n",
      "Epoch 66/3000, Beta: 0.65, Total Loss: 13.9580, Recon Loss: 27.4312, KL Loss: 0.3338, Percep Loss: 0.0254\n",
      "Epoch 67/3000, Beta: 0.66, Total Loss: 11.8644, Recon Loss: 23.1046, KL Loss: 0.4444, Percep Loss: 0.0188\n",
      "Epoch 68/3000, Beta: 0.67, Total Loss: 12.9254, Recon Loss: 25.3612, KL Loss: 0.3304, Percep Loss: 0.0234\n",
      "Epoch 69/3000, Beta: 0.68, Total Loss: 13.1957, Recon Loss: 25.9659, KL Loss: 0.2800, Percep Loss: 0.0224\n",
      "Epoch 70/3000, Beta: 0.69, Total Loss: 12.3732, Recon Loss: 23.5314, KL Loss: 0.8442, Percep Loss: 0.0251\n",
      "Epoch 71/3000, Beta: 0.70, Total Loss: 12.8072, Recon Loss: 24.7620, KL Loss: 0.5801, Percep Loss: 0.0202\n",
      "Epoch 72/3000, Beta: 0.71, Total Loss: 12.0574, Recon Loss: 23.7633, KL Loss: 0.2188, Percep Loss: 0.0204\n",
      "Epoch 73/3000, Beta: 0.72, Total Loss: 14.1509, Recon Loss: 27.4272, KL Loss: 0.5791, Percep Loss: 0.0203\n",
      "Epoch 74/3000, Beta: 0.73, Total Loss: 13.7295, Recon Loss: 26.6300, KL Loss: 0.5393, Percep Loss: 0.0208\n",
      "Epoch 75/3000, Beta: 0.74, Total Loss: 13.9434, Recon Loss: 27.1694, KL Loss: 0.4547, Percep Loss: 0.0222\n",
      "Epoch 76/3000, Beta: 0.75, Total Loss: 12.5705, Recon Loss: 24.8085, KL Loss: 0.1967, Percep Loss: 0.0187\n",
      "Epoch 77/3000, Beta: 0.76, Total Loss: 12.1726, Recon Loss: 23.4185, KL Loss: 0.5827, Percep Loss: 0.0204\n",
      "Epoch 78/3000, Beta: 0.77, Total Loss: 11.6898, Recon Loss: 22.6840, KL Loss: 0.4294, Percep Loss: 0.0172\n",
      "Epoch 79/3000, Beta: 0.78, Total Loss: 13.3624, Recon Loss: 25.7321, KL Loss: 0.6126, Percep Loss: 0.0185\n",
      "Epoch 80/3000, Beta: 0.79, Total Loss: 13.4873, Recon Loss: 26.5342, KL Loss: 0.2589, Percep Loss: 0.0157\n",
      "Epoch 81/3000, Beta: 0.80, Total Loss: 11.9702, Recon Loss: 23.0312, KL Loss: 0.5458, Percep Loss: 0.0179\n",
      "Epoch 82/3000, Beta: 0.81, Total Loss: 14.9493, Recon Loss: 28.3855, KL Loss: 0.9067, Percep Loss: 0.0221\n",
      "Epoch 83/3000, Beta: 0.82, Total Loss: 11.8466, Recon Loss: 23.2575, KL Loss: 0.2466, Percep Loss: 0.0156\n",
      "Epoch 84/3000, Beta: 0.83, Total Loss: 11.4267, Recon Loss: 22.5410, KL Loss: 0.1656, Percep Loss: 0.0188\n",
      "Epoch 85/3000, Beta: 0.84, Total Loss: 12.5891, Recon Loss: 24.6321, KL Loss: 0.3040, Percep Loss: 0.0177\n",
      "Epoch 86/3000, Beta: 0.85, Total Loss: 13.7962, Recon Loss: 26.6348, KL Loss: 0.5319, Percep Loss: 0.0266\n",
      "Epoch 87/3000, Beta: 0.86, Total Loss: 14.3764, Recon Loss: 27.8938, KL Loss: 0.4744, Percep Loss: 0.0216\n",
      "Epoch 88/3000, Beta: 0.87, Total Loss: 11.9296, Recon Loss: 23.4052, KL Loss: 0.2402, Percep Loss: 0.0180\n",
      "Epoch 89/3000, Beta: 0.88, Total Loss: 12.3013, Recon Loss: 24.1316, KL Loss: 0.2487, Percep Loss: 0.0167\n",
      "Epoch 90/3000, Beta: 0.89, Total Loss: 12.2474, Recon Loss: 24.0788, KL Loss: 0.2125, Percep Loss: 0.0188\n",
      "Epoch 91/3000, Beta: 0.90, Total Loss: 11.9123, Recon Loss: 23.4067, KL Loss: 0.2104, Percep Loss: 0.0196\n",
      "Epoch 92/3000, Beta: 0.91, Total Loss: 12.4164, Recon Loss: 24.5674, KL Loss: 0.1287, Percep Loss: 0.0156\n",
      "Epoch 93/3000, Beta: 0.92, Total Loss: 12.1676, Recon Loss: 23.9215, KL Loss: 0.2058, Percep Loss: 0.0175\n",
      "Epoch 94/3000, Beta: 0.93, Total Loss: 13.8001, Recon Loss: 26.8162, KL Loss: 0.3966, Percep Loss: 0.0232\n",
      "Epoch 95/3000, Beta: 0.94, Total Loss: 12.1410, Recon Loss: 23.8231, KL Loss: 0.2239, Percep Loss: 0.0190\n",
      "Epoch 96/3000, Beta: 0.95, Total Loss: 11.4909, Recon Loss: 22.4606, KL Loss: 0.2566, Percep Loss: 0.0169\n",
      "Epoch 97/3000, Beta: 0.96, Total Loss: 12.4340, Recon Loss: 24.3753, KL Loss: 0.2401, Percep Loss: 0.0159\n",
      "Epoch 98/3000, Beta: 0.97, Total Loss: 11.7339, Recon Loss: 23.1404, KL Loss: 0.1493, Percep Loss: 0.0189\n",
      "Epoch 99/3000, Beta: 0.98, Total Loss: 12.2777, Recon Loss: 23.9078, KL Loss: 0.3112, Percep Loss: 0.0189\n",
      "Epoch 100/3000, Beta: 0.99, Total Loss: 10.4708, Recon Loss: 20.3454, KL Loss: 0.2857, Percep Loss: 0.0153\n",
      "Epoch 101/3000, Beta: 1.00, Total Loss: 10.1760, Recon Loss: 20.0117, KL Loss: 0.1538, Percep Loss: 0.0164\n",
      "Epoch 102/3000, Beta: 1.00, Total Loss: 11.9584, Recon Loss: 23.6445, KL Loss: 0.1192, Percep Loss: 0.0169\n",
      "Epoch 103/3000, Beta: 1.00, Total Loss: 10.7874, Recon Loss: 21.1648, KL Loss: 0.1896, Percep Loss: 0.0154\n",
      "Epoch 104/3000, Beta: 1.00, Total Loss: 12.3556, Recon Loss: 24.2466, KL Loss: 0.2186, Percep Loss: 0.0137\n",
      "Epoch 105/3000, Beta: 1.00, Total Loss: 12.7323, Recon Loss: 22.6532, KL Loss: 1.3893, Percep Loss: 0.0164\n",
      "Epoch 106/3000, Beta: 1.00, Total Loss: 11.1683, Recon Loss: 21.5086, KL Loss: 0.3978, Percep Loss: 0.0162\n",
      "Epoch 107/3000, Beta: 1.00, Total Loss: 13.6018, Recon Loss: 26.7790, KL Loss: 0.1928, Percep Loss: 0.0194\n",
      "Epoch 108/3000, Beta: 1.00, Total Loss: 11.0292, Recon Loss: 21.6102, KL Loss: 0.2090, Percep Loss: 0.0151\n",
      "Epoch 109/3000, Beta: 1.00, Total Loss: 11.4761, Recon Loss: 22.3291, KL Loss: 0.2920, Percep Loss: 0.0196\n",
      "Epoch 110/3000, Beta: 1.00, Total Loss: 10.5389, Recon Loss: 20.8657, KL Loss: 0.0907, Percep Loss: 0.0154\n",
      "Epoch 111/3000, Beta: 1.00, Total Loss: 10.0288, Recon Loss: 19.7608, KL Loss: 0.1353, Percep Loss: 0.0131\n",
      "Epoch 112/3000, Beta: 1.00, Total Loss: 11.3302, Recon Loss: 22.1475, KL Loss: 0.2435, Percep Loss: 0.0129\n",
      "Epoch 113/3000, Beta: 1.00, Total Loss: 9.5437, Recon Loss: 18.7049, KL Loss: 0.1770, Percep Loss: 0.0143\n",
      "Epoch 114/3000, Beta: 1.00, Total Loss: 12.7491, Recon Loss: 25.2478, KL Loss: 0.1081, Percep Loss: 0.0171\n",
      "Epoch 115/3000, Beta: 1.00, Total Loss: 9.2775, Recon Loss: 18.2503, KL Loss: 0.1372, Percep Loss: 0.0152\n",
      "Epoch 116/3000, Beta: 1.00, Total Loss: 11.2419, Recon Loss: 22.0972, KL Loss: 0.1790, Percep Loss: 0.0143\n",
      "Epoch 117/3000, Beta: 1.00, Total Loss: 10.4246, Recon Loss: 20.5876, KL Loss: 0.1178, Percep Loss: 0.0129\n",
      "Epoch 118/3000, Beta: 1.00, Total Loss: 11.0439, Recon Loss: 21.8992, KL Loss: 0.0807, Percep Loss: 0.0135\n",
      "Epoch 119/3000, Beta: 1.00, Total Loss: 11.0556, Recon Loss: 21.8890, KL Loss: 0.0978, Percep Loss: 0.0133\n",
      "Epoch 120/3000, Beta: 1.00, Total Loss: 10.9535, Recon Loss: 21.5285, KL Loss: 0.1767, Percep Loss: 0.0126\n",
      "Epoch 121/3000, Beta: 1.00, Total Loss: 11.1601, Recon Loss: 22.0738, KL Loss: 0.1085, Percep Loss: 0.0147\n",
      "Epoch 122/3000, Beta: 1.00, Total Loss: 10.9149, Recon Loss: 21.5302, KL Loss: 0.1355, Percep Loss: 0.0143\n",
      "Epoch 123/3000, Beta: 1.00, Total Loss: 11.4534, Recon Loss: 20.9199, KL Loss: 0.9773, Percep Loss: 0.0162\n",
      "Epoch 124/3000, Beta: 1.00, Total Loss: 10.8537, Recon Loss: 21.3206, KL Loss: 0.1808, Percep Loss: 0.0125\n",
      "Epoch 125/3000, Beta: 1.00, Total Loss: 12.2242, Recon Loss: 24.1542, KL Loss: 0.1351, Percep Loss: 0.0119\n",
      "Epoch 126/3000, Beta: 1.00, Total Loss: 12.1655, Recon Loss: 23.6328, KL Loss: 0.3364, Percep Loss: 0.0127\n",
      "Epoch 127/3000, Beta: 1.00, Total Loss: 9.6759, Recon Loss: 18.7540, KL Loss: 0.2873, Percep Loss: 0.0116\n",
      "Epoch 128/3000, Beta: 1.00, Total Loss: 10.5808, Recon Loss: 20.9191, KL Loss: 0.1108, Percep Loss: 0.0105\n",
      "Epoch 129/3000, Beta: 1.00, Total Loss: 12.1637, Recon Loss: 23.2715, KL Loss: 0.5114, Percep Loss: 0.0165\n",
      "Epoch 130/3000, Beta: 1.00, Total Loss: 10.8496, Recon Loss: 21.4352, KL Loss: 0.1199, Percep Loss: 0.0122\n",
      "Epoch 131/3000, Beta: 1.00, Total Loss: 10.9418, Recon Loss: 21.6180, KL Loss: 0.1197, Percep Loss: 0.0131\n",
      "Epoch 132/3000, Beta: 1.00, Total Loss: 10.3412, Recon Loss: 20.5255, KL Loss: 0.0620, Percep Loss: 0.0165\n",
      "Epoch 133/3000, Beta: 1.00, Total Loss: 9.9954, Recon Loss: 19.7960, KL Loss: 0.0799, Percep Loss: 0.0176\n",
      "Epoch 134/3000, Beta: 1.00, Total Loss: 9.3424, Recon Loss: 18.4151, KL Loss: 0.1222, Percep Loss: 0.0126\n",
      "Epoch 135/3000, Beta: 1.00, Total Loss: 9.3991, Recon Loss: 18.4537, KL Loss: 0.1593, Percep Loss: 0.0130\n",
      "Epoch 136/3000, Beta: 1.00, Total Loss: 9.8869, Recon Loss: 19.5759, KL Loss: 0.0871, Percep Loss: 0.0118\n",
      "Epoch 137/3000, Beta: 1.00, Total Loss: 7.7752, Recon Loss: 15.3750, KL Loss: 0.0799, Percep Loss: 0.0077\n",
      "Epoch 138/3000, Beta: 1.00, Total Loss: 7.2280, Recon Loss: 14.3729, KL Loss: 0.0354, Percep Loss: 0.0062\n",
      "Epoch 139/3000, Beta: 1.00, Total Loss: 8.4046, Recon Loss: 16.6902, KL Loss: 0.0516, Percep Loss: 0.0079\n",
      "Epoch 140/3000, Beta: 1.00, Total Loss: 12.7867, Recon Loss: 24.5637, KL Loss: 0.4940, Percep Loss: 0.0109\n",
      "Epoch 141/3000, Beta: 1.00, Total Loss: 7.3444, Recon Loss: 14.6066, KL Loss: 0.0338, Percep Loss: 0.0073\n",
      "Epoch 142/3000, Beta: 1.00, Total Loss: 8.6322, Recon Loss: 17.1477, KL Loss: 0.0510, Percep Loss: 0.0073\n",
      "Epoch 143/3000, Beta: 1.00, Total Loss: 7.0827, Recon Loss: 14.0185, KL Loss: 0.0663, Percep Loss: 0.0071\n",
      "Epoch 144/3000, Beta: 1.00, Total Loss: 6.4911, Recon Loss: 12.8865, KL Loss: 0.0416, Percep Loss: 0.0063\n",
      "Epoch 145/3000, Beta: 1.00, Total Loss: 7.3088, Recon Loss: 14.5320, KL Loss: 0.0365, Percep Loss: 0.0062\n",
      "Epoch 146/3000, Beta: 1.00, Total Loss: 7.3381, Recon Loss: 14.5292, KL Loss: 0.0669, Percep Loss: 0.0066\n",
      "Epoch 147/3000, Beta: 1.00, Total Loss: 8.3373, Recon Loss: 16.5801, KL Loss: 0.0415, Percep Loss: 0.0058\n",
      "Epoch 148/3000, Beta: 1.00, Total Loss: 7.2353, Recon Loss: 14.3950, KL Loss: 0.0312, Percep Loss: 0.0065\n",
      "Epoch 149/3000, Beta: 1.00, Total Loss: 6.8258, Recon Loss: 13.5739, KL Loss: 0.0328, Percep Loss: 0.0060\n",
      "Epoch 150/3000, Beta: 1.00, Total Loss: 8.2000, Recon Loss: 15.9895, KL Loss: 0.1989, Percep Loss: 0.0064\n",
      "Epoch 151/3000, Beta: 1.00, Total Loss: 8.0199, Recon Loss: 15.8579, KL Loss: 0.0852, Percep Loss: 0.0058\n",
      "Epoch 152/3000, Beta: 1.00, Total Loss: 7.9993, Recon Loss: 15.8837, KL Loss: 0.0496, Percep Loss: 0.0079\n",
      "Epoch 153/3000, Beta: 1.00, Total Loss: 10.0465, Recon Loss: 19.1305, KL Loss: 0.4715, Percep Loss: 0.0097\n",
      "Epoch 154/3000, Beta: 1.00, Total Loss: 9.2127, Recon Loss: 18.2711, KL Loss: 0.0691, Percep Loss: 0.0080\n",
      "Epoch 155/3000, Beta: 1.00, Total Loss: 7.4397, Recon Loss: 14.8153, KL Loss: 0.0251, Percep Loss: 0.0070\n",
      "Epoch 156/3000, Beta: 1.00, Total Loss: 6.9194, Recon Loss: 13.8080, KL Loss: 0.0099, Percep Loss: 0.0056\n",
      "Epoch 157/3000, Beta: 1.00, Total Loss: 8.4838, Recon Loss: 16.9180, KL Loss: 0.0178, Percep Loss: 0.0070\n",
      "Epoch 158/3000, Beta: 1.00, Total Loss: 8.4107, Recon Loss: 16.2540, KL Loss: 0.2744, Percep Loss: 0.0093\n",
      "Epoch 159/3000, Beta: 1.00, Total Loss: 7.0680, Recon Loss: 14.0657, KL Loss: 0.0295, Percep Loss: 0.0057\n",
      "Epoch 160/3000, Beta: 1.00, Total Loss: 7.2482, Recon Loss: 14.4169, KL Loss: 0.0343, Percep Loss: 0.0054\n",
      "Epoch 161/3000, Beta: 1.00, Total Loss: 7.1131, Recon Loss: 14.1532, KL Loss: 0.0307, Percep Loss: 0.0058\n",
      "Epoch 162/3000, Beta: 1.00, Total Loss: 7.4694, Recon Loss: 14.8836, KL Loss: 0.0210, Percep Loss: 0.0065\n",
      "Epoch 163/3000, Beta: 1.00, Total Loss: 7.7355, Recon Loss: 15.4156, KL Loss: 0.0221, Percep Loss: 0.0056\n",
      "Epoch 164/3000, Beta: 1.00, Total Loss: 6.8808, Recon Loss: 13.6786, KL Loss: 0.0354, Percep Loss: 0.0060\n",
      "Epoch 165/3000, Beta: 1.00, Total Loss: 7.2304, Recon Loss: 13.9261, KL Loss: 0.2607, Percep Loss: 0.0067\n",
      "Epoch 166/3000, Beta: 1.00, Total Loss: 7.4232, Recon Loss: 14.7455, KL Loss: 0.0456, Percep Loss: 0.0049\n",
      "Epoch 167/3000, Beta: 1.00, Total Loss: 5.7936, Recon Loss: 11.5264, KL Loss: 0.0265, Percep Loss: 0.0039\n",
      "Epoch 168/3000, Beta: 1.00, Total Loss: 5.8367, Recon Loss: 11.6352, KL Loss: 0.0140, Percep Loss: 0.0051\n",
      "Epoch 169/3000, Beta: 1.00, Total Loss: 5.4174, Recon Loss: 10.8083, KL Loss: 0.0089, Percep Loss: 0.0043\n",
      "Epoch 170/3000, Beta: 1.00, Total Loss: 6.4329, Recon Loss: 12.8393, KL Loss: 0.0093, Percep Loss: 0.0040\n",
      "Epoch 171/3000, Beta: 1.00, Total Loss: 6.0277, Recon Loss: 12.0352, KL Loss: 0.0063, Percep Loss: 0.0038\n",
      "Epoch 172/3000, Beta: 1.00, Total Loss: 6.4140, Recon Loss: 12.8077, KL Loss: 0.0058, Percep Loss: 0.0043\n",
      "Epoch 173/3000, Beta: 1.00, Total Loss: 6.3968, Recon Loss: 12.7623, KL Loss: 0.0111, Percep Loss: 0.0046\n",
      "Epoch 174/3000, Beta: 1.00, Total Loss: 7.7532, Recon Loss: 15.4630, KL Loss: 0.0169, Percep Loss: 0.0047\n",
      "Epoch 175/3000, Beta: 1.00, Total Loss: 6.1038, Recon Loss: 12.1841, KL Loss: 0.0080, Percep Loss: 0.0037\n",
      "Epoch 176/3000, Beta: 1.00, Total Loss: 6.4699, Recon Loss: 12.9094, KL Loss: 0.0110, Percep Loss: 0.0042\n",
      "Epoch 177/3000, Beta: 1.00, Total Loss: 6.4728, Recon Loss: 12.9134, KL Loss: 0.0112, Percep Loss: 0.0049\n",
      "Epoch 178/3000, Beta: 1.00, Total Loss: 5.8841, Recon Loss: 11.7338, KL Loss: 0.0135, Percep Loss: 0.0036\n",
      "Epoch 179/3000, Beta: 1.00, Total Loss: 5.1199, Recon Loss: 10.2197, KL Loss: 0.0065, Percep Loss: 0.0036\n",
      "Epoch 180/3000, Beta: 1.00, Total Loss: 6.0084, Recon Loss: 11.9985, KL Loss: 0.0053, Percep Loss: 0.0038\n",
      "Epoch 181/3000, Beta: 1.00, Total Loss: 6.1834, Recon Loss: 12.3483, KL Loss: 0.0053, Percep Loss: 0.0039\n",
      "Epoch 182/3000, Beta: 1.00, Total Loss: 6.5727, Recon Loss: 13.1212, KL Loss: 0.0089, Percep Loss: 0.0032\n",
      "Epoch 183/3000, Beta: 1.00, Total Loss: 6.1531, Recon Loss: 12.2792, KL Loss: 0.0103, Percep Loss: 0.0032\n",
      "Epoch 184/3000, Beta: 1.00, Total Loss: 6.2469, Recon Loss: 12.4549, KL Loss: 0.0156, Percep Loss: 0.0039\n",
      "Epoch 185/3000, Beta: 1.00, Total Loss: 6.3218, Recon Loss: 12.6262, KL Loss: 0.0051, Percep Loss: 0.0036\n",
      "Epoch 186/3000, Beta: 1.00, Total Loss: 6.0653, Recon Loss: 12.1043, KL Loss: 0.0084, Percep Loss: 0.0047\n",
      "Epoch 187/3000, Beta: 1.00, Total Loss: 5.6961, Recon Loss: 11.3728, KL Loss: 0.0064, Percep Loss: 0.0033\n",
      "Epoch 188/3000, Beta: 1.00, Total Loss: 5.9166, Recon Loss: 11.8017, KL Loss: 0.0118, Percep Loss: 0.0039\n",
      "Epoch 189/3000, Beta: 1.00, Total Loss: 6.9761, Recon Loss: 13.9172, KL Loss: 0.0128, Percep Loss: 0.0048\n",
      "Epoch 190/3000, Beta: 1.00, Total Loss: 6.2608, Recon Loss: 12.4980, KL Loss: 0.0083, Percep Loss: 0.0035\n",
      "Epoch 191/3000, Beta: 1.00, Total Loss: 6.0319, Recon Loss: 12.0470, KL Loss: 0.0048, Percep Loss: 0.0035\n",
      "Epoch 192/3000, Beta: 1.00, Total Loss: 6.8031, Recon Loss: 13.5806, KL Loss: 0.0087, Percep Loss: 0.0040\n",
      "Epoch 193/3000, Beta: 1.00, Total Loss: 5.8637, Recon Loss: 11.7086, KL Loss: 0.0060, Percep Loss: 0.0034\n",
      "Epoch 194/3000, Beta: 1.00, Total Loss: 6.1489, Recon Loss: 12.2817, KL Loss: 0.0042, Percep Loss: 0.0038\n",
      "Epoch 195/3000, Beta: 1.00, Total Loss: 5.6349, Recon Loss: 11.2323, KL Loss: 0.0148, Percep Loss: 0.0039\n",
      "Epoch 196/3000, Beta: 1.00, Total Loss: 5.3267, Recon Loss: 10.6349, KL Loss: 0.0064, Percep Loss: 0.0028\n",
      "Epoch 197/3000, Beta: 1.00, Total Loss: 6.0674, Recon Loss: 12.1092, KL Loss: 0.0090, Percep Loss: 0.0037\n",
      "Epoch 198/3000, Beta: 1.00, Total Loss: 6.3023, Recon Loss: 12.5790, KL Loss: 0.0079, Percep Loss: 0.0049\n",
      "Epoch 199/3000, Beta: 1.00, Total Loss: 7.0006, Recon Loss: 13.9783, KL Loss: 0.0079, Percep Loss: 0.0036\n",
      "Epoch 200/3000, Beta: 1.00, Total Loss: 7.2586, Recon Loss: 14.4911, KL Loss: 0.0080, Percep Loss: 0.0051\n",
      "Epoch 201/3000, Beta: 1.00, Total Loss: 5.6066, Recon Loss: 11.1965, KL Loss: 0.0051, Percep Loss: 0.0032\n",
      "Epoch 202/3000, Beta: 1.00, Total Loss: 6.6628, Recon Loss: 13.3129, KL Loss: 0.0035, Percep Loss: 0.0028\n",
      "Epoch 203/3000, Beta: 1.00, Total Loss: 12.7418, Recon Loss: 25.4576, KL Loss: 0.0072, Percep Loss: 0.0058\n",
      "Epoch 204/3000, Beta: 1.00, Total Loss: 5.1612, Recon Loss: 10.2943, KL Loss: 0.0115, Percep Loss: 0.0025\n",
      "Epoch 205/3000, Beta: 1.00, Total Loss: 5.3756, Recon Loss: 10.7376, KL Loss: 0.0042, Percep Loss: 0.0026\n",
      "Epoch 206/3000, Beta: 1.00, Total Loss: 5.0587, Recon Loss: 10.1003, KL Loss: 0.0054, Percep Loss: 0.0032\n",
      "Epoch 207/3000, Beta: 1.00, Total Loss: 4.8428, Recon Loss: 9.6741, KL Loss: 0.0033, Percep Loss: 0.0024\n",
      "Epoch 208/3000, Beta: 1.00, Total Loss: 5.1680, Recon Loss: 10.3028, KL Loss: 0.0142, Percep Loss: 0.0024\n",
      "Epoch 209/3000, Beta: 1.00, Total Loss: 4.3718, Recon Loss: 8.7308, KL Loss: 0.0040, Percep Loss: 0.0024\n",
      "Epoch 210/3000, Beta: 1.00, Total Loss: 4.6627, Recon Loss: 9.3150, KL Loss: 0.0027, Percep Loss: 0.0025\n",
      "Epoch 211/3000, Beta: 1.00, Total Loss: 4.4515, Recon Loss: 8.8915, KL Loss: 0.0033, Percep Loss: 0.0024\n",
      "Epoch 212/3000, Beta: 1.00, Total Loss: 5.4291, Recon Loss: 10.8459, KL Loss: 0.0031, Percep Loss: 0.0031\n",
      "Epoch 213/3000, Beta: 1.00, Total Loss: 4.3549, Recon Loss: 8.6964, KL Loss: 0.0043, Percep Loss: 0.0024\n",
      "Epoch 214/3000, Beta: 1.00, Total Loss: 5.3783, Recon Loss: 10.7415, KL Loss: 0.0048, Percep Loss: 0.0027\n",
      "Epoch 215/3000, Beta: 1.00, Total Loss: 5.5047, Recon Loss: 10.9988, KL Loss: 0.0031, Percep Loss: 0.0022\n",
      "Epoch 216/3000, Beta: 1.00, Total Loss: 5.1419, Recon Loss: 10.2744, KL Loss: 0.0025, Percep Loss: 0.0023\n",
      "Epoch 217/3000, Beta: 1.00, Total Loss: 5.9982, Recon Loss: 11.9833, KL Loss: 0.0033, Percep Loss: 0.0032\n",
      "Epoch 218/3000, Beta: 1.00, Total Loss: 5.6763, Recon Loss: 11.3266, KL Loss: 0.0105, Percep Loss: 0.0026\n",
      "Epoch 219/3000, Beta: 1.00, Total Loss: 5.7994, Recon Loss: 11.5797, KL Loss: 0.0071, Percep Loss: 0.0025\n",
      "Epoch 220/3000, Beta: 1.00, Total Loss: 5.6105, Recon Loss: 11.2092, KL Loss: 0.0029, Percep Loss: 0.0030\n",
      "Epoch 221/3000, Beta: 1.00, Total Loss: 4.7741, Recon Loss: 9.5374, KL Loss: 0.0027, Percep Loss: 0.0027\n",
      "Epoch 222/3000, Beta: 1.00, Total Loss: 5.9370, Recon Loss: 11.8621, KL Loss: 0.0021, Percep Loss: 0.0039\n",
      "Epoch 223/3000, Beta: 1.00, Total Loss: 5.7712, Recon Loss: 11.5281, KL Loss: 0.0032, Percep Loss: 0.0040\n",
      "Epoch 224/3000, Beta: 1.00, Total Loss: 4.7124, Recon Loss: 9.4136, KL Loss: 0.0028, Percep Loss: 0.0028\n",
      "Epoch 225/3000, Beta: 1.00, Total Loss: 4.7628, Recon Loss: 9.5170, KL Loss: 0.0019, Percep Loss: 0.0024\n",
      "Epoch 226/3000, Beta: 1.00, Total Loss: 5.3186, Recon Loss: 10.6161, KL Loss: 0.0078, Percep Loss: 0.0028\n",
      "Epoch 227/3000, Beta: 1.00, Total Loss: 5.2921, Recon Loss: 10.5696, KL Loss: 0.0046, Percep Loss: 0.0028\n",
      "Epoch 228/3000, Beta: 1.00, Total Loss: 5.0140, Recon Loss: 10.0143, KL Loss: 0.0045, Percep Loss: 0.0023\n",
      "Epoch 229/3000, Beta: 1.00, Total Loss: 5.3590, Recon Loss: 10.6885, KL Loss: 0.0123, Percep Loss: 0.0024\n",
      "Epoch 230/3000, Beta: 1.00, Total Loss: 5.1712, Recon Loss: 10.3144, KL Loss: 0.0117, Percep Loss: 0.0024\n",
      "Epoch 231/3000, Beta: 1.00, Total Loss: 5.9350, Recon Loss: 11.8553, KL Loss: 0.0049, Percep Loss: 0.0025\n",
      "Epoch 232/3000, Beta: 1.00, Total Loss: 6.3458, Recon Loss: 12.6788, KL Loss: 0.0040, Percep Loss: 0.0023\n",
      "Epoch 233/3000, Beta: 1.00, Total Loss: 5.3385, Recon Loss: 10.6659, KL Loss: 0.0031, Percep Loss: 0.0025\n",
      "Epoch 234/3000, Beta: 1.00, Total Loss: 4.8698, Recon Loss: 9.7282, KL Loss: 0.0033, Percep Loss: 0.0024\n",
      "Epoch 235/3000, Beta: 1.00, Total Loss: 5.0883, Recon Loss: 10.1681, KL Loss: 0.0020, Percep Loss: 0.0022\n",
      "Epoch 236/3000, Beta: 1.00, Total Loss: 5.1259, Recon Loss: 10.2436, KL Loss: 0.0022, Percep Loss: 0.0019\n",
      "Epoch 237/3000, Beta: 1.00, Total Loss: 4.1824, Recon Loss: 8.3581, KL Loss: 0.0013, Percep Loss: 0.0021\n",
      "Epoch 238/3000, Beta: 1.00, Total Loss: 4.9630, Recon Loss: 9.9194, KL Loss: 0.0009, Percep Loss: 0.0024\n",
      "Epoch 239/3000, Beta: 1.00, Total Loss: 4.9031, Recon Loss: 9.7999, KL Loss: 0.0011, Percep Loss: 0.0021\n",
      "Epoch 240/3000, Beta: 1.00, Total Loss: 5.0283, Recon Loss: 10.0500, KL Loss: 0.0012, Percep Loss: 0.0021\n",
      "Epoch 241/3000, Beta: 1.00, Total Loss: 5.3780, Recon Loss: 10.7476, KL Loss: 0.0018, Percep Loss: 0.0024\n",
      "Epoch 242/3000, Beta: 1.00, Total Loss: 4.6117, Recon Loss: 9.2139, KL Loss: 0.0027, Percep Loss: 0.0021\n",
      "Epoch 243/3000, Beta: 1.00, Total Loss: 5.3246, Recon Loss: 10.6356, KL Loss: 0.0048, Percep Loss: 0.0020\n",
      "Epoch 244/3000, Beta: 1.00, Total Loss: 5.5641, Recon Loss: 11.0820, KL Loss: 0.0209, Percep Loss: 0.0023\n",
      "Epoch 245/3000, Beta: 1.00, Total Loss: 5.8079, Recon Loss: 11.6040, KL Loss: 0.0041, Percep Loss: 0.0018\n",
      "Epoch 246/3000, Beta: 1.00, Total Loss: 4.8123, Recon Loss: 9.6168, KL Loss: 0.0019, Percep Loss: 0.0020\n",
      "Epoch 247/3000, Beta: 1.00, Total Loss: 5.4143, Recon Loss: 10.8218, KL Loss: 0.0011, Percep Loss: 0.0023\n",
      "Epoch 248/3000, Beta: 1.00, Total Loss: 5.3431, Recon Loss: 10.6794, KL Loss: 0.0014, Percep Loss: 0.0019\n",
      "Epoch 249/3000, Beta: 1.00, Total Loss: 4.6157, Recon Loss: 9.2253, KL Loss: 0.0012, Percep Loss: 0.0018\n",
      "Epoch 250/3000, Beta: 1.00, Total Loss: 4.6955, Recon Loss: 9.3835, KL Loss: 0.0013, Percep Loss: 0.0024\n",
      "Epoch 251/3000, Beta: 1.00, Total Loss: 5.2222, Recon Loss: 10.4384, KL Loss: 0.0013, Percep Loss: 0.0017\n",
      "Epoch 252/3000, Beta: 1.00, Total Loss: 4.5948, Recon Loss: 9.1845, KL Loss: 0.0007, Percep Loss: 0.0018\n",
      "Epoch 253/3000, Beta: 1.00, Total Loss: 5.5091, Recon Loss: 11.0126, KL Loss: 0.0007, Percep Loss: 0.0021\n",
      "Epoch 254/3000, Beta: 1.00, Total Loss: 4.5449, Recon Loss: 9.0842, KL Loss: 0.0008, Percep Loss: 0.0019\n",
      "Epoch 255/3000, Beta: 1.00, Total Loss: 5.0407, Recon Loss: 10.0739, KL Loss: 0.0012, Percep Loss: 0.0025\n",
      "Epoch 256/3000, Beta: 1.00, Total Loss: 4.5303, Recon Loss: 9.0538, KL Loss: 0.0011, Percep Loss: 0.0023\n",
      "Epoch 257/3000, Beta: 1.00, Total Loss: 5.0535, Recon Loss: 10.1003, KL Loss: 0.0017, Percep Loss: 0.0017\n",
      "Epoch 258/3000, Beta: 1.00, Total Loss: 4.1964, Recon Loss: 8.3859, KL Loss: 0.0017, Percep Loss: 0.0018\n",
      "Epoch 259/3000, Beta: 1.00, Total Loss: 5.1068, Recon Loss: 10.2082, KL Loss: 0.0010, Percep Loss: 0.0018\n",
      "Epoch 260/3000, Beta: 1.00, Total Loss: 4.4741, Recon Loss: 8.9419, KL Loss: 0.0009, Percep Loss: 0.0022\n",
      "Epoch 261/3000, Beta: 1.00, Total Loss: 4.4942, Recon Loss: 8.9824, KL Loss: 0.0007, Percep Loss: 0.0022\n",
      "Epoch 262/3000, Beta: 1.00, Total Loss: 4.7805, Recon Loss: 9.5543, KL Loss: 0.0014, Percep Loss: 0.0019\n",
      "Epoch 263/3000, Beta: 1.00, Total Loss: 4.4540, Recon Loss: 8.9021, KL Loss: 0.0011, Percep Loss: 0.0018\n",
      "Epoch 264/3000, Beta: 1.00, Total Loss: 5.1604, Recon Loss: 10.3159, KL Loss: 0.0006, Percep Loss: 0.0018\n",
      "Epoch 265/3000, Beta: 1.00, Total Loss: 4.4135, Recon Loss: 8.8225, KL Loss: 0.0006, Percep Loss: 0.0017\n",
      "Epoch 266/3000, Beta: 1.00, Total Loss: 5.4529, Recon Loss: 10.8983, KL Loss: 0.0012, Percep Loss: 0.0025\n",
      "Epoch 267/3000, Beta: 1.00, Total Loss: 5.8932, Recon Loss: 11.7727, KL Loss: 0.0039, Percep Loss: 0.0029\n",
      "Epoch 268/3000, Beta: 1.00, Total Loss: 4.3326, Recon Loss: 8.6484, KL Loss: 0.0063, Percep Loss: 0.0021\n",
      "Epoch 269/3000, Beta: 1.00, Total Loss: 3.4009, Recon Loss: 6.7914, KL Loss: 0.0036, Percep Loss: 0.0017\n",
      "Epoch 270/3000, Beta: 1.00, Total Loss: 4.5660, Recon Loss: 9.1221, KL Loss: 0.0031, Percep Loss: 0.0018\n",
      "Epoch 271/3000, Beta: 1.00, Total Loss: 4.6968, Recon Loss: 9.3853, KL Loss: 0.0025, Percep Loss: 0.0017\n",
      "Epoch 272/3000, Beta: 1.00, Total Loss: 5.2091, Recon Loss: 10.4096, KL Loss: 0.0023, Percep Loss: 0.0020\n",
      "Epoch 273/3000, Beta: 1.00, Total Loss: 4.2208, Recon Loss: 8.4338, KL Loss: 0.0020, Percep Loss: 0.0019\n",
      "Epoch 274/3000, Beta: 1.00, Total Loss: 4.3291, Recon Loss: 8.6502, KL Loss: 0.0020, Percep Loss: 0.0020\n",
      "Epoch 275/3000, Beta: 1.00, Total Loss: 4.5855, Recon Loss: 9.1630, KL Loss: 0.0019, Percep Loss: 0.0021\n",
      "Epoch 276/3000, Beta: 1.00, Total Loss: 4.2094, Recon Loss: 8.4128, KL Loss: 0.0014, Percep Loss: 0.0016\n",
      "Epoch 277/3000, Beta: 1.00, Total Loss: 4.1568, Recon Loss: 8.3084, KL Loss: 0.0011, Percep Loss: 0.0016\n",
      "Epoch 278/3000, Beta: 1.00, Total Loss: 5.3533, Recon Loss: 10.7011, KL Loss: 0.0012, Percep Loss: 0.0017\n",
      "Epoch 279/3000, Beta: 1.00, Total Loss: 4.5553, Recon Loss: 9.1046, KL Loss: 0.0011, Percep Loss: 0.0019\n",
      "Epoch 280/3000, Beta: 1.00, Total Loss: 4.3977, Recon Loss: 8.7895, KL Loss: 0.0009, Percep Loss: 0.0020\n",
      "Epoch 281/3000, Beta: 1.00, Total Loss: 4.7940, Recon Loss: 9.5823, KL Loss: 0.0008, Percep Loss: 0.0021\n",
      "Epoch 282/3000, Beta: 1.00, Total Loss: 3.8361, Recon Loss: 7.6672, KL Loss: 0.0008, Percep Loss: 0.0017\n",
      "Epoch 283/3000, Beta: 1.00, Total Loss: 5.1424, Recon Loss: 10.2799, KL Loss: 0.0008, Percep Loss: 0.0017\n",
      "Epoch 284/3000, Beta: 1.00, Total Loss: 4.4462, Recon Loss: 8.8873, KL Loss: 0.0009, Percep Loss: 0.0017\n",
      "Epoch 285/3000, Beta: 1.00, Total Loss: 5.0687, Recon Loss: 10.1320, KL Loss: 0.0010, Percep Loss: 0.0018\n",
      "Epoch 286/3000, Beta: 1.00, Total Loss: 4.1045, Recon Loss: 8.2019, KL Loss: 0.0013, Percep Loss: 0.0022\n",
      "Epoch 287/3000, Beta: 1.00, Total Loss: 4.5968, Recon Loss: 9.1863, KL Loss: 0.0019, Percep Loss: 0.0018\n",
      "Epoch 288/3000, Beta: 1.00, Total Loss: 4.8790, Recon Loss: 9.7529, KL Loss: 0.0008, Percep Loss: 0.0018\n",
      "Epoch 289/3000, Beta: 1.00, Total Loss: 4.4707, Recon Loss: 8.9356, KL Loss: 0.0011, Percep Loss: 0.0018\n",
      "Epoch 290/3000, Beta: 1.00, Total Loss: 4.2039, Recon Loss: 8.4017, KL Loss: 0.0013, Percep Loss: 0.0017\n",
      "Epoch 291/3000, Beta: 1.00, Total Loss: 4.3336, Recon Loss: 8.6610, KL Loss: 0.0012, Percep Loss: 0.0018\n",
      "Epoch 292/3000, Beta: 1.00, Total Loss: 4.4614, Recon Loss: 8.9163, KL Loss: 0.0014, Percep Loss: 0.0018\n",
      "Epoch 293/3000, Beta: 1.00, Total Loss: 4.0681, Recon Loss: 8.1267, KL Loss: 0.0032, Percep Loss: 0.0016\n",
      "Epoch 294/3000, Beta: 1.00, Total Loss: 4.2914, Recon Loss: 8.5717, KL Loss: 0.0035, Percep Loss: 0.0020\n",
      "Epoch 295/3000, Beta: 1.00, Total Loss: 4.7359, Recon Loss: 9.4634, KL Loss: 0.0026, Percep Loss: 0.0016\n",
      "Epoch 296/3000, Beta: 1.00, Total Loss: 4.2895, Recon Loss: 8.5709, KL Loss: 0.0021, Percep Loss: 0.0019\n",
      "Epoch 297/3000, Beta: 1.00, Total Loss: 4.3974, Recon Loss: 8.7881, KL Loss: 0.0017, Percep Loss: 0.0017\n",
      "Epoch 298/3000, Beta: 1.00, Total Loss: 4.4998, Recon Loss: 8.9925, KL Loss: 0.0016, Percep Loss: 0.0020\n",
      "Epoch 299/3000, Beta: 1.00, Total Loss: 4.0435, Recon Loss: 8.0808, KL Loss: 0.0013, Percep Loss: 0.0018\n",
      "Epoch 300/3000, Beta: 1.00, Total Loss: 4.8050, Recon Loss: 9.6040, KL Loss: 0.0010, Percep Loss: 0.0020\n",
      "Epoch 301/3000, Beta: 1.00, Total Loss: 4.2445, Recon Loss: 8.4841, KL Loss: 0.0008, Percep Loss: 0.0017\n",
      "Epoch 302/3000, Beta: 1.00, Total Loss: 4.2051, Recon Loss: 8.4054, KL Loss: 0.0008, Percep Loss: 0.0016\n",
      "Epoch 303/3000, Beta: 1.00, Total Loss: 5.1537, Recon Loss: 10.3001, KL Loss: 0.0019, Percep Loss: 0.0017\n",
      "Epoch 304/3000, Beta: 1.00, Total Loss: 7.1193, Recon Loss: 14.2240, KL Loss: 0.0029, Percep Loss: 0.0044\n",
      "Epoch 305/3000, Beta: 1.00, Total Loss: 4.7695, Recon Loss: 9.5235, KL Loss: 0.0057, Percep Loss: 0.0021\n",
      "Epoch 306/3000, Beta: 1.00, Total Loss: 4.5080, Recon Loss: 9.0038, KL Loss: 0.0042, Percep Loss: 0.0019\n",
      "Epoch 307/3000, Beta: 1.00, Total Loss: 4.2285, Recon Loss: 8.4455, KL Loss: 0.0040, Percep Loss: 0.0017\n",
      "Epoch 308/3000, Beta: 1.00, Total Loss: 5.3633, Recon Loss: 10.7147, KL Loss: 0.0036, Percep Loss: 0.0023\n",
      "Epoch 309/3000, Beta: 1.00, Total Loss: 4.4266, Recon Loss: 8.8436, KL Loss: 0.0029, Percep Loss: 0.0019\n",
      "Epoch 310/3000, Beta: 1.00, Total Loss: 5.3410, Recon Loss: 10.6723, KL Loss: 0.0025, Percep Loss: 0.0024\n",
      "Epoch 311/3000, Beta: 1.00, Total Loss: 4.4345, Recon Loss: 8.8610, KL Loss: 0.0022, Percep Loss: 0.0018\n",
      "Epoch 312/3000, Beta: 1.00, Total Loss: 4.6905, Recon Loss: 9.3737, KL Loss: 0.0020, Percep Loss: 0.0017\n",
      "Epoch 313/3000, Beta: 1.00, Total Loss: 4.3285, Recon Loss: 8.6484, KL Loss: 0.0021, Percep Loss: 0.0022\n",
      "Epoch 314/3000, Beta: 1.00, Total Loss: 4.7082, Recon Loss: 9.4081, KL Loss: 0.0022, Percep Loss: 0.0020\n",
      "Epoch 315/3000, Beta: 1.00, Total Loss: 4.8896, Recon Loss: 9.7713, KL Loss: 0.0019, Percep Loss: 0.0020\n",
      "Epoch 316/3000, Beta: 1.00, Total Loss: 4.1510, Recon Loss: 8.2940, KL Loss: 0.0018, Percep Loss: 0.0021\n",
      "Epoch 317/3000, Beta: 1.00, Total Loss: 4.6971, Recon Loss: 9.3865, KL Loss: 0.0018, Percep Loss: 0.0021\n",
      "Epoch 318/3000, Beta: 1.00, Total Loss: 4.1707, Recon Loss: 8.3348, KL Loss: 0.0017, Percep Loss: 0.0016\n",
      "Epoch 319/3000, Beta: 1.00, Total Loss: 4.1158, Recon Loss: 8.2242, KL Loss: 0.0016, Percep Loss: 0.0021\n",
      "Early stopping at epoch 319 due to no improvement in loss.\n",
      "Epoch 319: Generated 500 samples for Class 0 (Hatchback).\n",
      "Epoch 319: Generated 500 samples for Class 1 (Other).\n",
      "Epoch 319: Generated 500 samples for Class 2 (Pickup).\n",
      "Epoch 319: Generated 500 samples for Class 3 (Seden).\n",
      "Epoch 319: Generated 500 samples for Class 4 (SUV).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.models as models\n",
    "from torchvision.models import VGG16_Weights\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import kagglehub\n",
    "from PIL import Image\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.device_count() > 1 else 'cuda')\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 256\n",
    "intermediate_dim = 1024\n",
    "num_classes = 5\n",
    "batch_size = 64\n",
    "epochs = 3000\n",
    "learning_rate = 3e-4\n",
    "image_size = 128\n",
    "channels = 3\n",
    "output_dir = \"FL_CVAE\"\n",
    "beta_max = 1.0\n",
    "annealing_epochs = 100\n",
    "recon_weight = 0.5\n",
    "perceptual_weight = 5.0\n",
    "weight_decay = 1e-5\n",
    "patience = 50\n",
    "min_delta = 0.001\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Download Vehicle Type Image Dataset from Kaggle\n",
    "try:\n",
    "    path = kagglehub.dataset_download(\"sujaykapadnis/vehicle-type-image-dataset\")\n",
    "    print(\"Path to dataset files:\", path)\n",
    "    dataset_path = path\n",
    "except Exception as e:\n",
    "    print(f\"Failed to download dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Define the VehicleTypeDataset class\n",
    "class VehicleTypeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_names = []\n",
    "        self.class_to_idx = {}\n",
    "\n",
    "        print(f\"Searching for images in {root_dir}\")\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            image_files = [f for f in files if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "            if image_files:\n",
    "                class_name = os.path.basename(root)\n",
    "                if class_name not in self.class_to_idx:\n",
    "                    self.class_names.append(class_name)\n",
    "                    self.class_to_idx[class_name] = len(self.class_names) - 1\n",
    "                for img_file in image_files:\n",
    "                    img_path = os.path.join(root, img_file)\n",
    "                    self.images.append(img_path)\n",
    "                    self.labels.append(self.class_to_idx[class_name])\n",
    "\n",
    "        if not self.images:\n",
    "            raise ValueError(\n",
    "                f\"No images found in {root_dir}. \"\n",
    "                \"Expected class folders containing .jpg, .png, or .jpeg images.\"\n",
    "            )\n",
    "\n",
    "        print(f\"Found {len(self.images)} images across {len(self.class_names)} classes.\")\n",
    "        print(f\"Classes: {self.class_names}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = VehicleTypeDataset(root_dir=dataset_path, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the Enhanced Encoder network\n",
    "class EnhancedEncoder(nn.Module):\n",
    "    def __init__(self, latent_dim, intermediate_dim, num_classes):\n",
    "        super(EnhancedEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels + num_classes, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.conv5 = nn.Conv2d(512, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        self.conv6 = nn.Conv2d(512, 1024, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(1024)\n",
    "        \n",
    "        self.fc_intermediate = nn.Linear(1024 * 2 * 2, intermediate_dim)\n",
    "        self.bn_intermediate = nn.BatchNorm1d(intermediate_dim)\n",
    "        \n",
    "        self.fc_mean = nn.Linear(intermediate_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(intermediate_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        y = F.one_hot(y, num_classes=num_classes).float()\n",
    "        y = y.unsqueeze(-1).unsqueeze(-1)\n",
    "        y = y.expand(-1, -1, x.size(2), x.size(3))\n",
    "        x_with_y = torch.cat([x, y], dim=1)\n",
    "        \n",
    "        h1 = F.leaky_relu(self.bn1(self.conv1(x_with_y)), negative_slope=0.2)\n",
    "        h2 = F.leaky_relu(self.bn2(self.conv2(h1)), negative_slope=0.2)\n",
    "        h3 = F.leaky_relu(self.bn3(self.conv3(h2)), negative_slope=0.2)\n",
    "        h4 = F.leaky_relu(self.bn4(self.conv4(h3)), negative_slope=0.2)\n",
    "        h5 = F.leaky_relu(self.bn5(self.conv5(h4)), negative_slope=0.2)\n",
    "        h6 = F.leaky_relu(self.bn6(self.conv6(h5)), negative_slope=0.2)\n",
    "        \n",
    "        h = h6.view(h6.size(0), -1)\n",
    "        h = F.leaky_relu(self.bn_intermediate(self.fc_intermediate(h)), negative_slope=0.2)\n",
    "        \n",
    "        z_mean = self.fc_mean(h)\n",
    "        z_logvar = self.fc_logvar(h)\n",
    "        return z_mean, z_logvar, (h1, h2, h3, h4, h5, h6)\n",
    "\n",
    "# Define the Enhanced Decoder network\n",
    "class EnhancedDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, intermediate_dim, num_classes):\n",
    "        super(EnhancedDecoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim + num_classes, intermediate_dim)\n",
    "        self.bn_fc = nn.BatchNorm1d(intermediate_dim)\n",
    "        self.fc_to_features = nn.Linear(intermediate_dim, 1024 * 2 * 2)\n",
    "        \n",
    "        self.deconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(512)\n",
    "        self.deconv2 = nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(512)\n",
    "        self.deconv3 = nn.ConvTranspose2d(1024, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.deconv4 = nn.ConvTranspose2d(512, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.deconv5 = nn.ConvTranspose2d(256, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(64)\n",
    "        self.deconv6 = nn.ConvTranspose2d(128, channels, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, z, y, skip_connections):\n",
    "        h1, h2, h3, h4, h5, h6 = skip_connections\n",
    "        y = F.one_hot(y, num_classes=num_classes).float()\n",
    "        z_with_y = torch.cat([z, y], dim=-1)\n",
    "        \n",
    "        h = F.leaky_relu(self.bn_fc(self.fc(z_with_y)), negative_slope=0.2)\n",
    "        h = self.fc_to_features(h)\n",
    "        h = h.view(h.size(0), 1024, 2, 2)\n",
    "        \n",
    "        h = F.leaky_relu(self.bn1(self.deconv1(h)), negative_slope=0.2)\n",
    "        h = torch.cat([h, h5], dim=1)\n",
    "        h = F.leaky_relu(self.bn2(self.deconv2(h)), negative_slope=0.2)\n",
    "        h = torch.cat([h, h4], dim=1)\n",
    "        h = F.leaky_relu(self.bn3(self.deconv3(h)), negative_slope=0.2)\n",
    "        h = torch.cat([h, h3], dim=1)\n",
    "        h = F.leaky_relu(self.bn4(self.deconv4(h)), negative_slope=0.2)\n",
    "        h = torch.cat([h, h2], dim=1)\n",
    "        h = F.leaky_relu(self.bn5(self.deconv5(h)), negative_slope=0.2)\n",
    "        h = torch.cat([h, h1], dim=1)\n",
    "        x_reconstructed = self.deconv6(h)\n",
    "        return torch.clamp(x_reconstructed, 0, 1)\n",
    "\n",
    "# Conditional VAE model\n",
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def reparameterize(self, z_mean, z_logvar):\n",
    "        std = torch.exp(0.5 * z_logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return z_mean + eps * std\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        z_mean, z_logvar, skip_connections = self.encoder(x, y)\n",
    "        z = self.reparameterize(z_mean, z_logvar)\n",
    "        x_reconstructed = self.decoder(z, y, skip_connections)\n",
    "        return x_reconstructed, z_mean, z_logvar\n",
    "\n",
    "# Load pretrained VGG16 for perceptual loss\n",
    "vgg = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features.to(device).eval()\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "def perceptual_loss(x, x_reconstructed):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(x.device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(x.device)\n",
    "    x_normalized = (x - mean) / std\n",
    "    x_reconstructed_normalized = (x_reconstructed - mean) / std\n",
    "    x_features = vgg(x_normalized)\n",
    "    x_recon_features = vgg(x_reconstructed_normalized)\n",
    "    return F.mse_loss(x_features, x_recon_features)\n",
    "\n",
    "# Instantiate the model\n",
    "encoder = EnhancedEncoder(latent_dim, intermediate_dim, num_classes).to(device)\n",
    "decoder = EnhancedDecoder(latent_dim, intermediate_dim, num_classes).to(device)\n",
    "cvae = ConditionalVAE(encoder, decoder).to(device)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint_path = os.path.join(output_dir, \"cvae_vehicle_epoch_50.pth\")\n",
    "if os.path.exists(checkpoint_path):\n",
    "    cvae.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    print(f\"Loaded checkpoint from {checkpoint_path}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Checkpoint file {checkpoint_path} not found\")\n",
    "\n",
    "# Define optimizer and scheduler\n",
    "optimizer = optim.Adam(cvae.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=20, factor=0.5)\n",
    "\n",
    "# Define loss function\n",
    "def cvae_loss(x, x_reconstructed, z_mean, z_logvar, beta=1.0, recon_weight=1.0, perceptual_weight=1.0):\n",
    "    recon_loss = F.mse_loss(x_reconstructed, x, reduction='sum')\n",
    "    kl_loss = -0.5 * torch.sum(1 + z_logvar - z_mean.pow(2) - z_logvar.exp()) + 1e-6\n",
    "    percep_loss = perceptual_loss(x, x_reconstructed) * perceptual_weight\n",
    "    total_loss = recon_weight * recon_loss + beta * kl_loss + percep_loss\n",
    "    return total_loss, recon_loss, kl_loss, percep_loss\n",
    "\n",
    "# Generate samples for all classes\n",
    "def generate_samples_labelwise(cvae, num_samples, classes_to_generate, base_dir, latent_dim, device, epoch):\n",
    "    cvae.eval()\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    with torch.no_grad():\n",
    "        for class_label in classes_to_generate:\n",
    "            label_tensor = torch.tensor([class_label]).repeat(num_samples).to(device)\n",
    "            z = torch.randn(num_samples, latent_dim).to(device)\n",
    "            dummy_skips = [\n",
    "                torch.randn(num_samples, 64, 64, 64).to(device) * 0.1,\n",
    "                torch.randn(num_samples, 128, 32, 32).to(device) * 0.1,\n",
    "                torch.randn(num_samples, 256, 16, 16).to(device) * 0.1,\n",
    "                torch.randn(num_samples, 512, 8, 8).to(device) * 0.1,\n",
    "                torch.randn(num_samples, 512, 4, 4).to(device) * 0.1,\n",
    "                torch.randn(num_samples, 1024, 2, 2).to(device) * 0.1\n",
    "            ]\n",
    "            generated_samples = cvae.decoder(z, label_tensor, dummy_skips)\n",
    "            \n",
    "            class_dir = os.path.join(base_dir, f\"class_{class_label}\")\n",
    "            os.makedirs(class_dir, exist_ok=True)\n",
    "            for idx, sample in enumerate(generated_samples):\n",
    "                save_image(sample, os.path.join(class_dir, f\"sample_{idx}_epoch_{epoch}.png\"))\n",
    "            print(f\"Epoch {epoch}: Generated {num_samples} samples for Class {class_label} ({dataset.class_names[class_label]}).\")\n",
    "\n",
    "# Plot random samples for all classes\n",
    "def plot_random_samples(base_dir, classes_to_generate, num_images_per_class, epoch):\n",
    "    fig, axs = plt.subplots(len(classes_to_generate), num_images_per_class, figsize=(20, len(classes_to_generate) * 2))\n",
    "    for row, class_label in enumerate(classes_to_generate):\n",
    "        class_dir = os.path.join(base_dir, f\"class_{class_label}\")\n",
    "        sample_files = [f for f in os.listdir(class_dir) if f.endswith(f\"epoch_{epoch}.png\")]\n",
    "        if len(sample_files) < num_images_per_class:\n",
    "            continue\n",
    "        random_samples = np.random.choice(sample_files, num_images_per_class, replace=False)\n",
    "        \n",
    "        for col, sample_file in enumerate(random_samples):\n",
    "            sample_path = os.path.join(class_dir, sample_file)\n",
    "            sample_image = plt.imread(sample_path)\n",
    "            ax = axs[row, col] if len(classes_to_generate) > 1 else axs[col]\n",
    "            ax.imshow(sample_image)\n",
    "            ax.axis('off')\n",
    "            if col == 0:\n",
    "                ax.set_ylabel(dataset.class_names[class_label], rotation=90, labelpad=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(output_dir, f\"synthetic_samples_epoch_{epoch}.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    print(f\"Saved synthetic samples plot for epoch {epoch} to {plot_path}\")\n",
    "\n",
    "# Training loop with early stopping\n",
    "cvae.train()\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "start_epoch = 50  # Start from epoch 50\n",
    "classes_to_generate = list(range(num_classes))\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    if epoch < annealing_epochs:\n",
    "        beta = beta_max * (epoch / annealing_epochs)\n",
    "    else:\n",
    "        beta = beta_max\n",
    "\n",
    "    train_loss = 0\n",
    "    train_recon_loss = 0\n",
    "    train_kl_loss = 0\n",
    "    train_percep_loss = 0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        x_reconstructed, z_mean, z_logvar = cvae(data, labels)\n",
    "        total_loss, recon_loss, kl_loss, percep_loss = cvae_loss(\n",
    "            data, x_reconstructed, z_mean, z_logvar, \n",
    "            beta=beta, recon_weight=recon_weight, perceptual_weight=perceptual_weight\n",
    "        )\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(cvae.parameters(), max_norm=1.0)\n",
    "        train_loss += total_loss.item()\n",
    "        train_recon_loss += recon_loss.item()\n",
    "        train_kl_loss += kl_loss.item()\n",
    "        train_percep_loss += percep_loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    scheduler.step(avg_loss)\n",
    "    avg_recon_loss = train_recon_loss / len(train_loader.dataset)\n",
    "    avg_kl_loss = train_kl_loss / len(train_loader.dataset)\n",
    "    avg_percep_loss = train_percep_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Beta: {beta:.2f}, Total Loss: {avg_loss:.4f}, '\n",
    "          f'Recon Loss: {avg_recon_loss:.4f}, KL Loss: {avg_kl_loss:.4f}, Percep Loss: {avg_percep_loss:.4f}')\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_loss < best_loss - min_delta:\n",
    "        best_loss = avg_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1} due to no improvement in loss.\")\n",
    "            # Generate and plot samples at early stopping\n",
    "            base_dir = os.path.join(output_dir, f\"generated_samples_epoch_{epoch + 1}\")\n",
    "            generate_samples_labelwise(cvae, num_samples=500, classes_to_generate=classes_to_generate, \n",
    "                                     base_dir=base_dir, latent_dim=latent_dim, device=device, epoch=epoch + 1)\n",
    "            plot_random_samples(base_dir=base_dir, classes_to_generate=classes_to_generate, \n",
    "                               num_images_per_class=10, epoch=epoch + 1)\n",
    "            break\n",
    "\n",
    "# Save the final model\n",
    "model_path = os.path.join(output_dir, \"cvae_vehicle_final.pth\")\n",
    "torch.save(cvae.state_dict(), model_path)\n",
    "print(f\"Saved final CVAE model to {model_path}\")\n",
    "\n",
    "# Generate and plot samples at the final epoch (either 3000 or early stopping)\n",
    "base_dir = os.path.join(output_dir, \"generated_samples_final\")\n",
    "generate_samples_labelwise(cvae, num_samples=500, classes_to_generate=classes_to_generate, \n",
    "                         base_dir=base_dir, latent_dim=latent_dim, device=device, epoch=epoch + 1)\n",
    "plot_random_samples(base_dir=base_dir, classes_to_generate=classes_to_generate, \n",
    "                   num_images_per_class=10, epoch=epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450f0855-f60f-4495-a72d-51ae67db1082",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
