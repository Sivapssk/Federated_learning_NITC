{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52479166-070c-4f29-b909-d29d33202235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ac21489-11db-4026-922c-927d8475486a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Dataset path: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Directory contents: ['Vehicle Type Image Dataset (Version 2) VTID2']\n",
      "Contents of Vehicle Type Image Dataset (Version 2) VTID2: ['Hatchback', 'Other', 'Pickup', 'Seden', 'SUV']\n",
      "Scanning dataset directory: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "No images found in C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\\Vehicle Type Image Dataset (Version 2) VTID2\n",
      "Found 0 images across 1 classes\n",
      "Class mapping: {'Vehicle Type Image Dataset (Version 2) VTID2': 0}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No valid images found in the dataset. Check the directory structure and file formats.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 130\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(subdir_path):\n\u001b[0;32m    128\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContents of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mlistdir(subdir_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 130\u001b[0m dataset \u001b[38;5;241m=\u001b[39m VehicleDataset(root_dir\u001b[38;5;241m=\u001b[39mpath, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    132\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mclass_to_idx)  \u001b[38;5;66;03m# Update num_classes dynamically\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 102\u001b[0m, in \u001b[0;36mVehicleDataset.__init__\u001b[1;34m(self, root_dir, transform)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass mapping: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_to_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_paths) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo valid images found in the dataset. Check the directory structure and file formats.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: No valid images found in the dataset. Check the directory structure and file formats."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import kagglehub\n",
    "\n",
    "# Configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters (optimized for 128x128 RGB vehicle images)\n",
    "latent_dim = 256\n",
    "num_classes = 5\n",
    "batch_size = 64\n",
    "epochs = 3000\n",
    "learning_rate = 5e-4\n",
    "image_size = 128\n",
    "channels = 3\n",
    "beta_max = 1.0\n",
    "annealing_epochs = 100\n",
    "perceptual_weight = 10.0\n",
    "recon_weight = 1.0\n",
    "\n",
    "# Output directories\n",
    "output_dir = \"FL_CVAE\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, \"checkpoints\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, \"samples\"), exist_ok=True)\n",
    "\n",
    "# Dataset Class (replace with your actual dataset)\n",
    "\n",
    "## Dataset Class\n",
    "class VehicleDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.class_to_idx = {}\n",
    "\n",
    "        print(f\"Scanning dataset directory: {root_dir}\")\n",
    "        if not os.path.isdir(root_dir):\n",
    "            raise ValueError(f\"Root directory {root_dir} does not exist or is not a directory.\")\n",
    "\n",
    "        for idx, class_dir in enumerate(sorted(os.listdir(root_dir))):\n",
    "            class_path = os.path.join(root_dir, class_dir)\n",
    "            if os.path.isdir(class_path):\n",
    "                self.class_to_idx[class_dir] = idx\n",
    "                image_files = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "                if not image_files:\n",
    "                    print(f\"No images found in {class_path}\")\n",
    "                    continue\n",
    "                for img_file in image_files:\n",
    "                    img_path = os.path.join(class_path, img_file)\n",
    "                    try:\n",
    "                        with Image.open(img_path) as img:\n",
    "                            img.verify()\n",
    "                        self.image_paths.append(img_path)\n",
    "                        self.labels.append(idx)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Skipping corrupted image {img_path}: {e}\")\n",
    "\n",
    "        print(f\"Found {len(self.image_paths)} images across {len(self.class_to_idx)} classes\")\n",
    "        print(f\"Class mapping: {self.class_to_idx}\")\n",
    "        if len(self.image_paths) == 0:\n",
    "            raise ValueError(\"No valid images found in the dataset. Check the directory structure and file formats.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            raise\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "path = kagglehub.dataset_download(\"sujaykapadnis/vehicle-type-image-dataset\")\n",
    "print(f\"Dataset path: {path}\")\n",
    "print(f\"Directory contents: {os.listdir(path)}\")\n",
    "for subdir in os.listdir(path):\n",
    "    subdir_path = os.path.join(path, subdir)\n",
    "    if os.path.isdir(subdir_path):\n",
    "        print(f\"Contents of {subdir}: {os.listdir(subdir_path)}\")\n",
    "\n",
    "dataset = VehicleDataset(root_dir=path, transform=transform)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "num_classes = len(dataset.class_to_idx)  # Update num_classes dynamically\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "# Initialize dataset and dataloader\n",
    "path = kagglehub.dataset_download(\"sujaykapadnis/vehicle-type-image-dataset\")\n",
    "print(f\"Dataset path: {path}\")\n",
    "print(f\"Directory contents: {os.listdir(path)}\")\n",
    "\n",
    "dataset = VehicleDataset(root_dir=path, transform=transform)\n",
    "if len(dataset) == 0:\n",
    "    raise ValueError(\"No images found in the dataset. Check the directory structure and file formats.\")\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)  # Set num_workers=0 for debugging\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "path = kagglehub.dataset_download(\"sujaykapadnis/vehicle-type-image-dataset\")\n",
    "dataset = VehicleDataset(root_dir= path, transform=transform)  # Replace with your dataset path\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# Enhanced Encoder\n",
    "class EnhancedEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.initial_conv = nn.Sequential(\n",
    "            nn.Conv2d(channels + num_classes, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.down3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.down4 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(512 * 4 * 4, latent_dim)\n",
    "        self.fc_var = nn.Linear(512 * 4 * 4, latent_dim)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        y = F.one_hot(y, num_classes).float()\n",
    "        y = y.view(-1, num_classes, 1, 1).expand(-1, -1, x.size(2), x.size(3))\n",
    "        x = torch.cat([x, y], dim=1)\n",
    "        h0 = self.initial_conv(x)\n",
    "        h1 = self.down1(h0)\n",
    "        h2 = self.down2(h1)\n",
    "        h3 = self.down3(h2)\n",
    "        h4 = self.down4(h3)\n",
    "        h4 = h4.view(h4.size(0), -1)\n",
    "        return self.fc_mu(h4), self.fc_var(h4), (h0, h1, h2, h3)\n",
    "\n",
    "# Enhanced Decoder\n",
    "class EnhancedDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(latent_dim + num_classes, 512 * 4 * 4),\n",
    "            nn.BatchNorm1d(512 * 4 * 4),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.up2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.up3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.up4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.final = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, channels, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z, y, skip_connections):\n",
    "        h0, h1, h2, h3 = skip_connections\n",
    "        y = F.one_hot(y, num_classes).float()\n",
    "        z = torch.cat([z, y], dim=1)\n",
    "        h = self.fc(z)\n",
    "        h = h.view(-1, 512, 4, 4)\n",
    "        h = self.up1(h)\n",
    "        h = torch.cat([h, h3], dim=1)\n",
    "        h = self.up2(h)\n",
    "        h = torch.cat([h, h2], dim=1)\n",
    "        h = self.up3(h)\n",
    "        h = torch.cat([h, h1], dim=1)\n",
    "        h = self.up4(h)\n",
    "        h = torch.cat([h, h0], dim=1)\n",
    "        return self.final(h)\n",
    "\n",
    "# Perceptual Loss\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vgg = vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features[:16].to(device).eval()\n",
    "        for param in self.vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.criterion = nn.L1Loss()\n",
    "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
    "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input = (input - self.mean) / self.std\n",
    "        target = (target - self.mean) / self.std\n",
    "        input_features = self.vgg(input)\n",
    "        target_features = self.vgg(target)\n",
    "        return self.criterion(input_features, target_features)\n",
    "\n",
    "# CVAE Model\n",
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = EnhancedEncoder().to(device)\n",
    "        self.decoder = EnhancedDecoder().to(device)\n",
    "        self.perceptual_loss = PerceptualLoss().to(device)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        mu, logvar, skips = self.encoder(x, y)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decoder(z, y, skips)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "    def loss_function(self, x, x_recon, mu, logvar, beta=1.0):\n",
    "        recon_loss = F.l1_loss(x_recon, x, reduction='sum')\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        percep_loss = self.perceptual_loss(x_recon, x) * perceptual_weight\n",
    "        total_loss = recon_weight * recon_loss + beta * kl_loss + percep_loss\n",
    "        return total_loss, recon_loss, kl_loss, percep_loss\n",
    "\n",
    "# Training function\n",
    "def train():\n",
    "    cvae = ConditionalVAE().to(device)\n",
    "    optimizer = optim.Adam(cvae.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=50, factor=0.5)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        beta = min(beta_max, beta_max * (epoch / annealing_epochs))\n",
    "        total_loss = 0\n",
    "        recon_loss = 0\n",
    "        kl_loss = 0\n",
    "        percep_loss = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        for batch_idx, (data, labels) in enumerate(pbar):\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            x_recon, mu, logvar = cvae(data, labels)\n",
    "            loss, r_loss, k_loss, p_loss = cvae.loss_function(data, x_recon, mu, logvar, beta)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(cvae.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            recon_loss += r_loss.item()\n",
    "            kl_loss += k_loss.item()\n",
    "            percep_loss += p_loss.item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'Loss': total_loss/(batch_idx+1),\n",
    "                'Recon': recon_loss/(batch_idx+1),\n",
    "                'KL': kl_loss/(batch_idx+1),\n",
    "                'Percep': percep_loss/(batch_idx+1),\n",
    "                'Beta': beta\n",
    "            })\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            checkpoint_path = os.path.join(output_dir, \"checkpoints\", f\"cvae_epoch_{epoch+1}.pth\")\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': cvae.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "            }, checkpoint_path)\n",
    "            generate_samples(cvae, epoch + 1)\n",
    "    \n",
    "    final_path = os.path.join(output_dir, \"cvae_final.pth\")\n",
    "    torch.save(cvae.state_dict(), final_path)\n",
    "\n",
    "# Sample generation function\n",
    "def generate_samples(model, epoch, num_samples=100):\n",
    "    model.eval()\n",
    "    os.makedirs(os.path.join(output_dir, \"samples\", f\"epoch_{epoch}\"), exist_ok=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for class_idx in range(num_classes):\n",
    "            z = torch.randn(num_samples, latent_dim).to(device)\n",
    "            labels = torch.full((num_samples,), class_idx, dtype=torch.long).to(device)\n",
    "            dummy_skips = (\n",
    "                torch.zeros(num_samples, 64, 64, 64).to(device),\n",
    "                torch.zeros(num_samples, 128, 32, 32).to(device),\n",
    "                torch.zeros(num_samples, 256, 16, 16).to(device),\n",
    "                torch.zeros(num_samples, 512, 8, 8).to(device)\n",
    "            )\n",
    "            samples = model.decoder(z, labels, dummy_skips)\n",
    "            samples = (samples + 1) / 2\n",
    "            \n",
    "            class_dir = os.path.join(output_dir, \"samples\", f\"epoch_{epoch}\", f\"class_{class_idx}\")\n",
    "            os.makedirs(class_dir, exist_ok=True)\n",
    "            \n",
    "            for i in range(num_samples):\n",
    "                save_image(samples[i], os.path.join(class_dir, f\"sample_{i}.png\"))\n",
    "    \n",
    "    print(f\"Generated samples for epoch {epoch}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting training...\")\n",
    "    train()\n",
    "    print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
