{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ec7dab7-3eae-41d1-b3e4-e777d9dafc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic cave with 500epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89af4ce-0013-4dcd-ab2b-fa2a0da508e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Searching for images in C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Found 4793 images across 5 classes.\n",
      "Classes: ['Hatchback', 'Other', 'Pickup', 'Seden', 'SUV']\n",
      "Epoch 1/500, Loss: 30144.3848\n",
      "Epoch 2/500, Loss: 28314.4293\n",
      "Epoch 3/500, Loss: 27927.6297\n",
      "Epoch 4/500, Loss: 27679.7753\n",
      "Epoch 5/500, Loss: 27499.5243\n",
      "Epoch 6/500, Loss: 27365.2830\n",
      "Epoch 7/500, Loss: 27253.1438\n",
      "Epoch 8/500, Loss: 27167.9315\n",
      "Epoch 9/500, Loss: 27077.4151\n",
      "Epoch 10/500, Loss: 27007.5056\n",
      "Epoch 11/500, Loss: 26909.6327\n",
      "Epoch 12/500, Loss: 26854.3746\n",
      "Epoch 13/500, Loss: 26799.4794\n",
      "Epoch 14/500, Loss: 26761.7552\n",
      "Epoch 15/500, Loss: 26714.2218\n",
      "Epoch 16/500, Loss: 26690.3673\n",
      "Epoch 17/500, Loss: 26626.9966\n",
      "Epoch 18/500, Loss: 26601.9060\n",
      "Epoch 19/500, Loss: 26567.9397\n",
      "Epoch 20/500, Loss: 26543.2446\n",
      "Epoch 21/500, Loss: 26527.5651\n",
      "Epoch 22/500, Loss: 26494.2508\n",
      "Epoch 23/500, Loss: 26480.1469\n",
      "Epoch 24/500, Loss: 26453.4253\n",
      "Epoch 25/500, Loss: 26430.5158\n",
      "Epoch 26/500, Loss: 26394.4692\n",
      "Epoch 27/500, Loss: 26390.5198\n",
      "Epoch 28/500, Loss: 26365.4848\n",
      "Epoch 29/500, Loss: 26357.4653\n",
      "Epoch 30/500, Loss: 26343.2774\n",
      "Epoch 31/500, Loss: 26324.6208\n",
      "Epoch 32/500, Loss: 26340.3693\n",
      "Epoch 33/500, Loss: 26306.5965\n",
      "Epoch 34/500, Loss: 26284.2629\n",
      "Epoch 35/500, Loss: 26296.9045\n",
      "Epoch 36/500, Loss: 26267.3668\n",
      "Epoch 37/500, Loss: 26275.0584\n",
      "Epoch 38/500, Loss: 26247.2397\n",
      "Epoch 39/500, Loss: 26228.9165\n",
      "Epoch 40/500, Loss: 26238.1614\n",
      "Epoch 41/500, Loss: 26228.8301\n",
      "Epoch 42/500, Loss: 26214.4651\n",
      "Epoch 43/500, Loss: 26206.7084\n",
      "Epoch 44/500, Loss: 26181.2452\n",
      "Epoch 45/500, Loss: 26190.8832\n",
      "Epoch 46/500, Loss: 26178.7636\n",
      "Epoch 47/500, Loss: 26184.9509\n",
      "Epoch 48/500, Loss: 26169.3601\n",
      "Epoch 49/500, Loss: 26163.4980\n",
      "Epoch 50/500, Loss: 26149.3728\n",
      "Epoch 51/500, Loss: 26151.7666\n",
      "Epoch 52/500, Loss: 26148.1417\n",
      "Epoch 53/500, Loss: 26129.2409\n",
      "Epoch 54/500, Loss: 26136.3616\n",
      "Epoch 55/500, Loss: 26120.7383\n",
      "Epoch 56/500, Loss: 26118.9834\n",
      "Epoch 57/500, Loss: 26117.0467\n",
      "Epoch 58/500, Loss: 26099.9287\n",
      "Epoch 59/500, Loss: 26117.9349\n",
      "Epoch 60/500, Loss: 26116.9377\n",
      "Epoch 61/500, Loss: 26082.3564\n",
      "Epoch 62/500, Loss: 26094.3748\n",
      "Epoch 63/500, Loss: 26077.8439\n",
      "Epoch 64/500, Loss: 26077.8355\n",
      "Epoch 65/500, Loss: 26073.4474\n",
      "Epoch 66/500, Loss: 26076.2695\n",
      "Epoch 67/500, Loss: 26064.7172\n",
      "Epoch 68/500, Loss: 26064.0348\n",
      "Epoch 69/500, Loss: 26066.4096\n",
      "Epoch 70/500, Loss: 26061.8595\n",
      "Epoch 71/500, Loss: 26055.6032\n",
      "Epoch 72/500, Loss: 26050.4904\n",
      "Epoch 73/500, Loss: 26043.6213\n",
      "Epoch 74/500, Loss: 26045.2945\n",
      "Epoch 75/500, Loss: 26036.8982\n",
      "Epoch 76/500, Loss: 26035.7226\n",
      "Epoch 77/500, Loss: 26046.3781\n",
      "Epoch 78/500, Loss: 26037.5965\n",
      "Epoch 79/500, Loss: 26027.4376\n",
      "Epoch 80/500, Loss: 26030.1531\n",
      "Epoch 81/500, Loss: 26027.3806\n",
      "Epoch 82/500, Loss: 26014.5790\n",
      "Epoch 83/500, Loss: 26015.4097\n",
      "Epoch 84/500, Loss: 26009.3552\n",
      "Epoch 85/500, Loss: 26010.3496\n",
      "Epoch 86/500, Loss: 26008.6962\n",
      "Epoch 87/500, Loss: 26004.9689\n",
      "Epoch 88/500, Loss: 26008.1363\n",
      "Epoch 89/500, Loss: 26014.9724\n",
      "Epoch 90/500, Loss: 25992.0070\n",
      "Epoch 91/500, Loss: 25994.4982\n",
      "Epoch 92/500, Loss: 26003.8140\n",
      "Epoch 93/500, Loss: 25991.7171\n",
      "Epoch 94/500, Loss: 25993.4371\n",
      "Epoch 95/500, Loss: 25987.6840\n",
      "Epoch 96/500, Loss: 25980.8441\n",
      "Epoch 97/500, Loss: 25984.7466\n",
      "Epoch 98/500, Loss: 25977.6921\n",
      "Epoch 99/500, Loss: 25983.1188\n",
      "Epoch 100/500, Loss: 25974.3829\n",
      "Epoch 101/500, Loss: 25977.6466\n",
      "Epoch 102/500, Loss: 25966.1678\n",
      "Epoch 103/500, Loss: 25985.2865\n",
      "Epoch 104/500, Loss: 25970.9517\n",
      "Epoch 105/500, Loss: 25965.4884\n",
      "Epoch 106/500, Loss: 25961.7222\n",
      "Epoch 107/500, Loss: 25963.2693\n",
      "Epoch 108/500, Loss: 25968.3171\n",
      "Epoch 109/500, Loss: 25960.7252\n",
      "Epoch 110/500, Loss: 25949.5256\n",
      "Epoch 111/500, Loss: 25955.9908\n",
      "Epoch 112/500, Loss: 25948.2537\n",
      "Epoch 113/500, Loss: 25951.3181\n",
      "Epoch 114/500, Loss: 25952.7468\n",
      "Epoch 115/500, Loss: 25952.6508\n",
      "Epoch 116/500, Loss: 25963.8131\n",
      "Epoch 117/500, Loss: 25957.7241\n",
      "Epoch 118/500, Loss: 25941.1854\n",
      "Epoch 119/500, Loss: 25933.2439\n",
      "Epoch 120/500, Loss: 25938.6579\n",
      "Epoch 121/500, Loss: 25941.2886\n",
      "Epoch 122/500, Loss: 25938.9075\n",
      "Epoch 123/500, Loss: 25942.1086\n",
      "Epoch 124/500, Loss: 25937.3414\n",
      "Epoch 125/500, Loss: 25934.9924\n",
      "Epoch 126/500, Loss: 25951.8089\n",
      "Epoch 127/500, Loss: 25927.5519\n",
      "Epoch 128/500, Loss: 25919.8877\n",
      "Epoch 129/500, Loss: 25923.1712\n",
      "Epoch 130/500, Loss: 25930.1805\n",
      "Epoch 131/500, Loss: 25928.6928\n",
      "Epoch 132/500, Loss: 25922.3157\n",
      "Epoch 133/500, Loss: 25925.6759\n",
      "Epoch 134/500, Loss: 25921.2778\n",
      "Epoch 135/500, Loss: 25921.2908\n",
      "Epoch 136/500, Loss: 25921.8672\n",
      "Epoch 137/500, Loss: 25922.6744\n",
      "Epoch 138/500, Loss: 25915.9297\n",
      "Epoch 139/500, Loss: 25913.2569\n",
      "Epoch 140/500, Loss: 25913.8756\n",
      "Epoch 141/500, Loss: 25914.7063\n",
      "Epoch 142/500, Loss: 25918.4670\n",
      "Epoch 143/500, Loss: 25933.2099\n",
      "Epoch 144/500, Loss: 25905.5273\n",
      "Epoch 145/500, Loss: 25906.0365\n",
      "Epoch 146/500, Loss: 25898.2001\n",
      "Epoch 147/500, Loss: 25903.5138\n",
      "Epoch 148/500, Loss: 25900.8752\n",
      "Epoch 149/500, Loss: 25903.9072\n",
      "Epoch 150/500, Loss: 25905.8618\n",
      "Epoch 151/500, Loss: 25908.9779\n",
      "Epoch 152/500, Loss: 25905.5870\n",
      "Epoch 153/500, Loss: 25897.5762\n",
      "Epoch 154/500, Loss: 25892.6370\n",
      "Epoch 155/500, Loss: 25898.3982\n",
      "Epoch 156/500, Loss: 25903.1731\n",
      "Epoch 157/500, Loss: 25896.9328\n",
      "Epoch 158/500, Loss: 25903.5750\n",
      "Epoch 159/500, Loss: 25889.4393\n",
      "Epoch 160/500, Loss: 25890.3340\n",
      "Epoch 161/500, Loss: 25893.0467\n",
      "Epoch 162/500, Loss: 25892.6692\n",
      "Epoch 163/500, Loss: 25890.8075\n",
      "Epoch 164/500, Loss: 25888.0222\n",
      "Epoch 165/500, Loss: 25901.5424\n",
      "Epoch 166/500, Loss: 25885.4073\n",
      "Epoch 167/500, Loss: 25886.1380\n",
      "Epoch 168/500, Loss: 25882.5609\n",
      "Epoch 169/500, Loss: 25880.1764\n",
      "Epoch 170/500, Loss: 25884.5509\n",
      "Epoch 171/500, Loss: 25883.3865\n",
      "Epoch 172/500, Loss: 25885.4735\n",
      "Epoch 173/500, Loss: 25889.0814\n",
      "Epoch 174/500, Loss: 25878.2079\n",
      "Epoch 175/500, Loss: 25905.8262\n",
      "Epoch 176/500, Loss: 25877.4997\n",
      "Epoch 177/500, Loss: 25873.2479\n",
      "Epoch 178/500, Loss: 25870.8595\n",
      "Epoch 179/500, Loss: 25871.7131\n",
      "Epoch 180/500, Loss: 25873.0347\n",
      "Epoch 181/500, Loss: 25869.8312\n",
      "Epoch 182/500, Loss: 25867.7071\n",
      "Epoch 183/500, Loss: 25871.2674\n",
      "Epoch 184/500, Loss: 25871.0450\n",
      "Epoch 185/500, Loss: 25876.1227\n",
      "Epoch 186/500, Loss: 25870.0306\n",
      "Epoch 187/500, Loss: 25882.7239\n",
      "Epoch 188/500, Loss: 25872.3081\n",
      "Epoch 189/500, Loss: 25867.6723\n",
      "Epoch 190/500, Loss: 25861.2476\n",
      "Epoch 191/500, Loss: 25865.5343\n",
      "Epoch 192/500, Loss: 25865.1383\n",
      "Epoch 193/500, Loss: 25860.2623\n",
      "Epoch 194/500, Loss: 25869.3313\n",
      "Epoch 195/500, Loss: 25881.9639\n",
      "Epoch 196/500, Loss: 25859.9496\n",
      "Epoch 197/500, Loss: 25870.0717\n",
      "Epoch 198/500, Loss: 25861.8915\n",
      "Epoch 199/500, Loss: 25853.8815\n",
      "Epoch 200/500, Loss: 25856.0946\n",
      "Epoch 201/500, Loss: 25865.4692\n",
      "Epoch 202/500, Loss: 25867.3028\n",
      "Epoch 203/500, Loss: 25855.8289\n",
      "Epoch 204/500, Loss: 25862.8212\n",
      "Epoch 205/500, Loss: 25853.8735\n",
      "Epoch 206/500, Loss: 25849.6871\n",
      "Epoch 207/500, Loss: 25852.6599\n",
      "Epoch 208/500, Loss: 25852.7121\n",
      "Epoch 209/500, Loss: 25853.0267\n",
      "Epoch 210/500, Loss: 25857.5804\n",
      "Epoch 211/500, Loss: 25857.0252\n",
      "Epoch 212/500, Loss: 25853.4277\n",
      "Epoch 213/500, Loss: 25860.5318\n",
      "Epoch 214/500, Loss: 25850.0321\n",
      "Epoch 215/500, Loss: 25855.3177\n",
      "Epoch 216/500, Loss: 25848.5378\n",
      "Epoch 217/500, Loss: 25851.9931\n",
      "Epoch 218/500, Loss: 25844.2712\n",
      "Epoch 219/500, Loss: 25849.6215\n",
      "Epoch 220/500, Loss: 25849.9784\n",
      "Epoch 221/500, Loss: 25846.8749\n",
      "Epoch 222/500, Loss: 25849.8744\n",
      "Epoch 223/500, Loss: 25853.6606\n",
      "Epoch 224/500, Loss: 25846.3734\n",
      "Epoch 225/500, Loss: 25844.5827\n",
      "Epoch 226/500, Loss: 25839.1098\n",
      "Epoch 227/500, Loss: 25847.7305\n",
      "Epoch 228/500, Loss: 25852.4346\n",
      "Epoch 229/500, Loss: 25839.1392\n",
      "Epoch 230/500, Loss: 25855.0228\n",
      "Epoch 231/500, Loss: 25843.3442\n",
      "Epoch 232/500, Loss: 25833.7669\n",
      "Epoch 233/500, Loss: 25834.6790\n",
      "Epoch 234/500, Loss: 25843.3108\n",
      "Epoch 235/500, Loss: 25845.4240\n",
      "Epoch 236/500, Loss: 25834.9254\n",
      "Epoch 237/500, Loss: 25837.3504\n",
      "Epoch 238/500, Loss: 25831.5797\n",
      "Epoch 239/500, Loss: 25834.8397\n",
      "Epoch 240/500, Loss: 25838.4886\n",
      "Epoch 241/500, Loss: 25834.1194\n",
      "Epoch 242/500, Loss: 25848.6672\n",
      "Epoch 243/500, Loss: 25846.0095\n",
      "Epoch 244/500, Loss: 25841.1690\n",
      "Epoch 245/500, Loss: 25827.7942\n",
      "Epoch 246/500, Loss: 25828.5581\n",
      "Epoch 247/500, Loss: 25829.2352\n",
      "Epoch 248/500, Loss: 25829.2065\n",
      "Epoch 249/500, Loss: 25837.3227\n",
      "Epoch 250/500, Loss: 25833.2801\n",
      "Epoch 251/500, Loss: 25842.6698\n",
      "Epoch 252/500, Loss: 25832.1663\n",
      "Epoch 253/500, Loss: 25828.3184\n",
      "Epoch 254/500, Loss: 25830.2583\n",
      "Epoch 255/500, Loss: 25827.3786\n",
      "Epoch 256/500, Loss: 25830.3311\n",
      "Epoch 257/500, Loss: 25829.2323\n",
      "Epoch 258/500, Loss: 25832.0010\n",
      "Epoch 259/500, Loss: 25833.4962\n",
      "Epoch 260/500, Loss: 25830.5418\n",
      "Epoch 261/500, Loss: 25838.6914\n",
      "Epoch 262/500, Loss: 25829.8063\n",
      "Epoch 263/500, Loss: 25835.8437\n",
      "Epoch 264/500, Loss: 25832.8375\n",
      "Epoch 265/500, Loss: 25814.6347\n",
      "Epoch 266/500, Loss: 25819.6233\n",
      "Epoch 267/500, Loss: 25824.5936\n",
      "Epoch 268/500, Loss: 25819.8012\n",
      "Epoch 269/500, Loss: 25824.2500\n",
      "Epoch 270/500, Loss: 25821.5802\n",
      "Epoch 271/500, Loss: 25818.9787\n",
      "Epoch 272/500, Loss: 25836.1371\n",
      "Epoch 273/500, Loss: 25823.9971\n",
      "Epoch 274/500, Loss: 25816.0493\n",
      "Epoch 275/500, Loss: 25817.2005\n",
      "Epoch 276/500, Loss: 25820.0346\n",
      "Epoch 277/500, Loss: 25819.4408\n",
      "Epoch 278/500, Loss: 25823.6963\n",
      "Epoch 279/500, Loss: 25824.7040\n",
      "Epoch 280/500, Loss: 25815.8929\n",
      "Epoch 281/500, Loss: 25822.0004\n",
      "Epoch 282/500, Loss: 25817.8566\n",
      "Epoch 283/500, Loss: 25824.0536\n",
      "Epoch 284/500, Loss: 25822.7038\n",
      "Epoch 285/500, Loss: 25810.1266\n",
      "Epoch 286/500, Loss: 25818.9085\n",
      "Epoch 287/500, Loss: 25816.5222\n",
      "Epoch 288/500, Loss: 25817.9613\n",
      "Epoch 289/500, Loss: 25815.2768\n",
      "Epoch 290/500, Loss: 25814.6726\n",
      "Epoch 291/500, Loss: 25817.3091\n",
      "Epoch 292/500, Loss: 25817.0473\n",
      "Epoch 293/500, Loss: 25818.1315\n",
      "Epoch 294/500, Loss: 25814.5715\n",
      "Epoch 295/500, Loss: 25807.8460\n",
      "Epoch 296/500, Loss: 25814.4867\n",
      "Epoch 297/500, Loss: 25817.0087\n",
      "Epoch 298/500, Loss: 25819.4839\n",
      "Epoch 299/500, Loss: 25816.4216\n",
      "Epoch 300/500, Loss: 25816.9761\n",
      "Epoch 301/500, Loss: 25806.4740\n",
      "Epoch 302/500, Loss: 25809.8493\n",
      "Epoch 303/500, Loss: 25812.7662\n",
      "Epoch 304/500, Loss: 25809.1144\n",
      "Epoch 305/500, Loss: 25806.2894\n",
      "Epoch 306/500, Loss: 25810.4153\n",
      "Epoch 307/500, Loss: 25807.6768\n",
      "Epoch 308/500, Loss: 25819.5101\n",
      "Epoch 309/500, Loss: 25807.8150\n",
      "Epoch 310/500, Loss: 25805.6920\n",
      "Epoch 311/500, Loss: 25808.5711\n",
      "Epoch 312/500, Loss: 25807.3018\n",
      "Epoch 313/500, Loss: 25811.3923\n",
      "Epoch 314/500, Loss: 25813.5266\n",
      "Epoch 315/500, Loss: 25815.1586\n",
      "Epoch 316/500, Loss: 25803.0125\n",
      "Epoch 317/500, Loss: 25803.9289\n",
      "Epoch 318/500, Loss: 25803.7211\n",
      "Epoch 319/500, Loss: 25801.9495\n",
      "Epoch 320/500, Loss: 25824.5970\n",
      "Epoch 321/500, Loss: 25812.1990\n",
      "Epoch 322/500, Loss: 25805.0430\n",
      "Epoch 323/500, Loss: 25800.9028\n",
      "Epoch 324/500, Loss: 25798.6226\n",
      "Epoch 325/500, Loss: 25803.2571\n",
      "Epoch 326/500, Loss: 25806.4449\n",
      "Epoch 327/500, Loss: 25813.2102\n",
      "Epoch 328/500, Loss: 25804.8100\n",
      "Epoch 329/500, Loss: 25807.4351\n",
      "Epoch 330/500, Loss: 25797.1154\n",
      "Epoch 331/500, Loss: 25798.9516\n",
      "Epoch 332/500, Loss: 25800.6993\n",
      "Epoch 333/500, Loss: 25800.3155\n",
      "Epoch 334/500, Loss: 25803.0448\n",
      "Epoch 335/500, Loss: 25800.0917\n",
      "Epoch 336/500, Loss: 25803.9176\n",
      "Epoch 337/500, Loss: 25803.9336\n",
      "Epoch 338/500, Loss: 25801.3351\n",
      "Epoch 339/500, Loss: 25802.3610\n",
      "Epoch 340/500, Loss: 25801.2218\n",
      "Epoch 341/500, Loss: 25808.3225\n",
      "Epoch 342/500, Loss: 25797.0359\n",
      "Epoch 343/500, Loss: 25798.1071\n",
      "Epoch 344/500, Loss: 25796.1589\n",
      "Epoch 345/500, Loss: 25798.1425\n",
      "Epoch 346/500, Loss: 25795.6633\n",
      "Epoch 347/500, Loss: 25798.4050\n",
      "Epoch 348/500, Loss: 25794.7305\n",
      "Epoch 349/500, Loss: 25800.2356\n",
      "Epoch 350/500, Loss: 25805.6876\n",
      "Epoch 351/500, Loss: 25796.9994\n",
      "Epoch 352/500, Loss: 25795.2817\n",
      "Epoch 353/500, Loss: 25792.0092\n",
      "Epoch 354/500, Loss: 25798.4388\n",
      "Epoch 355/500, Loss: 25794.5525\n",
      "Epoch 356/500, Loss: 25793.6022\n",
      "Epoch 357/500, Loss: 25795.8863\n",
      "Epoch 358/500, Loss: 25798.8542\n",
      "Epoch 359/500, Loss: 25795.1729\n",
      "Epoch 360/500, Loss: 25796.2812\n",
      "Epoch 361/500, Loss: 25790.8014\n",
      "Epoch 362/500, Loss: 25798.6490\n",
      "Epoch 363/500, Loss: 25795.3621\n",
      "Epoch 364/500, Loss: 25796.4786\n",
      "Epoch 365/500, Loss: 25789.4172\n",
      "Epoch 366/500, Loss: 25791.2329\n",
      "Epoch 367/500, Loss: 25788.7899\n",
      "Epoch 368/500, Loss: 25792.4261\n",
      "Epoch 369/500, Loss: 25795.3898\n",
      "Epoch 370/500, Loss: 25795.4048\n",
      "Epoch 371/500, Loss: 25792.8754\n",
      "Epoch 372/500, Loss: 25790.0875\n",
      "Epoch 373/500, Loss: 25785.6662\n",
      "Epoch 374/500, Loss: 25791.4399\n",
      "Epoch 375/500, Loss: 25796.2833\n",
      "Epoch 376/500, Loss: 25794.7098\n",
      "Epoch 377/500, Loss: 25800.8603\n",
      "Epoch 378/500, Loss: 25792.3467\n",
      "Epoch 379/500, Loss: 25780.4494\n",
      "Epoch 380/500, Loss: 25783.7490\n",
      "Epoch 381/500, Loss: 25789.9521\n",
      "Epoch 382/500, Loss: 25788.2080\n",
      "Epoch 383/500, Loss: 25789.5783\n",
      "Epoch 384/500, Loss: 25787.5776\n",
      "Epoch 385/500, Loss: 25798.4970\n",
      "Epoch 386/500, Loss: 25794.2643\n",
      "Epoch 387/500, Loss: 25786.2351\n",
      "Epoch 388/500, Loss: 25786.3682\n",
      "Epoch 389/500, Loss: 25786.6111\n",
      "Epoch 390/500, Loss: 25783.5518\n",
      "Epoch 391/500, Loss: 25791.5109\n",
      "Epoch 392/500, Loss: 25793.9078\n",
      "Epoch 393/500, Loss: 25788.6456\n",
      "Epoch 394/500, Loss: 25781.2534\n",
      "Epoch 395/500, Loss: 25779.1741\n",
      "Epoch 396/500, Loss: 25782.4551\n",
      "Epoch 397/500, Loss: 25786.9594\n",
      "Epoch 398/500, Loss: 25789.5890\n",
      "Epoch 399/500, Loss: 25787.0713\n",
      "Epoch 400/500, Loss: 25784.8357\n",
      "Epoch 401/500, Loss: 25785.5985\n",
      "Epoch 402/500, Loss: 25789.3669\n",
      "Epoch 403/500, Loss: 25785.4217\n",
      "Epoch 404/500, Loss: 25782.4568\n",
      "Epoch 405/500, Loss: 25778.3613\n",
      "Epoch 406/500, Loss: 25787.6826\n",
      "Epoch 407/500, Loss: 25787.3333\n",
      "Epoch 408/500, Loss: 25783.9685\n",
      "Epoch 409/500, Loss: 25780.4859\n",
      "Epoch 410/500, Loss: 25782.7450\n",
      "Epoch 411/500, Loss: 25782.8279\n",
      "Epoch 412/500, Loss: 25780.0944\n",
      "Epoch 413/500, Loss: 25781.9214\n",
      "Epoch 414/500, Loss: 25786.6585\n",
      "Epoch 415/500, Loss: 25781.3893\n",
      "Epoch 416/500, Loss: 25781.0459\n",
      "Epoch 417/500, Loss: 25782.6579\n",
      "Epoch 418/500, Loss: 25778.3115\n",
      "Epoch 419/500, Loss: 25778.6940\n",
      "Epoch 420/500, Loss: 25776.9516\n",
      "Epoch 421/500, Loss: 25786.0099\n",
      "Epoch 422/500, Loss: 25782.3452\n",
      "Epoch 423/500, Loss: 25777.7629\n",
      "Epoch 424/500, Loss: 25794.0310\n",
      "Epoch 425/500, Loss: 25777.2076\n",
      "Epoch 426/500, Loss: 25774.6693\n",
      "Epoch 427/500, Loss: 25777.7074\n",
      "Epoch 428/500, Loss: 25778.1568\n",
      "Epoch 429/500, Loss: 25779.9890\n",
      "Epoch 430/500, Loss: 25780.6140\n",
      "Epoch 431/500, Loss: 25777.2553\n",
      "Epoch 432/500, Loss: 25777.7244\n",
      "Epoch 433/500, Loss: 25777.0315\n",
      "Epoch 434/500, Loss: 25783.3308\n",
      "Epoch 435/500, Loss: 25789.4088\n",
      "Epoch 436/500, Loss: 25788.3566\n",
      "Epoch 437/500, Loss: 25773.4918\n",
      "Epoch 438/500, Loss: 25770.9134\n",
      "Epoch 439/500, Loss: 25772.1535\n",
      "Epoch 440/500, Loss: 25773.8214\n",
      "Epoch 441/500, Loss: 25781.1658\n",
      "Epoch 442/500, Loss: 25793.6664\n",
      "Epoch 443/500, Loss: 25777.9551\n",
      "Epoch 444/500, Loss: 25769.6491\n",
      "Epoch 445/500, Loss: 25773.4334\n",
      "Epoch 446/500, Loss: 25777.1752\n",
      "Epoch 447/500, Loss: 25770.9789\n",
      "Epoch 448/500, Loss: 25772.2145\n",
      "Epoch 449/500, Loss: 25775.1132\n",
      "Epoch 450/500, Loss: 25775.9343\n",
      "Epoch 451/500, Loss: 25771.8142\n",
      "Epoch 452/500, Loss: 25771.4335\n",
      "Epoch 453/500, Loss: 25772.7291\n",
      "Epoch 454/500, Loss: 25782.5984\n",
      "Epoch 455/500, Loss: 25777.4239\n",
      "Epoch 456/500, Loss: 25772.6300\n",
      "Epoch 457/500, Loss: 25779.8022\n",
      "Epoch 458/500, Loss: 25774.2034\n",
      "Epoch 459/500, Loss: 25769.8806\n",
      "Epoch 460/500, Loss: 25773.9273\n",
      "Epoch 461/500, Loss: 25777.7129\n",
      "Epoch 462/500, Loss: 25774.0641\n",
      "Epoch 463/500, Loss: 25768.9720\n",
      "Epoch 464/500, Loss: 25772.4096\n",
      "Epoch 465/500, Loss: 25774.0558\n",
      "Epoch 466/500, Loss: 25773.3790\n",
      "Epoch 467/500, Loss: 25772.4968\n",
      "Epoch 468/500, Loss: 25775.0312\n",
      "Epoch 469/500, Loss: 25767.5824\n",
      "Epoch 470/500, Loss: 25777.0216\n",
      "Epoch 471/500, Loss: 25773.0374\n",
      "Epoch 472/500, Loss: 25767.6810\n",
      "Epoch 473/500, Loss: 25768.0785\n",
      "Epoch 474/500, Loss: 25768.6336\n",
      "Epoch 475/500, Loss: 25773.1665\n",
      "Epoch 476/500, Loss: 25778.2426\n",
      "Epoch 477/500, Loss: 25767.4698\n",
      "Epoch 478/500, Loss: 25773.4584\n",
      "Epoch 479/500, Loss: 25779.3739\n",
      "Epoch 480/500, Loss: 25769.5038\n",
      "Epoch 481/500, Loss: 25772.6738\n",
      "Epoch 482/500, Loss: 25764.9939\n",
      "Epoch 483/500, Loss: 25761.1155\n",
      "Epoch 484/500, Loss: 25766.1664\n",
      "Epoch 485/500, Loss: 25768.9311\n",
      "Epoch 486/500, Loss: 25771.6283\n",
      "Epoch 487/500, Loss: 25767.7867\n",
      "Epoch 488/500, Loss: 25770.7895\n",
      "Epoch 489/500, Loss: 25771.4253\n",
      "Epoch 490/500, Loss: 25771.1077\n",
      "Epoch 491/500, Loss: 25765.4872\n",
      "Epoch 492/500, Loss: 25769.7298\n",
      "Epoch 493/500, Loss: 25764.2550\n",
      "Epoch 494/500, Loss: 25765.5323\n",
      "Epoch 495/500, Loss: 25763.2809\n",
      "Epoch 496/500, Loss: 25767.2010\n",
      "Epoch 497/500, Loss: 25775.0316\n",
      "Epoch 498/500, Loss: 25771.9413\n",
      "Epoch 499/500, Loss: 25769.1566\n",
      "Epoch 500/500, Loss: 25765.6220\n",
      "Saved CVAE model to FL_VEHICLE_NON_IID\\cvae_vehicle.pth\n",
      "Generated 500 samples for Class 3 (Seden).\n",
      "Generated 500 samples for Class 4 (SUV).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import kagglehub\n",
    "from PIL import Image\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 16\n",
    "num_classes = 5\n",
    "batch_size = 32\n",
    "epochs = 500\n",
    "learning_rate = 1e-3\n",
    "image_size = 128\n",
    "channels = 3\n",
    "\n",
    "# Download Vehicle Type Image Dataset from Kaggle\n",
    "try:\n",
    "    path = kagglehub.dataset_download(\"sujaykapadnis/vehicle-type-image-dataset\")\n",
    "    print(\"Path to dataset files:\", path)\n",
    "    dataset_path = path\n",
    "except Exception as e:\n",
    "    print(f\"Failed to download dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Define the VehicleTypeDataset class (from your previous code)\n",
    "class VehicleTypeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_names = []\n",
    "        self.class_to_idx = {}\n",
    "\n",
    "        print(f\"Searching for images in {root_dir}\")\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            image_files = [f for f in files if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "            if image_files:\n",
    "                class_name = os.path.basename(root)\n",
    "                if class_name not in self.class_to_idx:\n",
    "                    self.class_names.append(class_name)\n",
    "                    self.class_to_idx[class_name] = len(self.class_names) - 1\n",
    "                for img_file in image_files:\n",
    "                    img_path = os.path.join(root, img_file)\n",
    "                    self.images.append(img_path)\n",
    "                    self.labels.append(self.class_to_idx[class_name])\n",
    "\n",
    "        if not self.images:\n",
    "            raise ValueError(\n",
    "                f\"No images found in {root_dir}. \"\n",
    "                \"Expected class folders containing .jpg, .png, or .jpeg images.\"\n",
    "            )\n",
    "\n",
    "        print(f\"Found {len(self.images)} images across {len(self.class_names)} classes.\")\n",
    "        print(f\"Classes: {self.class_names}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Define transforms (normalize to [0, 1] for CVAE training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),  # Converts to [0, 1]\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = VehicleTypeDataset(root_dir=dataset_path, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the Encoder network (convolutional)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels + num_classes, 32, kernel_size=4, stride=2, padding=1)  # Output: 32x64x64\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)  # Output: 64x32x32\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)  # Output: 128x16x16\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)  # Output: 256x8x8\n",
    "        self.fc_mean = nn.Linear(256 * 8 * 8, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256 * 8 * 8, latent_dim)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # Expand one-hot labels to match image dimensions and concatenate\n",
    "        y = F.one_hot(y, num_classes=num_classes).float()  # Shape: (batch_size, num_classes)\n",
    "        y = y.unsqueeze(-1).unsqueeze(-1)  # Shape: (batch_size, num_classes, 1, 1)\n",
    "        y = y.expand(-1, -1, x.size(2), x.size(3))  # Shape: (batch_size, num_classes, 128, 128)\n",
    "        x_with_y = torch.cat([x, y], dim=1)  # Shape: (batch_size, channels + num_classes, 128, 128)\n",
    "        \n",
    "        h = F.relu(self.conv1(x_with_y))\n",
    "        h = F.relu(self.conv2(h))\n",
    "        h = F.relu(self.conv3(h))\n",
    "        h = F.relu(self.conv4(h))\n",
    "        h = h.view(h.size(0), -1)  # Flatten: (batch_size, 256*8*8)\n",
    "        z_mean = self.fc_mean(h)\n",
    "        z_logvar = self.fc_logvar(h)\n",
    "        return z_mean, z_logvar\n",
    "\n",
    "# Define the Decoder network (transposed convolutional)\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim + num_classes, 256 * 8 * 8)\n",
    "        self.deconv1 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)  # Output: 128x16x16\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)  # Output: 64x32x32\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)  # Output: 32x64x64\n",
    "        self.deconv4 = nn.ConvTranspose2d(32, channels, kernel_size=4, stride=2, padding=1)  # Output: 3x128x128\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        y = F.one_hot(y, num_classes=num_classes).float()  # Shape: (batch_size, num_classes)\n",
    "        z_with_y = torch.cat([z, y], dim=-1)  # Shape: (batch_size, latent_dim + num_classes)\n",
    "        h = F.relu(self.fc(z_with_y))\n",
    "        h = h.view(h.size(0), 256, 8, 8)  # Reshape: (batch_size, 256, 8, 8)\n",
    "        h = F.relu(self.deconv1(h))\n",
    "        h = F.relu(self.deconv2(h))\n",
    "        h = F.relu(self.deconv3(h))\n",
    "        x_reconstructed = torch.sigmoid(self.deconv4(h))  # Output: (batch_size, 3, 128, 128)\n",
    "        return x_reconstructed\n",
    "\n",
    "# Conditional VAE model\n",
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def reparameterize(self, z_mean, z_logvar):\n",
    "        std = torch.exp(0.5 * z_logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return z_mean + eps * std\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        z_mean, z_logvar = self.encoder(x, y)\n",
    "        z = self.reparameterize(z_mean, z_logvar)\n",
    "        x_reconstructed = self.decoder(z, y)\n",
    "        return x_reconstructed, z_mean, z_logvar\n",
    "\n",
    "# Instantiate Encoder, Decoder, and CVAE\n",
    "encoder = Encoder(latent_dim, num_classes).to(device)\n",
    "decoder = Decoder(latent_dim, num_classes).to(device)\n",
    "cvae = ConditionalVAE(encoder, decoder).to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(cvae.parameters(), lr=learning_rate)\n",
    "\n",
    "def cvae_loss(x, x_reconstructed, z_mean, z_logvar):\n",
    "    recon_loss = F.binary_cross_entropy(x_reconstructed, x, reduction='sum')\n",
    "    kl_loss = -0.5 * torch.sum(1 + z_logvar - z_mean.pow(2) - z_logvar.exp())\n",
    "    return recon_loss + kl_loss\n",
    "\n",
    "# Training loop\n",
    "cvae.train()\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        x_reconstructed, z_mean, z_logvar = cvae(data, labels)\n",
    "        loss = cvae_loss(data, x_reconstructed, z_mean, z_logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {train_loss / len(train_loader.dataset):.4f}')\n",
    "\n",
    "# Save the trained CVAE model\n",
    "output_dir = \"FL_VEHICLE_NON_IID\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "model_path = os.path.join(output_dir, \"cvae_vehicle.pth\")\n",
    "torch.save(cvae.state_dict(), model_path)\n",
    "print(f\"Saved CVAE model to {model_path}\")\n",
    "\n",
    "# Generate samples for Classes 3 and 4\n",
    "def generate_samples_labelwise(cvae, num_samples, classes_to_generate, base_dir, latent_dim, device):\n",
    "    cvae.eval()\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    with torch.no_grad():\n",
    "        for class_label in classes_to_generate:\n",
    "            label_tensor = torch.tensor([class_label]).repeat(num_samples).to(device)\n",
    "            z = torch.randn(num_samples, latent_dim).to(device)\n",
    "            generated_samples = cvae.decoder(z, label_tensor)  # Shape: (num_samples, 3, 128, 128)\n",
    "            \n",
    "            class_dir = os.path.join(base_dir, str(class_label))\n",
    "            os.makedirs(class_dir, exist_ok=True)\n",
    "            for idx, sample in enumerate(generated_samples):\n",
    "                save_image(sample, os.path.join(class_dir, f\"sample_{idx}.png\"))\n",
    "            print(f\"Generated {num_samples} samples for Class {class_label} ({dataset.class_names[class_label]}).\")\n",
    "\n",
    "# Generate 500 samples each for Classes 3 and 4\n",
    "base_dir = os.path.join(output_dir, \"generated_samples\")\n",
    "classes_to_generate = [3, 4]  # Classes 3 and 4\n",
    "generate_samples_labelwise(cvae, num_samples=500, classes_to_generate=classes_to_generate, base_dir=base_dir, latent_dim=latent_dim, device=device)\n",
    "\n",
    "# Plot random samples for Classes 3 and 4\n",
    "def plot_random_samples(base_dir, classes_to_generate, num_images_per_class=10):\n",
    "    fig, axs = plt.subplots(len(classes_to_generate), num_images_per_class, figsize=(20, 4))\n",
    "    for row, class_label in enumerate(classes_to_generate):\n",
    "        class_dir = os.path.join(base_dir, str(class_label))\n",
    "        sample_files = os.listdir(class_dir)\n",
    "        random_samples = np.random.choice(sample_files, num_images_per_class, replace=False)\n",
    "        \n",
    "        for col, sample_file in enumerate(random_samples):\n",
    "            sample_path = os.path.join(class_dir, sample_file)\n",
    "            sample_image = plt.imread(sample_path)  # RGB image\n",
    "            if len(classes_to_generate) == 1:\n",
    "                ax = axs[col]\n",
    "            else:\n",
    "                ax = axs[row, col]\n",
    "            ax.imshow(sample_image)\n",
    "            ax.axis('off')\n",
    "            if col == 0:\n",
    "                ax.set_ylabel(dataset.class_names[class_label], rotation=90, labelpad=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(output_dir, \"synthetic_samples_classes_3_4.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.show()\n",
    "    print(f\"Saved synthetic samples plot to {plot_path}\")\n",
    "\n",
    "# Plot 10 samples each for Classes 3 and 4\n",
    "plot_random_samples(base_dir=base_dir, classes_to_generate=classes_to_generate, num_images_per_class=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f624ec-e3bb-4536-8652-4d66185f592f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e2b0266-04f2-4111-b164-0d051b7b01ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "##RGB--->GREY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec1285f-2e1f-4da3-8f9d-a7ec702f48aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Searching for images in C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Found 4793 images across 5 classes.\n",
      "Classes: ['Hatchback', 'Other', 'Pickup', 'Seden', 'SUV']\n",
      "Epoch 1/1000, Beta: 0.00, Total Loss: 6675.5323, Recon Loss: 13350.9480, KL Loss: 1723.3214, Percep Loss: 0.0582\n",
      "Epoch 2/1000, Beta: 0.20, Total Loss: 4283.7780, Recon Loss: 8566.2397, KL Loss: 3.0640, Percep Loss: 0.0454\n",
      "Epoch 3/1000, Beta: 0.40, Total Loss: 4250.8905, Recon Loss: 8501.7092, KL Loss: 0.0008, Percep Loss: 0.0356\n",
      "Epoch 4/1000, Beta: 0.60, Total Loss: 4239.6933, Recon Loss: 8479.3253, KL Loss: 0.0001, Percep Loss: 0.0306\n",
      "Epoch 5/1000, Beta: 0.80, Total Loss: 4232.8399, Recon Loss: 8465.6227, KL Loss: 0.0001, Percep Loss: 0.0285\n",
      "Epoch 6/1000, Beta: 1.00, Total Loss: 4224.1928, Recon Loss: 8448.3358, KL Loss: 0.0001, Percep Loss: 0.0248\n",
      "Epoch 7/1000, Beta: 1.20, Total Loss: 4220.3307, Recon Loss: 8440.6172, KL Loss: 0.0000, Percep Loss: 0.0220\n",
      "Epoch 8/1000, Beta: 1.40, Total Loss: 4216.3062, Recon Loss: 8432.5700, KL Loss: 0.0000, Percep Loss: 0.0211\n",
      "Epoch 9/1000, Beta: 1.60, Total Loss: 4211.8939, Recon Loss: 8423.7527, KL Loss: 0.0000, Percep Loss: 0.0176\n",
      "Epoch 10/1000, Beta: 1.80, Total Loss: 4210.8257, Recon Loss: 8421.6132, KL Loss: 0.0001, Percep Loss: 0.0190\n",
      "Epoch 11/1000, Beta: 2.00, Total Loss: 4208.5661, Recon Loss: 8417.1008, KL Loss: 0.0000, Percep Loss: 0.0157\n",
      "Epoch 12/1000, Beta: 2.20, Total Loss: 4207.9259, Recon Loss: 8415.8242, KL Loss: 0.0000, Percep Loss: 0.0138\n",
      "Epoch 13/1000, Beta: 2.40, Total Loss: 4207.3215, Recon Loss: 8414.6155, KL Loss: 0.0000, Percep Loss: 0.0138\n",
      "Epoch 14/1000, Beta: 2.60, Total Loss: 4205.3472, Recon Loss: 8410.6670, KL Loss: 0.0000, Percep Loss: 0.0136\n",
      "Epoch 15/1000, Beta: 2.80, Total Loss: 4204.1787, Recon Loss: 8408.3307, KL Loss: 0.0000, Percep Loss: 0.0133\n",
      "Epoch 16/1000, Beta: 3.00, Total Loss: 4204.1188, Recon Loss: 8408.2153, KL Loss: 0.0000, Percep Loss: 0.0111\n",
      "Epoch 17/1000, Beta: 3.20, Total Loss: 4203.4633, Recon Loss: 8406.9038, KL Loss: 0.0000, Percep Loss: 0.0113\n",
      "Epoch 18/1000, Beta: 3.40, Total Loss: 4202.4222, Recon Loss: 8404.8231, KL Loss: 0.0000, Percep Loss: 0.0106\n",
      "Epoch 19/1000, Beta: 3.60, Total Loss: 4202.4167, Recon Loss: 8404.8135, KL Loss: 0.0000, Percep Loss: 0.0099\n",
      "Epoch 20/1000, Beta: 3.80, Total Loss: 4201.8765, Recon Loss: 8403.7335, KL Loss: 0.0000, Percep Loss: 0.0098\n",
      "Epoch 21/1000, Beta: 4.00, Total Loss: 4201.2158, Recon Loss: 8402.4120, KL Loss: 0.0000, Percep Loss: 0.0098\n",
      "Epoch 22/1000, Beta: 4.20, Total Loss: 4200.0554, Recon Loss: 8400.0914, KL Loss: 0.0000, Percep Loss: 0.0096\n",
      "Epoch 23/1000, Beta: 4.40, Total Loss: 4199.7731, Recon Loss: 8399.5270, KL Loss: 0.0000, Percep Loss: 0.0097\n",
      "Epoch 24/1000, Beta: 4.60, Total Loss: 4199.2494, Recon Loss: 8398.4809, KL Loss: 0.0000, Percep Loss: 0.0089\n",
      "Epoch 25/1000, Beta: 4.80, Total Loss: 4199.1106, Recon Loss: 8398.2031, KL Loss: 0.0000, Percep Loss: 0.0089\n",
      "Epoch 26/1000, Beta: 5.00, Total Loss: 4198.9043, Recon Loss: 8397.7906, KL Loss: 0.0000, Percep Loss: 0.0090\n",
      "Epoch 27/1000, Beta: 5.20, Total Loss: 4199.0385, Recon Loss: 8398.0620, KL Loss: 0.0000, Percep Loss: 0.0075\n",
      "Epoch 28/1000, Beta: 5.40, Total Loss: 4198.1874, Recon Loss: 8396.3584, KL Loss: 0.0000, Percep Loss: 0.0081\n",
      "Epoch 29/1000, Beta: 5.60, Total Loss: 4197.7626, Recon Loss: 8395.5103, KL Loss: 0.0000, Percep Loss: 0.0075\n",
      "Epoch 30/1000, Beta: 5.80, Total Loss: 4197.5310, Recon Loss: 8395.0470, KL Loss: 0.0000, Percep Loss: 0.0075\n",
      "Epoch 31/1000, Beta: 6.00, Total Loss: 4197.3292, Recon Loss: 8394.6439, KL Loss: 0.0000, Percep Loss: 0.0072\n",
      "Epoch 32/1000, Beta: 6.20, Total Loss: 4197.4415, Recon Loss: 8394.8683, KL Loss: 0.0000, Percep Loss: 0.0073\n",
      "Epoch 33/1000, Beta: 6.40, Total Loss: 4196.8311, Recon Loss: 8393.6472, KL Loss: 0.0000, Percep Loss: 0.0076\n",
      "Epoch 34/1000, Beta: 6.60, Total Loss: 4196.5381, Recon Loss: 8393.0633, KL Loss: 0.0000, Percep Loss: 0.0064\n",
      "Epoch 35/1000, Beta: 6.80, Total Loss: 4196.5233, Recon Loss: 8393.0326, KL Loss: 0.0000, Percep Loss: 0.0069\n",
      "Epoch 36/1000, Beta: 7.00, Total Loss: 4196.1905, Recon Loss: 8392.3684, KL Loss: 0.0000, Percep Loss: 0.0063\n",
      "Epoch 37/1000, Beta: 7.20, Total Loss: 4196.1222, Recon Loss: 8392.2307, KL Loss: 0.0000, Percep Loss: 0.0069\n",
      "Epoch 38/1000, Beta: 7.40, Total Loss: 4196.0070, Recon Loss: 8392.0022, KL Loss: 0.0000, Percep Loss: 0.0055\n",
      "Epoch 39/1000, Beta: 7.60, Total Loss: 4195.8034, Recon Loss: 8391.5944, KL Loss: 0.0000, Percep Loss: 0.0062\n",
      "Epoch 40/1000, Beta: 7.80, Total Loss: 4195.6776, Recon Loss: 8391.3426, KL Loss: 0.0000, Percep Loss: 0.0063\n",
      "Epoch 41/1000, Beta: 8.00, Total Loss: 4195.3851, Recon Loss: 8390.7589, KL Loss: 0.0000, Percep Loss: 0.0056\n",
      "Epoch 42/1000, Beta: 8.20, Total Loss: 4195.1881, Recon Loss: 8390.3663, KL Loss: 0.0000, Percep Loss: 0.0049\n",
      "Epoch 43/1000, Beta: 8.40, Total Loss: 4195.1676, Recon Loss: 8390.3245, KL Loss: 0.0000, Percep Loss: 0.0053\n",
      "Epoch 44/1000, Beta: 8.60, Total Loss: 4195.0229, Recon Loss: 8390.0358, KL Loss: 0.0000, Percep Loss: 0.0050\n",
      "Epoch 45/1000, Beta: 8.80, Total Loss: 4194.9069, Recon Loss: 8389.8037, KL Loss: 0.0000, Percep Loss: 0.0051\n",
      "Epoch 46/1000, Beta: 9.00, Total Loss: 4194.7409, Recon Loss: 8389.4726, KL Loss: 0.0000, Percep Loss: 0.0046\n",
      "Epoch 47/1000, Beta: 9.20, Total Loss: 4195.0642, Recon Loss: 8390.1190, KL Loss: 0.0000, Percep Loss: 0.0047\n",
      "Epoch 48/1000, Beta: 9.40, Total Loss: 4194.6352, Recon Loss: 8389.2600, KL Loss: 0.0000, Percep Loss: 0.0052\n",
      "Epoch 49/1000, Beta: 9.60, Total Loss: 4194.5405, Recon Loss: 8389.0720, KL Loss: 0.0000, Percep Loss: 0.0045\n",
      "Epoch 50/1000, Beta: 9.80, Total Loss: 4194.4285, Recon Loss: 8388.8487, KL Loss: 0.0000, Percep Loss: 0.0041\n",
      "Saved checkpoint at epoch 50 to FL_CVAE\\cvae_vehicle_epoch_50.pth\n",
      "Generated 500 samples for Class 3 (Seden) at epoch 50.\n",
      "Generated 500 samples for Class 4 (SUV) at epoch 50.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms.functional import adjust_sharpness\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import kagglehub\n",
    "from PIL import Image\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "# Set CUDA_LAUNCH_BLOCKING for debugging\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.device_count() > 1 else 'cuda')\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 128\n",
    "num_classes = 5\n",
    "batch_size = 32\n",
    "epochs = 1000\n",
    "learning_rate = 5e-4\n",
    "image_size = 128\n",
    "channels = 1  # Grayscale\n",
    "output_dir = \"FL_CVAE\"\n",
    "beta_max = 10.0\n",
    "annealing_epochs = 50\n",
    "perceptual_weight = 1.0\n",
    "recon_weight = 0.5\n",
    "\n",
    "# Download Vehicle Type Image Dataset from Kaggle\n",
    "try:\n",
    "    path = kagglehub.dataset_download(\"sujaykapadnis/vehicle-type-image-dataset\")\n",
    "    print(\"Path to dataset files:\", path)\n",
    "    dataset_path = path\n",
    "except Exception as e:\n",
    "    print(f\"Failed to download dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Define the VehicleTypeDataset class\n",
    "class VehicleTypeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_names = []\n",
    "        self.class_to_idx = {}\n",
    "\n",
    "        print(f\"Searching for images in {root_dir}\")\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            image_files = [f for f in files if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "            if image_files:\n",
    "                class_name = os.path.basename(root)\n",
    "                if class_name not in self.class_to_idx:\n",
    "                    self.class_names.append(class_name)\n",
    "                    self.class_to_idx[class_name] = len(self.class_names) - 1\n",
    "                for img_file in image_files:\n",
    "                    img_path = os.path.join(root, img_file)\n",
    "                    self.images.append(img_path)\n",
    "                    self.labels.append(self.class_to_idx[class_name])\n",
    "\n",
    "        if not self.images:\n",
    "            raise ValueError(\n",
    "                f\"No images found in {root_dir}. \"\n",
    "                \"Expected class folders containing .jpg, .png, or .jpeg images.\"\n",
    "            )\n",
    "\n",
    "        print(f\"Found {len(self.images)} images across {len(self.class_names)} classes.\")\n",
    "        print(f\"Classes: {self.class_names}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"L\")  # Convert to grayscale\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),  # Converts to [0, 1]\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = VehicleTypeDataset(root_dir=dataset_path, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the Encoder network\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels + num_classes, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.fc_mean = nn.Linear(512 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(512 * 4 * 4, latent_dim)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        y = F.one_hot(y, num_classes=num_classes).float()\n",
    "        y = y.unsqueeze(-1).unsqueeze(-1)\n",
    "        y = y.expand(-1, -1, x.size(2), x.size(3))\n",
    "        x_with_y = torch.cat([x, y], dim=1)\n",
    "        \n",
    "        h1 = F.relu(self.conv1(x_with_y))\n",
    "        if torch.isnan(h1).any() or torch.isinf(h1).any():\n",
    "            print(\"NaN or Inf in h1\")\n",
    "        h2 = F.relu(self.conv2(h1))\n",
    "        if torch.isnan(h2).any() or torch.isinf(h2).any():\n",
    "            print(\"NaN or Inf in h2\")\n",
    "        h3 = F.relu(self.conv3(h2))\n",
    "        if torch.isnan(h3).any() or torch.isinf(h3).any():\n",
    "            print(\"NaN or Inf in h3\")\n",
    "        h4 = F.relu(self.conv4(h3))\n",
    "        if torch.isnan(h4).any() or torch.isinf(h4).any():\n",
    "            print(\"NaN or Inf in h4\")\n",
    "        h5 = F.relu(self.conv5(h4))\n",
    "        if torch.isnan(h5).any() or torch.isinf(h5).any():\n",
    "            print(\"NaN or Inf in h5\")\n",
    "        h = h5.view(h5.size(0), -1)\n",
    "        z_mean = self.fc_mean(h)\n",
    "        z_logvar = self.fc_logvar(h)\n",
    "        return z_mean, z_logvar, (h1, h2, h3, h4, h5)\n",
    "\n",
    "# Define the Decoder network\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim + num_classes, 512 * 4 * 4)\n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(512, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(256, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(128, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv5 = nn.ConvTranspose2d(64, channels, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, z, y, skip_connections):\n",
    "        h1, h2, h3, h4, h5 = skip_connections\n",
    "        y = F.one_hot(y, num_classes=num_classes).float()\n",
    "        z_with_y = torch.cat([z, y], dim=-1)\n",
    "        h = F.relu(self.fc(z_with_y))\n",
    "        h = h.view(h.size(0), 512, 4, 4)\n",
    "        \n",
    "        h = F.relu(self.deconv1(h))\n",
    "        h = torch.cat([h, h4], dim=1)\n",
    "        h = F.relu(self.deconv2(h))\n",
    "        h = torch.cat([h, h3], dim=1)\n",
    "        h = F.relu(self.deconv3(h))\n",
    "        h = torch.cat([h, h2], dim=1)\n",
    "        h = F.relu(self.deconv4(h))\n",
    "        h = torch.cat([h, h1], dim=1)\n",
    "        x_reconstructed = torch.sigmoid(self.deconv5(h))\n",
    "        if torch.isnan(x_reconstructed).any() or torch.isinf(x_reconstructed).any():\n",
    "            print(\"NaN or Inf in x_reconstructed\")\n",
    "        return x_reconstructed\n",
    "\n",
    "# Convert grayscale to RGB by duplicating channels\n",
    "def grayscale_to_rgb(tensor):\n",
    "    return tensor.repeat(1, 3, 1, 1)  # (batch_size, 1, H, W) -> (batch_size, 3, H, W)\n",
    "\n",
    "# Conditional VAE model\n",
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def reparameterize(self, z_mean, z_logvar):\n",
    "        std = torch.exp(0.5 * z_logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return z_mean + eps * std\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        z_mean, z_logvar, skip_connections = self.encoder(x, y)\n",
    "        z = self.reparameterize(z_mean, z_logvar)\n",
    "        x_reconstructed = self.decoder(z, y, skip_connections)\n",
    "        return x_reconstructed, z_mean, z_logvar\n",
    "\n",
    "# Load pretrained VGG16 for perceptual loss\n",
    "vgg = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features.to(device).eval()\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "def perceptual_loss(x, x_reconstructed):\n",
    "    # Convert grayscale to RGB for VGG\n",
    "    x_rgb = grayscale_to_rgb(x)\n",
    "    x_reconstructed_rgb = grayscale_to_rgb(x_reconstructed)\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(x.device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(x.device)\n",
    "    x_normalized = (x_rgb - mean) / std\n",
    "    x_reconstructed_normalized = (x_reconstructed_rgb - mean) / std\n",
    "    x_features = vgg(x_normalized)\n",
    "    x_recon_features = vgg(x_reconstructed_normalized)\n",
    "    return F.mse_loss(x_features, x_recon_features)\n",
    "\n",
    "# Instantiate Encoder, Decoder, and CVAE\n",
    "encoder = Encoder(latent_dim, num_classes).to(device)\n",
    "decoder = Decoder(latent_dim, num_classes).to(device)\n",
    "cvae = ConditionalVAE(encoder, decoder).to(device)\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "optimizer = optim.Adam(cvae.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.5)\n",
    "\n",
    "# Define loss function with beta annealing\n",
    "def cvae_loss(x, x_reconstructed, z_mean, z_logvar, beta=1.0, recon_weight=1.0, perceptual_weight=1.0):\n",
    "    if torch.isnan(x).any() or torch.isinf(x).any():\n",
    "        print(\"NaN or Inf detected in x\")\n",
    "    if torch.isnan(x_reconstructed).any() or torch.isinf(x_reconstructed).any():\n",
    "        print(\"NaN or Inf detected in x_reconstructed\")\n",
    "    if torch.isnan(z_mean).any() or torch.isinf(z_mean).any():\n",
    "        print(\"NaN or Inf detected in z_mean\")\n",
    "    if torch.isnan(z_logvar).any() or torch.isinf(z_logvar).any():\n",
    "        print(\"NaN or Inf detected in z_logvar\")\n",
    "\n",
    "    recon_loss = F.binary_cross_entropy(x_reconstructed, x, reduction='sum')\n",
    "    kl_loss = -0.5 * torch.sum(1 + torch.clamp(z_logvar, -10, 10) - z_mean.pow(2) - torch.clamp(z_logvar, -10, 10).exp())\n",
    "    percep_loss = perceptual_loss(x, x_reconstructed) * perceptual_weight\n",
    "    total_loss = recon_weight * recon_loss + beta * kl_loss + percep_loss\n",
    "    return total_loss, recon_loss, kl_loss, percep_loss\n",
    "\n",
    "# Training loop with checkpointing\n",
    "cvae.train()\n",
    "for epoch in range(epochs):\n",
    "    if epoch < annealing_epochs:\n",
    "        beta = beta_max * (epoch / annealing_epochs)\n",
    "    else:\n",
    "        beta = beta_max\n",
    "\n",
    "    train_loss = 0\n",
    "    train_recon_loss = 0\n",
    "    train_kl_loss = 0\n",
    "    train_percep_loss = 0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        x_reconstructed, z_mean, z_logvar = cvae(data, labels)\n",
    "        total_loss, recon_loss, kl_loss, percep_loss = cvae_loss(\n",
    "            data, x_reconstructed, z_mean, z_logvar, \n",
    "            beta=beta, recon_weight=recon_weight, perceptual_weight=perceptual_weight\n",
    "        )\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(cvae.parameters(), max_norm=1.0)\n",
    "        train_loss += total_loss.item()\n",
    "        train_recon_loss += recon_loss.item()\n",
    "        train_kl_loss += kl_loss.item()\n",
    "        train_percep_loss += percep_loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    avg_recon_loss = train_recon_loss / len(train_loader.dataset)\n",
    "    avg_kl_loss = train_kl_loss / len(train_loader.dataset)\n",
    "    avg_percep_loss = train_percep_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Beta: {beta:.2f}, Total Loss: {avg_loss:.4f}, '\n",
    "          f'Recon Loss: {avg_recon_loss:.4f}, KL Loss: {avg_kl_loss:.4f}, '\n",
    "          f'Percep Loss: {avg_percep_loss:.4f}')\n",
    "\n",
    "    # Save checkpoint every 50 epochs\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        checkpoint_path = os.path.join(output_dir, f\"cvae_vehicle_epoch_{epoch + 1}.pth\")\n",
    "        torch.save(cvae.state_dict(), checkpoint_path)\n",
    "        print(f\"Saved checkpoint at epoch {epoch + 1} to {checkpoint_path}\")\n",
    "\n",
    "        # Generate and visualize samples at this checkpoint\n",
    "        cvae.eval()\n",
    "        base_dir = os.path.join(output_dir, f\"generated_samples_epoch_{epoch + 1}\")\n",
    "        classes_to_generate = [3, 4]\n",
    "        num_samples = 500\n",
    "        with torch.no_grad():\n",
    "            for class_label in classes_to_generate:\n",
    "                label_tensor = torch.tensor([class_label]).repeat(num_samples).to(device)\n",
    "                z = torch.randn(num_samples, latent_dim).to(device)\n",
    "                dummy_skips = [\n",
    "                    torch.randn(num_samples, 32, 64, 64).to(device) * 0.1,\n",
    "                    torch.randn(num_samples, 64, 32, 32).to(device) * 0.1,\n",
    "                    torch.randn(num_samples, 128, 16, 16).to(device) * 0.1,\n",
    "                    torch.randn(num_samples, 256, 8, 8).to(device) * 0.1,\n",
    "                    torch.randn(num_samples, 512, 4, 4).to(device) * 0.1\n",
    "                ]\n",
    "                generated_samples = cvae.decoder(z, label_tensor, dummy_skips)\n",
    "                # Convert grayscale to RGB\n",
    "                generated_samples_rgb = grayscale_to_rgb(generated_samples)\n",
    "                class_dir = os.path.join(base_dir, str(class_label))\n",
    "                os.makedirs(class_dir, exist_ok=True)\n",
    "                for idx, sample in enumerate(generated_samples_rgb):\n",
    "                    sample = adjust_sharpness(sample, sharpness_factor=2.0)\n",
    "                    save_image(sample, os.path.join(class_dir, f\"sample_{idx}.png\"))\n",
    "                print(f\"Generated {num_samples} samples for Class {class_label} ({dataset.class_names[class_label]}) at epoch {epoch + 1}.\")\n",
    "\n",
    "        # Plot samples\n",
    "        fig, axs = plt.subplots(len(classes_to_generate), 10, figsize=(20, 4))\n",
    "        for row, class_label in enumerate(classes_to_generate):\n",
    "            class_dir = os.path.join(base_dir, str(class_label))\n",
    "            sample_files = os.listdir(class_dir)\n",
    "            random_samples = np.random.choice(sample_files, 10, replace=False)\n",
    "            for col, sample_file in enumerate(random_samples):\n",
    "                sample_path = os.path.join(class_dir, sample_file)\n",
    "                sample_image = Image.open(sample_path).convert(\"RGB\")\n",
    "                sample_image = sample_image.resize((128, 128), Image.LANCZOS)\n",
    "                sample_image = np.array(sample_image) / 255.0\n",
    "                ax = axs[row, col] if len(classes_to_generate) > 1 else axs[col]\n",
    "                ax.imshow(sample_image)\n",
    "                ax.axis('off')\n",
    "                if col == 0:\n",
    "                    ax.set_ylabel(dataset.class_names[class_label], rotation=90, labelpad=10)\n",
    "        plt.tight_layout()\n",
    "        plot_path = os.path.join(output_dir, f\"synthetic_samples_classes_3_4_epoch_{epoch + 1}.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        print(f\"Saved synthetic samples plot at epoch {epoch + 1} to {plot_path}\")\n",
    "        cvae.train()\n",
    "\n",
    "# Final model save\n",
    "final_model_path = os.path.join(output_dir, \"cvae_vehicle_final.pth\")\n",
    "torch.save(cvae.state_dict(), final_model_path)\n",
    "print(f\"Saved final CVAE model to {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6fc39a-4050-4017-bb3e-ee6f2d1165e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1a3dcf-98a7-42af-bc67-f8d338e0fa9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e943e39b-75a4-42bc-b8e2-eb76a7b61f70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
