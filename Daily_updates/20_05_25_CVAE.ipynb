{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fee8fbdc-ba16-4b0b-9353-72c59e78d0cb",
   "metadata": {},
   "source": [
    "CVAE with 50 epoches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bb385e-c36c-492f-97f5-d5c2906a4d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import kagglehub\n",
    "from PIL import Image\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 16\n",
    "num_classes = 5\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "learning_rate = 1e-3\n",
    "image_size = 128\n",
    "channels = 3\n",
    "\n",
    "# Download Vehicle Type Image Dataset from Kaggle\n",
    "try:\n",
    "    path = kagglehub.dataset_download(\"sujaykapadnis/vehicle-type-image-dataset\")\n",
    "    print(\"Path to dataset files:\", path)\n",
    "    dataset_path = path\n",
    "except Exception as e:\n",
    "    print(f\"Failed to download dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Define the VehicleTypeDataset class (from your previous code)\n",
    "class VehicleTypeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_names = []\n",
    "        self.class_to_idx = {}\n",
    "\n",
    "        print(f\"Searching for images in {root_dir}\")\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            image_files = [f for f in files if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "            if image_files:\n",
    "                class_name = os.path.basename(root)\n",
    "                if class_name not in self.class_to_idx:\n",
    "                    self.class_names.append(class_name)\n",
    "                    self.class_to_idx[class_name] = len(self.class_names) - 1\n",
    "                for img_file in image_files:\n",
    "                    img_path = os.path.join(root, img_file)\n",
    "                    self.images.append(img_path)\n",
    "                    self.labels.append(self.class_to_idx[class_name])\n",
    "\n",
    "        if not self.images:\n",
    "            raise ValueError(\n",
    "                f\"No images found in {root_dir}. \"\n",
    "                \"Expected class folders containing .jpg, .png, or .jpeg images.\"\n",
    "            )\n",
    "\n",
    "        print(f\"Found {len(self.images)} images across {len(self.class_names)} classes.\")\n",
    "        print(f\"Classes: {self.class_names}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Define transforms (normalize to [0, 1] for CVAE training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),  # Converts to [0, 1]\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = VehicleTypeDataset(root_dir=dataset_path, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the Encoder network (convolutional)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels + num_classes, 32, kernel_size=4, stride=2, padding=1)  # Output: 32x64x64\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)  # Output: 64x32x32\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)  # Output: 128x16x16\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)  # Output: 256x8x8\n",
    "        self.fc_mean = nn.Linear(256 * 8 * 8, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(256 * 8 * 8, latent_dim)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # Expand one-hot labels to match image dimensions and concatenate\n",
    "        y = F.one_hot(y, num_classes=num_classes).float()  # Shape: (batch_size, num_classes)\n",
    "        y = y.unsqueeze(-1).unsqueeze(-1)  # Shape: (batch_size, num_classes, 1, 1)\n",
    "        y = y.expand(-1, -1, x.size(2), x.size(3))  # Shape: (batch_size, num_classes, 128, 128)\n",
    "        x_with_y = torch.cat([x, y], dim=1)  # Shape: (batch_size, channels + num_classes, 128, 128)\n",
    "        \n",
    "        h = F.relu(self.conv1(x_with_y))\n",
    "        h = F.relu(self.conv2(h))\n",
    "        h = F.relu(self.conv3(h))\n",
    "        h = F.relu(self.conv4(h))\n",
    "        h = h.view(h.size(0), -1)  # Flatten: (batch_size, 256*8*8)\n",
    "        z_mean = self.fc_mean(h)\n",
    "        z_logvar = self.fc_logvar(h)\n",
    "        return z_mean, z_logvar\n",
    "\n",
    "# Define the Decoder network (transposed convolutional)\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim + num_classes, 256 * 8 * 8)\n",
    "        self.deconv1 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)  # Output: 128x16x16\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)  # Output: 64x32x32\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)  # Output: 32x64x64\n",
    "        self.deconv4 = nn.ConvTranspose2d(32, channels, kernel_size=4, stride=2, padding=1)  # Output: 3x128x128\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        y = F.one_hot(y, num_classes=num_classes).float()  # Shape: (batch_size, num_classes)\n",
    "        z_with_y = torch.cat([z, y], dim=-1)  # Shape: (batch_size, latent_dim + num_classes)\n",
    "        h = F.relu(self.fc(z_with_y))\n",
    "        h = h.view(h.size(0), 256, 8, 8)  # Reshape: (batch_size, 256, 8, 8)\n",
    "        h = F.relu(self.deconv1(h))\n",
    "        h = F.relu(self.deconv2(h))\n",
    "        h = F.relu(self.deconv3(h))\n",
    "        x_reconstructed = torch.sigmoid(self.deconv4(h))  # Output: (batch_size, 3, 128, 128)\n",
    "        return x_reconstructed\n",
    "\n",
    "# Conditional VAE model\n",
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def reparameterize(self, z_mean, z_logvar):\n",
    "        std = torch.exp(0.5 * z_logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return z_mean + eps * std\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        z_mean, z_logvar = self.encoder(x, y)\n",
    "        z = self.reparameterize(z_mean, z_logvar)\n",
    "        x_reconstructed = self.decoder(z, y)\n",
    "        return x_reconstructed, z_mean, z_logvar\n",
    "\n",
    "# Instantiate Encoder, Decoder, and CVAE\n",
    "encoder = Encoder(latent_dim, num_classes).to(device)\n",
    "decoder = Decoder(latent_dim, num_classes).to(device)\n",
    "cvae = ConditionalVAE(encoder, decoder).to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(cvae.parameters(), lr=learning_rate)\n",
    "\n",
    "def cvae_loss(x, x_reconstructed, z_mean, z_logvar):\n",
    "    recon_loss = F.binary_cross_entropy(x_reconstructed, x, reduction='sum')\n",
    "    kl_loss = -0.5 * torch.sum(1 + z_logvar - z_mean.pow(2) - z_logvar.exp())\n",
    "    return recon_loss + kl_loss\n",
    "\n",
    "# Training loop\n",
    "cvae.train()\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        x_reconstructed, z_mean, z_logvar = cvae(data, labels)\n",
    "        loss = cvae_loss(data, x_reconstructed, z_mean, z_logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {train_loss / len(train_loader.dataset):.4f}')\n",
    "\n",
    "# Save the trained CVAE model\n",
    "output_dir = \"FL_VEHICLE_NON_IID\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "model_path = os.path.join(output_dir, \"cvae_vehicle.pth\")\n",
    "torch.save(cvae.state_dict(), model_path)\n",
    "print(f\"Saved CVAE model to {model_path}\")\n",
    "\n",
    "# Generate samples for Classes 3 and 4\n",
    "def generate_samples_labelwise(cvae, num_samples, classes_to_generate, base_dir, latent_dim, device):\n",
    "    cvae.eval()\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    with torch.no_grad():\n",
    "        for class_label in classes_to_generate:\n",
    "            label_tensor = torch.tensor([class_label]).repeat(num_samples).to(device)\n",
    "            z = torch.randn(num_samples, latent_dim).to(device)\n",
    "            generated_samples = cvae.decoder(z, label_tensor)  # Shape: (num_samples, 3, 128, 128)\n",
    "            \n",
    "            class_dir = os.path.join(base_dir, str(class_label))\n",
    "            os.makedirs(class_dir, exist_ok=True)\n",
    "            for idx, sample in enumerate(generated_samples):\n",
    "                save_image(sample, os.path.join(class_dir, f\"sample_{idx}.png\"))\n",
    "            print(f\"Generated {num_samples} samples for Class {class_label} ({dataset.class_names[class_label]}).\")\n",
    "\n",
    "# Generate 500 samples each for Classes 3 and 4\n",
    "base_dir = os.path.join(output_dir, \"generated_samples\")\n",
    "classes_to_generate = [3, 4]  # Classes 3 and 4\n",
    "generate_samples_labelwise(cvae, num_samples=500, classes_to_generate=classes_to_generate, base_dir=base_dir, latent_dim=latent_dim, device=device)\n",
    "\n",
    "# Plot random samples for Classes 3 and 4\n",
    "def plot_random_samples(base_dir, classes_to_generate, num_images_per_class=10):\n",
    "    fig, axs = plt.subplots(len(classes_to_generate), num_images_per_class, figsize=(20, 4))\n",
    "    for row, class_label in enumerate(classes_to_generate):\n",
    "        class_dir = os.path.join(base_dir, str(class_label))\n",
    "        sample_files = os.listdir(class_dir)\n",
    "        random_samples = np.random.choice(sample_files, num_images_per_class, replace=False)\n",
    "        \n",
    "        for col, sample_file in enumerate(random_samples):\n",
    "            sample_path = os.path.join(class_dir, sample_file)\n",
    "            sample_image = plt.imread(sample_path)  # RGB image\n",
    "            if len(classes_to_generate) == 1:\n",
    "                ax = axs[col]\n",
    "            else:\n",
    "                ax = axs[row, col]\n",
    "            ax.imshow(sample_image)\n",
    "            ax.axis('off')\n",
    "            if col == 0:\n",
    "                ax.set_ylabel(dataset.class_names[class_label], rotation=90, labelpad=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plot_path = os.path.join(output_dir, \"synthetic_samples_classes_3_4.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.show()\n",
    "    print(f\"Saved synthetic samples plot to {plot_path}\")\n",
    "\n",
    "# Plot 10 samples each for Classes 3 and 4\n",
    "plot_random_samples(base_dir=base_dir, classes_to_generate=classes_to_generate, num_images_per_class=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b14bd7-9737-4ade-a4d0-6628660efbb6",
   "metadata": {},
   "source": [
    "MODIDFED CVAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c264d06-66ef-4936-bc3a-0ee546c3707d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Searching for images in C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Found 4793 images across 5 classes.\n",
      "Classes: ['Hatchback', 'Other', 'Pickup', 'Seden', 'SUV']\n",
      "Epoch 1/500, Total Loss: 26322.5639, Recon Loss: 26321.3656, KL Loss: 1.1939, Percep Loss: 0.0042\n",
      "Epoch 2/500, Total Loss: 25217.1438, Recon Loss: 25217.1412, KL Loss: 0.0008, Percep Loss: 0.0021\n",
      "Epoch 3/500, Total Loss: 25122.8553, Recon Loss: 25122.8534, KL Loss: 0.0001, Percep Loss: 0.0013\n",
      "Epoch 4/500, Total Loss: 25086.5595, Recon Loss: 25086.5589, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 5/500, Total Loss: 25070.5351, Recon Loss: 25070.5351, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 6/500, Total Loss: 25054.9869, Recon Loss: 25054.9869, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 7/500, Total Loss: 25052.4975, Recon Loss: 25052.4975, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 8/500, Total Loss: 25039.3093, Recon Loss: 25039.3093, KL Loss: 0.0000, Percep Loss: 0.0004\n",
      "Epoch 9/500, Total Loss: 25038.7604, Recon Loss: 25038.7604, KL Loss: 0.0000, Percep Loss: 0.0004\n",
      "Epoch 10/500, Total Loss: 25030.8728, Recon Loss: 25030.8728, KL Loss: 0.0000, Percep Loss: 0.0003\n",
      "Epoch 11/500, Total Loss: 25025.0515, Recon Loss: 25025.0515, KL Loss: 0.0000, Percep Loss: 0.0003\n",
      "Epoch 12/500, Total Loss: 25023.2381, Recon Loss: 25023.2381, KL Loss: 0.0000, Percep Loss: 0.0003\n",
      "Epoch 13/500, Total Loss: 25022.5802, Recon Loss: 25022.5802, KL Loss: 0.0000, Percep Loss: 0.0002\n",
      "Epoch 14/500, Total Loss: 25018.0206, Recon Loss: 25018.0206, KL Loss: 0.0000, Percep Loss: 0.0002\n",
      "Epoch 15/500, Total Loss: 25014.2251, Recon Loss: 25014.2251, KL Loss: 0.0000, Percep Loss: 0.0002\n",
      "Epoch 16/500, Total Loss: 25014.6417, Recon Loss: 25014.6417, KL Loss: 0.0000, Percep Loss: 0.0002\n",
      "Epoch 17/500, Total Loss: 25014.1745, Recon Loss: 25014.1745, KL Loss: 0.0000, Percep Loss: 0.0002\n",
      "Epoch 18/500, Total Loss: 25012.1217, Recon Loss: 25012.1217, KL Loss: 0.0000, Percep Loss: 0.0002\n",
      "Epoch 19/500, Total Loss: 25008.9829, Recon Loss: 25008.9829, KL Loss: 0.0000, Percep Loss: 0.0001\n",
      "Epoch 20/500, Total Loss: 25012.4213, Recon Loss: 25012.4213, KL Loss: 0.0000, Percep Loss: 0.0001\n",
      "Epoch 21/500, Total Loss: 25008.2530, Recon Loss: 25008.2530, KL Loss: 0.0000, Percep Loss: 0.0001\n",
      "Epoch 22/500, Total Loss: 25006.6540, Recon Loss: 25006.6540, KL Loss: 0.0000, Percep Loss: 0.0001\n",
      "Epoch 23/500, Total Loss: 25006.1369, Recon Loss: 25006.1369, KL Loss: 0.0000, Percep Loss: 0.0001\n",
      "Epoch 24/500, Total Loss: 25010.6735, Recon Loss: 25010.6734, KL Loss: 0.0001, Percep Loss: 0.0001\n",
      "Epoch 25/500, Total Loss: 25002.5257, Recon Loss: 25002.5257, KL Loss: 0.0000, Percep Loss: 0.0001\n",
      "Epoch 26/500, Total Loss: 25003.7075, Recon Loss: 25003.7075, KL Loss: 0.0000, Percep Loss: 0.0001\n",
      "Epoch 27/500, Total Loss: 25003.4492, Recon Loss: 25003.4492, KL Loss: 0.0000, Percep Loss: 0.0001\n",
      "Epoch 28/500, Total Loss: 25004.4788, Recon Loss: 25004.4788, KL Loss: 0.0000, Percep Loss: 0.0001\n",
      "Epoch 29/500, Total Loss: 25001.1106, Recon Loss: 25001.1106, KL Loss: 0.0000, Percep Loss: 0.0001\n",
      "Epoch 30/500, Total Loss: 25001.6957, Recon Loss: 25001.6957, KL Loss: 0.0000, Percep Loss: 0.0001\n",
      "Epoch 31/500, Total Loss: 25004.1301, Recon Loss: 25004.1300, KL Loss: 0.0000, Percep Loss: 0.0001\n",
      "Epoch 32/500, Total Loss: 25002.7351, Recon Loss: 25002.7351, KL Loss: 0.0000, Percep Loss: 0.0001\n",
      "Epoch 33/500, Total Loss: 24998.9288, Recon Loss: 24998.9288, KL Loss: 0.0000, Percep Loss: 0.0001\n",
      "Epoch 34/500, Total Loss: 24999.8986, Recon Loss: 24999.8986, KL Loss: 0.0000, Percep Loss: 0.0001\n",
      "Epoch 35/500, Total Loss: 25000.7151, Recon Loss: 25000.7151, KL Loss: 0.0000, Percep Loss: 0.0001\n",
      "Epoch 36/500, Total Loss: 24998.2954, Recon Loss: 24998.2954, KL Loss: 0.0000, Percep Loss: 0.0001\n",
      "Epoch 37/500, Total Loss: 24998.9065, Recon Loss: 24998.9065, KL Loss: 0.0000, Percep Loss: 0.0001\n",
      "Epoch 38/500, Total Loss: 24999.2106, Recon Loss: 24999.2106, KL Loss: 0.0000, Percep Loss: 0.0001\n",
      "Epoch 39/500, Total Loss: 24996.4881, Recon Loss: 24996.4881, KL Loss: 0.0000, Percep Loss: 0.0001\n",
      "Epoch 40/500, Total Loss: 24998.6927, Recon Loss: 24998.6927, KL Loss: 0.0000, Percep Loss: 0.0001\n",
      "Epoch 41/500, Total Loss: 24996.8943, Recon Loss: 24996.8943, KL Loss: 0.0000, Percep Loss: 0.0001\n",
      "Epoch 42/500, Total Loss: 24997.5425, Recon Loss: 24997.5425, KL Loss: 0.0000, Percep Loss: 0.0001\n",
      "Epoch 43/500, Total Loss: 24997.5914, Recon Loss: 24997.5914, KL Loss: 0.0000, Percep Loss: 0.0001\n",
      "Epoch 44/500, Total Loss: 24997.3993, Recon Loss: 24997.3993, KL Loss: 0.0000, Percep Loss: 0.0001\n",
      "Epoch 45/500, Total Loss: 24995.1277, Recon Loss: 24995.1277, KL Loss: 0.0000, Percep Loss: 0.0001\n",
      "Epoch 46/500, Total Loss: 24995.6295, Recon Loss: 24995.6295, KL Loss: 0.0000, Percep Loss: 0.0001\n",
      "Epoch 47/500, Total Loss: 24996.9170, Recon Loss: 24996.9170, KL Loss: 0.0000, Percep Loss: 0.0001\n",
      "Epoch 48/500, Total Loss: 24994.0836, Recon Loss: 24994.0836, KL Loss: 0.0000, Percep Loss: 0.0001\n",
      "Epoch 49/500, Total Loss: 24996.9836, Recon Loss: 24996.9836, KL Loss: 0.0000, Percep Loss: 0.0001\n",
      "Epoch 50/500, Total Loss: 24995.6941, Recon Loss: 24995.6941, KL Loss: 0.0001, Percep Loss: 0.0001\n",
      "Saved checkpoint at epoch 50 to FL_VEHICLE_NON_IID\\cvae_vehicle_epoch_50.pth\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 500 but got size 16 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 245\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# Dummy skip connections for generation (since we're not passing through encoder)\u001b[39;00m\n\u001b[0;32m    238\u001b[0m dummy_skips \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    239\u001b[0m     torch\u001b[38;5;241m.\u001b[39mzeros(batch_size, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m    240\u001b[0m     torch\u001b[38;5;241m.\u001b[39mzeros(batch_size, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    243\u001b[0m     torch\u001b[38;5;241m.\u001b[39mzeros(batch_size, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    244\u001b[0m ]\n\u001b[1;32m--> 245\u001b[0m generated_samples \u001b[38;5;241m=\u001b[39m cvae\u001b[38;5;241m.\u001b[39mdecoder(z, label_tensor, dummy_skips)\n\u001b[0;32m    246\u001b[0m class_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_dir, \u001b[38;5;28mstr\u001b[39m(class_label))\n\u001b[0;32m    247\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(class_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\Desktop\\New folder\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Desktop\\New folder\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[3], line 139\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, z, y, skip_connections)\u001b[0m\n\u001b[0;32m    136\u001b[0m h \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mview(h\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m    138\u001b[0m h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeconv1(h))  \u001b[38;5;66;03m# 256x8x8\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([h, h4], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Concatenate skip connection: 256 + 256 = 512\u001b[39;00m\n\u001b[0;32m    140\u001b[0m h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeconv2(h))  \u001b[38;5;66;03m# 128x16x16\u001b[39;00m\n\u001b[0;32m    141\u001b[0m h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([h, h3], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Concatenate skip connection: 128 + 128 = 256\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 500 but got size 16 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms.functional import adjust_sharpness\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import kagglehub\n",
    "from PIL import Image\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 64\n",
    "num_classes = 5\n",
    "batch_size = 16\n",
    "epochs = 500\n",
    "learning_rate = 1e-3\n",
    "image_size = 128\n",
    "channels = 3\n",
    "output_dir = \"FL_VEHICLE_NON_IID\"\n",
    "\n",
    "# Download Vehicle Type Image Dataset from Kaggle\n",
    "try:\n",
    "    path = kagglehub.dataset_download(\"sujaykapadnis/vehicle-type-image-dataset\")\n",
    "    print(\"Path to dataset files:\", path)\n",
    "    dataset_path = path\n",
    "except Exception as e:\n",
    "    print(f\"Failed to download dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Define the VehicleTypeDataset class\n",
    "class VehicleTypeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_names = []\n",
    "        self.class_to_idx = {}\n",
    "\n",
    "        print(f\"Searching for images in {root_dir}\")\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            image_files = [f for f in files if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "            if image_files:\n",
    "                class_name = os.path.basename(root)\n",
    "                if class_name not in self.class_to_idx:\n",
    "                    self.class_names.append(class_name)\n",
    "                    self.class_to_idx[class_name] = len(self.class_names) - 1\n",
    "                for img_file in image_files:\n",
    "                    img_path = os.path.join(root, img_file)\n",
    "                    self.images.append(img_path)\n",
    "                    self.labels.append(self.class_to_idx[class_name])\n",
    "\n",
    "        if not self.images:\n",
    "            raise ValueError(\n",
    "                f\"No images found in {root_dir}. \"\n",
    "                \"Expected class folders containing .jpg, .png, or .jpeg images.\"\n",
    "            )\n",
    "\n",
    "        print(f\"Found {len(self.images)} images across {len(self.class_names)} classes.\")\n",
    "        print(f\"Classes: {self.class_names}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),  # Converts to [0, 1]\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = VehicleTypeDataset(root_dir=dataset_path, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the Encoder network with deeper layers and skip connections\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels + num_classes, 32, kernel_size=4, stride=2, padding=1)  # 32x64x64\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)  # 64x32x32\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)  # 128x16x16\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)  # 256x8x8\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)  # 512x4x4\n",
    "        self.fc_mean = nn.Linear(512 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(512 * 4 * 4, latent_dim)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        y = F.one_hot(y, num_classes=num_classes).float()\n",
    "        y = y.unsqueeze(-1).unsqueeze(-1)\n",
    "        y = y.expand(-1, -1, x.size(2), x.size(3))\n",
    "        x_with_y = torch.cat([x, y], dim=1)\n",
    "        \n",
    "        h1 = F.relu(self.conv1(x_with_y))\n",
    "        h2 = F.relu(self.conv2(h1))\n",
    "        h3 = F.relu(self.conv3(h2))\n",
    "        h4 = F.relu(self.conv4(h3))\n",
    "        h5 = F.relu(self.conv5(h4))\n",
    "        h = h5.view(h5.size(0), -1)  # Fixed: Use h5.size(0) instead of h.size(0)\n",
    "        z_mean = self.fc_mean(h)\n",
    "        z_logvar = self.fc_logvar(h)\n",
    "        return z_mean, z_logvar, (h1, h2, h3, h4, h5)\n",
    "\n",
    "# Define the Decoder network with skip connections\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim + num_classes, 512 * 4 * 4)\n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1)  # 256x8x8\n",
    "        self.deconv2 = nn.ConvTranspose2d(512, 128, kernel_size=4, stride=2, padding=1)  # 128x16x16\n",
    "        self.deconv3 = nn.ConvTranspose2d(256, 64, kernel_size=4, stride=2, padding=1)  # 64x32x32\n",
    "        self.deconv4 = nn.ConvTranspose2d(128, 32, kernel_size=4, stride=2, padding=1)  # 32x64x64\n",
    "        self.deconv5 = nn.ConvTranspose2d(64, channels, kernel_size=4, stride=2, padding=1)  # 3x128x128\n",
    "\n",
    "    def forward(self, z, y, skip_connections):\n",
    "        h1, h2, h3, h4, h5 = skip_connections\n",
    "        y = F.one_hot(y, num_classes=num_classes).float()\n",
    "        z_with_y = torch.cat([z, y], dim=-1)\n",
    "        h = F.relu(self.fc(z_with_y))\n",
    "        h = h.view(h.size(0), 512, 4, 4)\n",
    "        \n",
    "        h = F.relu(self.deconv1(h))  # 256x8x8\n",
    "        h = torch.cat([h, h4], dim=1)  # Concatenate skip connection: 256 + 256 = 512\n",
    "        h = F.relu(self.deconv2(h))  # 128x16x16\n",
    "        h = torch.cat([h, h3], dim=1)  # Concatenate skip connection: 128 + 128 = 256\n",
    "        h = F.relu(self.deconv3(h))  # 64x32x32\n",
    "        h = torch.cat([h, h2], dim=1)  # Concatenate skip connection: 64 + 64 = 128\n",
    "        h = F.relu(self.deconv4(h))  # 32x64x64\n",
    "        h = torch.cat([h, h1], dim=1)  # Concatenate skip connection: 32 + 32 = 64\n",
    "        x_reconstructed = torch.sigmoid(self.deconv5(h))\n",
    "        return x_reconstructed\n",
    "\n",
    "# Conditional VAE model\n",
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def reparameterize(self, z_mean, z_logvar):\n",
    "        std = torch.exp(0.5 * z_logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return z_mean + eps * std\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        z_mean, z_logvar, skip_connections = self.encoder(x, y)\n",
    "        z = self.reparameterize(z_mean, z_logvar)\n",
    "        x_reconstructed = self.decoder(z, y, skip_connections)\n",
    "        return x_reconstructed, z_mean, z_logvar\n",
    "\n",
    "# Load pretrained VGG16 for perceptual loss (updated API)\n",
    "vgg = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features.to(device).eval()\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "def perceptual_loss(x, x_reconstructed):\n",
    "    x_features = vgg(x)\n",
    "    x_recon_features = vgg(x_reconstructed)\n",
    "    return F.mse_loss(x_features, x_recon_features)\n",
    "\n",
    "# Instantiate Encoder, Decoder, and CVAE\n",
    "encoder = Encoder(latent_dim, num_classes).to(device)\n",
    "decoder = Decoder(latent_dim, num_classes).to(device)\n",
    "cvae = ConditionalVAE(encoder, decoder).to(device)\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "optimizer = optim.Adam(cvae.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "\n",
    "# Define loss function with separate logging\n",
    "def cvae_loss(x, x_reconstructed, z_mean, z_logvar, perceptual_weight=0.1):\n",
    "    recon_loss = F.binary_cross_entropy(x_reconstructed, x, reduction='sum')\n",
    "    kl_loss = -0.5 * torch.sum(1 + z_logvar - z_mean.pow(2) - z_logvar.exp())\n",
    "    percep_loss = perceptual_loss(x, x_reconstructed) * perceptual_weight\n",
    "    total_loss = recon_loss + kl_loss + percep_loss\n",
    "    return total_loss, recon_loss, kl_loss, percep_loss\n",
    "\n",
    "# Training loop with checkpointing\n",
    "cvae.train()\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    train_recon_loss = 0\n",
    "    train_kl_loss = 0\n",
    "    train_percep_loss = 0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        x_reconstructed, z_mean, z_logvar = cvae(data, labels)\n",
    "        total_loss, recon_loss, kl_loss, percep_loss = cvae_loss(data, x_reconstructed, z_mean, z_logvar)\n",
    "        total_loss.backward()\n",
    "        train_loss += total_loss.item()\n",
    "        train_recon_loss += recon_loss.item()\n",
    "        train_kl_loss += kl_loss.item()\n",
    "        train_percep_loss += percep_loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    avg_recon_loss = train_recon_loss / len(train_loader.dataset)\n",
    "    avg_kl_loss = train_kl_loss / len(train_loader.dataset)\n",
    "    avg_percep_loss = train_percep_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Total Loss: {avg_loss:.4f}, '\n",
    "          f'Recon Loss: {avg_recon_loss:.4f}, KL Loss: {avg_kl_loss:.4f}, '\n",
    "          f'Percep Loss: {avg_percep_loss:.4f}')\n",
    "\n",
    "    # Save checkpoint every 50 epochs\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        checkpoint_path = os.path.join(output_dir, f\"cvae_vehicle_epoch_{epoch + 1}.pth\")\n",
    "        torch.save(cvae.state_dict(), checkpoint_path)\n",
    "        print(f\"Saved checkpoint at epoch {epoch + 1} to {checkpoint_path}\")\n",
    "\n",
    "        # Generate and visualize samples at this checkpoint\n",
    "        cvae.eval()\n",
    "        base_dir = os.path.join(output_dir, f\"generated_samples_epoch_{epoch + 1}\")\n",
    "        classes_to_generate = [3, 4]\n",
    "        with torch.no_grad():\n",
    "            for class_label in classes_to_generate:\n",
    "                label_tensor = torch.tensor([class_label]).repeat(500).to(device)\n",
    "                z = torch.randn(500, latent_dim).to(device)\n",
    "                # Dummy skip connections for generation (since we're not passing through encoder)\n",
    "                dummy_skips = [\n",
    "                    torch.zeros(batch_size, 32, 64, 64).to(device),\n",
    "                    torch.zeros(batch_size, 64, 32, 32).to(device),\n",
    "                    torch.zeros(batch_size, 128, 16, 16).to(device),\n",
    "                    torch.zeros(batch_size, 256, 8, 8).to(device),\n",
    "                    torch.zeros(batch_size, 512, 4, 4).to(device)\n",
    "                ]\n",
    "                generated_samples = cvae.decoder(z, label_tensor, dummy_skips)\n",
    "                class_dir = os.path.join(base_dir, str(class_label))\n",
    "                os.makedirs(class_dir, exist_ok=True)\n",
    "                for idx, sample in enumerate(generated_samples):\n",
    "                    sample = adjust_sharpness(sample, sharpness_factor=2.0)\n",
    "                    save_image(sample, os.path.join(class_dir, f\"sample_{idx}.png\"))\n",
    "                print(f\"Generated 500 samples for Class {class_label} ({dataset.class_names[class_label]}) at epoch {epoch + 1}.\")\n",
    "\n",
    "        # Plot samples\n",
    "        fig, axs = plt.subplots(len(classes_to_generate), 10, figsize=(20, 4))\n",
    "        for row, class_label in enumerate(classes_to_generate):\n",
    "            class_dir = os.path.join(base_dir, str(class_label))\n",
    "            sample_files = os.listdir(class_dir)\n",
    "            random_samples = np.random.choice(sample_files, 10, replace=False)\n",
    "            for col, sample_file in enumerate(random_samples):\n",
    "                sample_path = os.path.join(class_dir, sample_file)\n",
    "                sample_image = Image.open(sample_path).convert(\"RGB\")\n",
    "                sample_image = sample_image.resize((128, 128), Image.LANCZOS)\n",
    "                sample_image = np.array(sample_image) / 255.0\n",
    "                ax = axs[row, col] if len(classes_to_generate) > 1 else axs[col]\n",
    "                ax.imshow(sample_image)\n",
    "                ax.axis('off')\n",
    "                if col == 0:\n",
    "                    ax.set_ylabel(dataset.class_names[class_label], rotation=90, labelpad=10)\n",
    "        plt.tight_layout()\n",
    "        plot_path = os.path.join(output_dir, f\"synthetic_samples_classes_3_4_epoch_{epoch + 1}.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        print(f\"Saved synthetic samples plot at epoch {epoch + 1} to {plot_path}\")\n",
    "        cvae.train()\n",
    "\n",
    "# Final model save\n",
    "final_model_path = os.path.join(output_dir, \"cvae_vehicle_final.pth\")\n",
    "torch.save(cvae.state_dict(), final_model_path)\n",
    "print(f\"Saved final CVAE model to {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1958b7cd-9d97-4bb6-949f-955d3b5f68b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Searching for images in C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Found 4793 images across 5 classes.\n",
      "Classes: ['Hatchback', 'Other', 'Pickup', 'Seden', 'SUV']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 209\u001b[0m\n\u001b[0;32m    207\u001b[0m train_kl_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    208\u001b[0m train_percep_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 209\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m    210\u001b[0m     data, labels \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    212\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\Desktop\\New folder\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\Desktop\\New folder\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\Desktop\\New folder\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[1], line 79\u001b[0m, in \u001b[0;36mVehicleTypeDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     77\u001b[0m img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages[idx]\n\u001b[0;32m     78\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[1;32m---> 79\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m     81\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n",
      "File \u001b[1;32m~\\Desktop\\New folder\\Lib\\site-packages\\PIL\\Image.py:995\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;15\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBGR;24\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    993\u001b[0m     deprecate(mode, \u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m--> 995\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m    997\u001b[0m has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\New folder\\Lib\\site-packages\\PIL\\ImageFile.py:293\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[0;32m    292\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 293\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(b)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms.functional import adjust_sharpness\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import kagglehub\n",
    "from PIL import Image\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 64\n",
    "num_classes = 5\n",
    "batch_size = 16\n",
    "epochs = 500\n",
    "learning_rate = 1e-3\n",
    "image_size = 128\n",
    "channels = 3\n",
    "output_dir = \"FL_VEHICLE_NON_IID\"\n",
    "beta_max = 10.0  # Maximum beta for KL loss\n",
    "annealing_epochs = 50  # Epochs over which to anneal beta\n",
    "\n",
    "# Download Vehicle Type Image Dataset from Kaggle\n",
    "try:\n",
    "    path = kagglehub.dataset_download(\"sujaykapadnis/vehicle-type-image-dataset\")\n",
    "    print(\"Path to dataset files:\", path)\n",
    "    dataset_path = path\n",
    "except Exception as e:\n",
    "    print(f\"Failed to download dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Define the VehicleTypeDataset class\n",
    "class VehicleTypeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_names = []\n",
    "        self.class_to_idx = {}\n",
    "\n",
    "        print(f\"Searching for images in {root_dir}\")\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            image_files = [f for f in files if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "            if image_files:\n",
    "                class_name = os.path.basename(root)\n",
    "                if class_name not in self.class_to_idx:\n",
    "                    self.class_names.append(class_name)\n",
    "                    self.class_to_idx[class_name] = len(self.class_names) - 1\n",
    "                for img_file in image_files:\n",
    "                    img_path = os.path.join(root, img_file)\n",
    "                    self.images.append(img_path)\n",
    "                    self.labels.append(self.class_to_idx[class_name])\n",
    "\n",
    "        if not self.images:\n",
    "            raise ValueError(\n",
    "                f\"No images found in {root_dir}. \"\n",
    "                \"Expected class folders containing .jpg, .png, or .jpeg images.\"\n",
    "            )\n",
    "\n",
    "        print(f\"Found {len(self.images)} images across {len(self.class_names)} classes.\")\n",
    "        print(f\"Classes: {self.class_names}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = VehicleTypeDataset(root_dir=dataset_path, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the Encoder network\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels + num_classes, 32, kernel_size=4, stride=2, padding=1)  # 32x64x64\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)  # 64x32x32\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)  # 128x16x16\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)  # 256x8x8\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)  # 512x4x4\n",
    "        self.fc_mean = nn.Linear(512 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(512 * 4 * 4, latent_dim)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        y = F.one_hot(y, num_classes=num_classes).float()\n",
    "        y = y.unsqueeze(-1).unsqueeze(-1)\n",
    "        y = y.expand(-1, -1, x.size(2), x.size(3))\n",
    "        x_with_y = torch.cat([x, y], dim=1)\n",
    "        \n",
    "        h1 = F.relu(self.conv1(x_with_y))\n",
    "        h2 = F.relu(self.conv2(h1))\n",
    "        h3 = F.relu(self.conv3(h2))\n",
    "        h4 = F.relu(self.conv4(h3))\n",
    "        h5 = F.relu(self.conv5(h4))\n",
    "        h = h5.view(h5.size(0), -1)\n",
    "        z_mean = self.fc_mean(h)\n",
    "        z_logvar = self.fc_logvar(h)\n",
    "        return z_mean, z_logvar, (h1, h2, h3, h4, h5)\n",
    "\n",
    "# Define the Decoder network\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim + num_classes, 512 * 4 * 4)\n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1)  # 256x8x8\n",
    "        self.deconv2 = nn.ConvTranspose2d(512, 128, kernel_size=4, stride=2, padding=1)  # 128x16x16\n",
    "        self.deconv3 = nn.ConvTranspose2d(256, 64, kernel_size=4, stride=2, padding=1)  # 64x32x32\n",
    "        self.deconv4 = nn.ConvTranspose2d(128, 32, kernel_size=4, stride=2, padding=1)  # 32x64x64\n",
    "        self.deconv5 = nn.ConvTranspose2d(64, channels, kernel_size=4, stride=2, padding=1)  # 3x128x128\n",
    "\n",
    "    def forward(self, z, y, skip_connections):\n",
    "        h1, h2, h3, h4, h5 = skip_connections\n",
    "        y = F.one_hot(y, num_classes=num_classes).float()\n",
    "        z_with_y = torch.cat([z, y], dim=-1)\n",
    "        h = F.relu(self.fc(z_with_y))\n",
    "        h = h.view(h.size(0), 512, 4, 4)\n",
    "        \n",
    "        h = F.relu(self.deconv1(h))\n",
    "        h = torch.cat([h, h4], dim=1)\n",
    "        h = F.relu(self.deconv2(h))\n",
    "        h = torch.cat([h, h3], dim=1)\n",
    "        h = F.relu(self.deconv3(h))\n",
    "        h = torch.cat([h, h2], dim=1)\n",
    "        h = F.relu(self.deconv4(h))\n",
    "        h = torch.cat([h, h1], dim=1)\n",
    "        x_reconstructed = torch.sigmoid(self.deconv5(h))\n",
    "        return x_reconstructed\n",
    "\n",
    "# Conditional VAE model\n",
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def reparameterize(self, z_mean, z_logvar):\n",
    "        std = torch.exp(0.5 * z_logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return z_mean + eps * std\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        z_mean, z_logvar, skip_connections = self.encoder(x, y)\n",
    "        z = self.reparameterize(z_mean, z_logvar)\n",
    "        x_reconstructed = self.decoder(z, y, skip_connections)\n",
    "        return x_reconstructed, z_mean, z_logvar\n",
    "\n",
    "# Load pretrained VGG16 for perceptual loss\n",
    "vgg = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features.to(device).eval()\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "def perceptual_loss(x, x_reconstructed):\n",
    "    x_features = vgg(x)\n",
    "    x_recon_features = vgg(x_reconstructed)\n",
    "    return F.mse_loss(x_features, x_recon_features)\n",
    "\n",
    "# Instantiate Encoder, Decoder, and CVAE\n",
    "encoder = Encoder(latent_dim, num_classes).to(device)\n",
    "decoder = Decoder(latent_dim, num_classes).to(device)\n",
    "cvae = ConditionalVAE(encoder, decoder).to(device)\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "optimizer = optim.Adam(cvae.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "\n",
    "# Define loss function with beta scaling and annealing\n",
    "def cvae_loss(x, x_reconstructed, z_mean, z_logvar, beta=1.0, perceptual_weight=1.0):\n",
    "    recon_loss = F.binary_cross_entropy(x_reconstructed, x, reduction='sum')\n",
    "    kl_loss = -0.5 * torch.sum(1 + z_logvar - z_mean.pow(2) - z_logvar.exp())\n",
    "    percep_loss = perceptual_loss(x, x_reconstructed) * perceptual_weight\n",
    "    total_loss = recon_loss + beta * kl_loss + percep_loss\n",
    "    return total_loss, recon_loss, kl_loss, percep_loss\n",
    "\n",
    "# Training loop with checkpointing\n",
    "cvae.train()\n",
    "for epoch in range(epochs):\n",
    "    # Compute beta for KL annealing\n",
    "    if epoch < annealing_epochs:\n",
    "        beta = beta_max * (epoch / annealing_epochs)\n",
    "    else:\n",
    "        beta = beta_max\n",
    "\n",
    "    train_loss = 0\n",
    "    train_recon_loss = 0\n",
    "    train_kl_loss = 0\n",
    "    train_percep_loss = 0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        x_reconstructed, z_mean, z_logvar = cvae(data, labels)\n",
    "        total_loss, recon_loss, kl_loss, percep_loss = cvae_loss(data, x_reconstructed, z_mean, z_logvar, beta=beta)\n",
    "        total_loss.backward()\n",
    "        train_loss += total_loss.item()\n",
    "        train_recon_loss += recon_loss.item()\n",
    "        train_kl_loss += kl_loss.item()\n",
    "        train_percep_loss += percep_loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    avg_recon_loss = train_recon_loss / len(train_loader.dataset)\n",
    "    avg_kl_loss = train_kl_loss / len(train_loader.dataset)\n",
    "    avg_percep_loss = train_percep_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Beta: {beta:.2f}, Total Loss: {avg_loss:.4f}, '\n",
    "          f'Recon Loss: {avg_recon_loss:.4f}, KL Loss: {avg_kl_loss:.4f}, '\n",
    "          f'Percep Loss: {avg_percep_loss:.4f}')\n",
    "\n",
    "    # Save checkpoint every 50 epochs\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        checkpoint_path = os.path.join(output_dir, f\"cvae_vehicle_epoch_{epoch + 1}.pth\")\n",
    "        torch.save(cvae.state_dict(), checkpoint_path)\n",
    "        print(f\"Saved checkpoint at epoch {epoch + 1} to {checkpoint_path}\")\n",
    "\n",
    "        # Generate and visualize samples at this checkpoint\n",
    "        cvae.eval()\n",
    "        base_dir = os.path.join(output_dir, f\"generated_samples_epoch_{epoch + 1}\")\n",
    "        classes_to_generate = [3, 4]\n",
    "        num_samples = 500\n",
    "        with torch.no_grad():\n",
    "            for class_label in classes_to_generate:\n",
    "                label_tensor = torch.tensor([class_label]).repeat(num_samples).to(device)\n",
    "                z = torch.randn(num_samples, latent_dim).to(device)\n",
    "                # Create dummy skip connections with correct batch size\n",
    "                dummy_skips = [\n",
    "                    torch.zeros(num_samples, 32, 64, 64).to(device),\n",
    "                    torch.zeros(num_samples, 64, 32, 32).to(device),\n",
    "                    torch.zeros(num_samples, 128, 16, 16).to(device),\n",
    "                    torch.zeros(num_samples, 256, 8, 8).to(device),\n",
    "                    torch.zeros(num_samples, 512, 4, 4).to(device)\n",
    "                ]\n",
    "                generated_samples = cvae.decoder(z, label_tensor, dummy_skips)\n",
    "                class_dir = os.path.join(base_dir, str(class_label))\n",
    "                os.makedirs(class_dir, exist_ok=True)\n",
    "                for idx, sample in enumerate(generated_samples):\n",
    "                    sample = adjust_sharpness(sample, sharpness_factor=2.0)\n",
    "                    save_image(sample, os.path.join(class_dir, f\"sample_{idx}.png\"))\n",
    "                print(f\"Generated {num_samples} samples for Class {class_label} ({dataset.class_names[class_label]}) at epoch {epoch + 1}.\")\n",
    "\n",
    "        # Plot samples\n",
    "        fig, axs = plt.subplots(len(classes_to_generate), 10, figsize=(20, 4))\n",
    "        for row, class_label in enumerate(classes_to_generate):\n",
    "            class_dir = os.path.join(base_dir, str(class_label))\n",
    "            sample_files = os.listdir(class_dir)\n",
    "            random_samples = np.random.choice(sample_files, 10, replace=False)\n",
    "            for col, sample_file in enumerate(random_samples):\n",
    "                sample_path = os.path.join(class_dir, sample_file)\n",
    "                sample_image = Image.open(sample_path).convert(\"RGB\")\n",
    "                sample_image = sample_image.resize((128, 128), Image.LANCZOS)\n",
    "                sample_image = np.array(sample_image) / 255.0\n",
    "                ax = axs[row, col] if len(classes_to_generate) > 1 else axs[col]\n",
    "                ax.imshow(sample_image)\n",
    "                ax.axis('off')\n",
    "                if col == 0:\n",
    "                    ax.set_ylabel(dataset.class_names[class_label], rotation=90, labelpad=10)\n",
    "        plt.tight_layout()\n",
    "        plot_path = os.path.join(output_dir, f\"synthetic_samples_classes_3_4_epoch_{epoch + 1}.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        print(f\"Saved synthetic samples plot at epoch {epoch + 1} to {plot_path}\")\n",
    "        cvae.train()\n",
    "\n",
    "# Final model save\n",
    "final_model_path = os.path.join(output_dir, \"cvae_vehicle_final.pth\")\n",
    "torch.save(cvae.state_dict(), final_model_path)\n",
    "print(f\"Saved final CVAE model to {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d0d6db5-40f2-468a-9217-bc978d8ade92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9193fcd-4b78-4df6-9408-610b059d02bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b379669d-4de2-4143-9174-c36d3e8094c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9ea65f-1849-4ed1-a6d1-538a1b0a964f",
   "metadata": {},
   "source": [
    "RGB MODIFED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf25708-d19c-4422-ac08-c29653da3528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Searching for images in C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Found 4793 images across 5 classes.\n",
      "Classes: ['Hatchback', 'Other', 'Pickup', 'Seden', 'SUV']\n",
      "Epoch 1/1000, Beta: 0.00, Total Loss: 16987.8285, Recon Loss: 33975.5287, KL Loss: 2253.9204, Percep Loss: 0.0641\n",
      "Epoch 2/1000, Beta: 0.20, Total Loss: 12782.9845, Recon Loss: 25562.7434, KL Loss: 7.8139, Percep Loss: 0.0500\n",
      "Epoch 3/1000, Beta: 0.40, Total Loss: 12708.2272, Recon Loss: 25416.2942, KL Loss: 0.0983, Percep Loss: 0.0407\n",
      "Epoch 4/1000, Beta: 0.60, Total Loss: 12653.7986, Recon Loss: 25307.5122, KL Loss: 0.0118, Percep Loss: 0.0354\n",
      "Epoch 5/1000, Beta: 0.80, Total Loss: 12615.7262, Recon Loss: 25231.3893, KL Loss: 0.0021, Percep Loss: 0.0298\n",
      "Epoch 6/1000, Beta: 1.00, Total Loss: 12595.2531, Recon Loss: 25190.4501, KL Loss: 0.0017, Percep Loss: 0.0265\n",
      "Epoch 7/1000, Beta: 1.20, Total Loss: 12582.4881, Recon Loss: 25164.9310, KL Loss: 0.0009, Percep Loss: 0.0216\n",
      "Epoch 8/1000, Beta: 1.40, Total Loss: 12570.0931, Recon Loss: 25140.1468, KL Loss: 0.0003, Percep Loss: 0.0193\n",
      "Epoch 9/1000, Beta: 1.60, Total Loss: 12564.3846, Recon Loss: 25128.7333, KL Loss: 0.0004, Percep Loss: 0.0173\n",
      "Epoch 10/1000, Beta: 1.80, Total Loss: 12552.7011, Recon Loss: 25105.3705, KL Loss: 0.0001, Percep Loss: 0.0156\n",
      "Epoch 11/1000, Beta: 2.00, Total Loss: 12549.7450, Recon Loss: 25099.4610, KL Loss: 0.0002, Percep Loss: 0.0143\n",
      "Epoch 12/1000, Beta: 2.20, Total Loss: 12543.7199, Recon Loss: 25087.4131, KL Loss: 0.0001, Percep Loss: 0.0131\n",
      "Epoch 13/1000, Beta: 2.40, Total Loss: 12542.6369, Recon Loss: 25085.2469, KL Loss: 0.0002, Percep Loss: 0.0131\n",
      "Epoch 14/1000, Beta: 2.60, Total Loss: 12535.2740, Recon Loss: 25070.5260, KL Loss: 0.0000, Percep Loss: 0.0110\n",
      "Epoch 15/1000, Beta: 2.80, Total Loss: 12532.3686, Recon Loss: 25064.7167, KL Loss: 0.0000, Percep Loss: 0.0102\n",
      "Epoch 16/1000, Beta: 3.00, Total Loss: 12532.0759, Recon Loss: 25064.1314, KL Loss: 0.0001, Percep Loss: 0.0101\n",
      "Epoch 17/1000, Beta: 3.20, Total Loss: 12528.5764, Recon Loss: 25057.1328, KL Loss: 0.0001, Percep Loss: 0.0098\n",
      "Epoch 18/1000, Beta: 3.40, Total Loss: 12525.7198, Recon Loss: 25051.4223, KL Loss: 0.0000, Percep Loss: 0.0085\n",
      "Epoch 19/1000, Beta: 3.60, Total Loss: 12524.7389, Recon Loss: 25049.4589, KL Loss: 0.0001, Percep Loss: 0.0091\n",
      "Epoch 20/1000, Beta: 3.80, Total Loss: 12521.7537, Recon Loss: 25043.4926, KL Loss: 0.0000, Percep Loss: 0.0074\n",
      "Epoch 21/1000, Beta: 4.00, Total Loss: 12520.6386, Recon Loss: 25041.2625, KL Loss: 0.0000, Percep Loss: 0.0072\n",
      "Epoch 22/1000, Beta: 4.20, Total Loss: 12519.4018, Recon Loss: 25038.7890, KL Loss: 0.0000, Percep Loss: 0.0072\n",
      "Epoch 23/1000, Beta: 4.40, Total Loss: 12517.5556, Recon Loss: 25035.0978, KL Loss: 0.0000, Percep Loss: 0.0067\n",
      "Epoch 24/1000, Beta: 4.60, Total Loss: 12516.3159, Recon Loss: 25032.6196, KL Loss: 0.0000, Percep Loss: 0.0060\n",
      "Epoch 25/1000, Beta: 4.80, Total Loss: 12516.0952, Recon Loss: 25032.1763, KL Loss: 0.0000, Percep Loss: 0.0070\n",
      "Epoch 26/1000, Beta: 5.00, Total Loss: 12514.5362, Recon Loss: 25029.0611, KL Loss: 0.0000, Percep Loss: 0.0055\n",
      "Epoch 27/1000, Beta: 5.20, Total Loss: 12514.3106, Recon Loss: 25028.6089, KL Loss: 0.0000, Percep Loss: 0.0061\n",
      "Epoch 28/1000, Beta: 5.40, Total Loss: 12512.5494, Recon Loss: 25025.0887, KL Loss: 0.0000, Percep Loss: 0.0050\n",
      "Epoch 29/1000, Beta: 5.60, Total Loss: 12512.3248, Recon Loss: 25024.6388, KL Loss: 0.0000, Percep Loss: 0.0052\n",
      "Epoch 30/1000, Beta: 5.80, Total Loss: 12511.4610, Recon Loss: 25022.9124, KL Loss: 0.0000, Percep Loss: 0.0047\n",
      "Epoch 31/1000, Beta: 6.00, Total Loss: 12511.1876, Recon Loss: 25022.3656, KL Loss: 0.0000, Percep Loss: 0.0047\n",
      "Epoch 32/1000, Beta: 6.20, Total Loss: 12510.4440, Recon Loss: 25020.8785, KL Loss: 0.0000, Percep Loss: 0.0047\n",
      "Epoch 33/1000, Beta: 6.40, Total Loss: 12509.5896, Recon Loss: 25019.1708, KL Loss: 0.0000, Percep Loss: 0.0043\n",
      "Epoch 34/1000, Beta: 6.60, Total Loss: 12509.8336, Recon Loss: 25019.6562, KL Loss: 0.0000, Percep Loss: 0.0054\n",
      "Epoch 35/1000, Beta: 6.80, Total Loss: 12508.9913, Recon Loss: 25017.9740, KL Loss: 0.0000, Percep Loss: 0.0043\n",
      "Epoch 36/1000, Beta: 7.00, Total Loss: 12508.4793, Recon Loss: 25016.9493, KL Loss: 0.0000, Percep Loss: 0.0046\n",
      "Epoch 37/1000, Beta: 7.20, Total Loss: 12507.7280, Recon Loss: 25015.4482, KL Loss: 0.0000, Percep Loss: 0.0038\n",
      "Epoch 38/1000, Beta: 7.40, Total Loss: 12507.4056, Recon Loss: 25014.8030, KL Loss: 0.0000, Percep Loss: 0.0040\n",
      "Epoch 39/1000, Beta: 7.60, Total Loss: 12507.1158, Recon Loss: 25014.2235, KL Loss: 0.0000, Percep Loss: 0.0039\n",
      "Epoch 40/1000, Beta: 7.80, Total Loss: 12507.0099, Recon Loss: 25014.0114, KL Loss: 0.0000, Percep Loss: 0.0042\n",
      "Epoch 41/1000, Beta: 8.00, Total Loss: 12506.1796, Recon Loss: 25012.3524, KL Loss: 0.0000, Percep Loss: 0.0034\n",
      "Epoch 42/1000, Beta: 8.20, Total Loss: 12506.3642, Recon Loss: 25012.7211, KL Loss: 0.0000, Percep Loss: 0.0038\n",
      "Epoch 43/1000, Beta: 8.40, Total Loss: 12505.5126, Recon Loss: 25011.0183, KL Loss: 0.0000, Percep Loss: 0.0036\n",
      "Epoch 44/1000, Beta: 8.60, Total Loss: 12505.7098, Recon Loss: 25011.4115, KL Loss: 0.0000, Percep Loss: 0.0039\n",
      "Epoch 45/1000, Beta: 8.80, Total Loss: 12505.4738, Recon Loss: 25010.9396, KL Loss: 0.0000, Percep Loss: 0.0041\n",
      "Epoch 46/1000, Beta: 9.00, Total Loss: 12505.2264, Recon Loss: 25010.4438, KL Loss: 0.0000, Percep Loss: 0.0045\n",
      "Epoch 47/1000, Beta: 9.20, Total Loss: 12504.6254, Recon Loss: 25009.2442, KL Loss: 0.0000, Percep Loss: 0.0033\n",
      "Epoch 48/1000, Beta: 9.40, Total Loss: 12504.3876, Recon Loss: 25008.7687, KL Loss: 0.0000, Percep Loss: 0.0033\n",
      "Epoch 49/1000, Beta: 9.60, Total Loss: 12503.8617, Recon Loss: 25007.7169, KL Loss: 0.0000, Percep Loss: 0.0032\n",
      "Epoch 50/1000, Beta: 9.80, Total Loss: 12503.5972, Recon Loss: 25007.1882, KL Loss: 0.0000, Percep Loss: 0.0030\n",
      "Saved checkpoint at epoch 50 to FL_VEHICLE_NON_IID\\cvae_vehicle_epoch_50.pth\n",
      "Generated 500 samples for Class 3 (Seden) at epoch 50.\n",
      "Generated 500 samples for Class 4 (SUV) at epoch 50.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms.functional import adjust_sharpness\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import kagglehub\n",
    "from PIL import Image\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "# Set CUDA_LAUNCH_BLOCKING for debugging\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 128  # Increased for better detail capture\n",
    "num_classes = 5\n",
    "batch_size = 32\n",
    "epochs = 1000  # Extended for better convergence\n",
    "learning_rate = 5e-4  # Reduced for stability\n",
    "image_size = 128\n",
    "channels = 3\n",
    "output_dir = \"FL_VEHICLE_NON_IID\"\n",
    "beta_max = 10.0  # Reduced to balance KL loss\n",
    "annealing_epochs = 50  # Gradual increase of KL weight\n",
    "perceptual_weight = 1.0  # Increased to emphasize perceptual quality\n",
    "recon_weight = 0.5  # Slightly reduced to balance losses\n",
    "\n",
    "# Download Vehicle Type Image Dataset from Kaggle\n",
    "try:\n",
    "    path = kagglehub.dataset_download(\"sujaykapadnis/vehicle-type-image-dataset\")\n",
    "    print(\"Path to dataset files:\", path)\n",
    "    dataset_path = path\n",
    "except Exception as e:\n",
    "    print(f\"Failed to download dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Define the VehicleTypeDataset class\n",
    "class VehicleTypeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_names = []\n",
    "        self.class_to_idx = {}\n",
    "\n",
    "        print(f\"Searching for images in {root_dir}\")\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            image_files = [f for f in files if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "            if image_files:\n",
    "                class_name = os.path.basename(root)\n",
    "                if class_name not in self.class_to_idx:\n",
    "                    self.class_names.append(class_name)\n",
    "                    self.class_to_idx[class_name] = len(self.class_names) - 1\n",
    "                for img_file in image_files:\n",
    "                    img_path = os.path.join(root, img_file)\n",
    "                    self.images.append(img_path)\n",
    "                    self.labels.append(self.class_to_idx[class_name])\n",
    "\n",
    "        if not self.images:\n",
    "            raise ValueError(\n",
    "                f\"No images found in {root_dir}. \"\n",
    "                \"Expected class folders containing .jpg, .png, or .jpeg images.\"\n",
    "            )\n",
    "\n",
    "        print(f\"Found {len(self.images)} images across {len(self.class_names)} classes.\")\n",
    "        print(f\"Classes: {self.class_names}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),  # Converts to [0, 1]\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = VehicleTypeDataset(root_dir=dataset_path, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the Encoder network with deeper layers and skip connections\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels + num_classes, 32, kernel_size=4, stride=2, padding=1)  # 32x64x64\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)  # 64x32x32\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)  # 128x16x16\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)  # 256x8x8\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)  # 512x4x4\n",
    "        self.fc_mean = nn.Linear(512 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(512 * 4 * 4, latent_dim)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        y = F.one_hot(y, num_classes=num_classes).float()\n",
    "        y = y.unsqueeze(-1).unsqueeze(-1)\n",
    "        y = y.expand(-1, -1, x.size(2), x.size(3))\n",
    "        x_with_y = torch.cat([x, y], dim=1)\n",
    "        \n",
    "        h1 = F.relu(self.conv1(x_with_y))\n",
    "        if torch.isnan(h1).any() or torch.isinf(h1).any():\n",
    "            print(\"NaN or Inf in h1\")\n",
    "        h2 = F.relu(self.conv2(h1))\n",
    "        if torch.isnan(h2).any() or torch.isinf(h2).any():\n",
    "            print(\"NaN or Inf in h2\")\n",
    "        h3 = F.relu(self.conv3(h2))\n",
    "        if torch.isnan(h3).any() or torch.isinf(h3).any():\n",
    "            print(\"NaN or Inf in h3\")\n",
    "        h4 = F.relu(self.conv4(h3))\n",
    "        if torch.isnan(h4).any() or torch.isinf(h4).any():\n",
    "            print(\"NaN or Inf in h4\")\n",
    "        h5 = F.relu(self.conv5(h4))\n",
    "        if torch.isnan(h5).any() or torch.isinf(h5).any():\n",
    "            print(\"NaN or Inf in h5\")\n",
    "        h = h5.view(h5.size(0), -1)\n",
    "        z_mean = self.fc_mean(h)\n",
    "        z_logvar = self.fc_logvar(h)\n",
    "        return z_mean, z_logvar, (h1, h2, h3, h4, h5)\n",
    "\n",
    "# Define the Decoder network with skip connections\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim + num_classes, 512 * 4 * 4)\n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1)  # 256x8x8\n",
    "        self.deconv2 = nn.ConvTranspose2d(512, 128, kernel_size=4, stride=2, padding=1)  # 128x16x16\n",
    "        self.deconv3 = nn.ConvTranspose2d(256, 64, kernel_size=4, stride=2, padding=1)  # 64x32x32\n",
    "        self.deconv4 = nn.ConvTranspose2d(128, 32, kernel_size=4, stride=2, padding=1)  # 32x64x64\n",
    "        self.deconv5 = nn.ConvTranspose2d(64, channels, kernel_size=4, stride=2, padding=1)  # 3x128x128\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, z, y, skip_connections):\n",
    "        h1, h2, h3, h4, h5 = skip_connections\n",
    "        y = F.one_hot(y, num_classes=num_classes).float()\n",
    "        z_with_y = torch.cat([z, y], dim=-1)\n",
    "        h = F.relu(self.fc(z_with_y))\n",
    "        h = h.view(h.size(0), 512, 4, 4)\n",
    "        \n",
    "        h = F.relu(self.deconv1(h))\n",
    "        h = torch.cat([h, h4], dim=1)\n",
    "        h = F.relu(self.deconv2(h))\n",
    "        h = torch.cat([h, h3], dim=1)\n",
    "        h = F.relu(self.deconv3(h))\n",
    "        h = torch.cat([h, h2], dim=1)\n",
    "        h = F.relu(self.deconv4(h))\n",
    "        h = torch.cat([h, h1], dim=1)\n",
    "        x_reconstructed = torch.sigmoid(self.deconv5(h))\n",
    "        if torch.isnan(x_reconstructed).any() or torch.isinf(x_reconstructed).any():\n",
    "            print(\"NaN or Inf in x_reconstructed\")\n",
    "        return x_reconstructed\n",
    "\n",
    "# Conditional VAE model\n",
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def reparameterize(self, z_mean, z_logvar):\n",
    "        std = torch.exp(0.5 * z_logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return z_mean + eps * std\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        z_mean, z_logvar, skip_connections = self.encoder(x, y)\n",
    "        z = self.reparameterize(z_mean, z_logvar)\n",
    "        x_reconstructed = self.decoder(z, y, skip_connections)\n",
    "        return x_reconstructed, z_mean, z_logvar\n",
    "\n",
    "# Load pretrained VGG16 for perceptual loss\n",
    "vgg = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features.to(device).eval()\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "def perceptual_loss(x, x_reconstructed):\n",
    "    # Normalize inputs to ImageNet mean and std\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(x.device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(x.device)\n",
    "    x_normalized = (x - mean) / std\n",
    "    x_reconstructed_normalized = (x_reconstructed - mean) / std\n",
    "    x_features = vgg(x_normalized)\n",
    "    x_recon_features = vgg(x_reconstructed_normalized)\n",
    "    return F.mse_loss(x_features, x_recon_features)\n",
    "\n",
    "# Instantiate Encoder, Decoder, and CVAE\n",
    "encoder = Encoder(latent_dim, num_classes).to(device)\n",
    "decoder = Decoder(latent_dim, num_classes).to(device)\n",
    "cvae = ConditionalVAE(encoder, decoder).to(device)\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "optimizer = optim.Adam(cvae.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.5)\n",
    "\n",
    "# Define loss function with beta annealing\n",
    "def cvae_loss(x, x_reconstructed, z_mean, z_logvar, beta=1.0, recon_weight=1.0, perceptual_weight=1.0):\n",
    "    if torch.isnan(x).any() or torch.isinf(x).any():\n",
    "        print(\"NaN or Inf detected in x\")\n",
    "    if torch.isnan(x_reconstructed).any() or torch.isinf(x_reconstructed).any():\n",
    "        print(\"NaN or Inf detected in x_reconstructed\")\n",
    "    if torch.isnan(z_mean).any() or torch.isinf(z_mean).any():\n",
    "        print(\"NaN or Inf detected in z_mean\")\n",
    "    if torch.isnan(z_logvar).any() or torch.isinf(z_logvar).any():\n",
    "        print(\"NaN or Inf detected in z_logvar\")\n",
    "\n",
    "    recon_loss = F.binary_cross_entropy(x_reconstructed, x, reduction='sum')\n",
    "    kl_loss = -0.5 * torch.sum(1 + torch.clamp(z_logvar, -10, 10) - z_mean.pow(2) - torch.clamp(z_logvar, -10, 10).exp())\n",
    "    percep_loss = perceptual_loss(x, x_reconstructed) * perceptual_weight\n",
    "    total_loss = recon_weight * recon_loss + beta * kl_loss + percep_loss\n",
    "    return total_loss, recon_loss, kl_loss, percep_loss\n",
    "\n",
    "# Training loop with checkpointing\n",
    "cvae.train()\n",
    "for epoch in range(epochs):\n",
    "    if epoch < annealing_epochs:\n",
    "        beta = beta_max * (epoch / annealing_epochs)\n",
    "    else:\n",
    "        beta = beta_max\n",
    "\n",
    "    train_loss = 0\n",
    "    train_recon_loss = 0\n",
    "    train_kl_loss = 0\n",
    "    train_percep_loss = 0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "       \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        x_reconstructed, z_mean, z_logvar = cvae(data, labels)\n",
    "        total_loss, recon_loss, kl_loss, percep_loss = cvae_loss(\n",
    "            data, x_reconstructed, z_mean, z_logvar, \n",
    "            beta=beta, recon_weight=recon_weight, perceptual_weight=perceptual_weight\n",
    "        )\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(cvae.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        train_loss += total_loss.item()\n",
    "        train_recon_loss += recon_loss.item()\n",
    "        train_kl_loss += kl_loss.item()\n",
    "        train_percep_loss += percep_loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    avg_recon_loss = train_recon_loss / len(train_loader.dataset)\n",
    "    avg_kl_loss = train_kl_loss / len(train_loader.dataset)\n",
    "    avg_percep_loss = train_percep_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Beta: {beta:.2f}, Total Loss: {avg_loss:.4f}, '\n",
    "          f'Recon Loss: {avg_recon_loss:.4f}, KL Loss: {avg_kl_loss:.4f}, '\n",
    "          f'Percep Loss: {avg_percep_loss:.4f}')\n",
    "\n",
    "    # Save checkpoint every 50 epochs\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        checkpoint_path = os.path.join(output_dir, f\"cvae_vehicle_epoch_{epoch + 1}.pth\")\n",
    "        torch.save(cvae.state_dict(), checkpoint_path)\n",
    "        print(f\"Saved checkpoint at epoch {epoch + 1} to {checkpoint_path}\")\n",
    "\n",
    "        # Generate and visualize samples at this checkpoint\n",
    "        cvae.eval()\n",
    "        base_dir = os.path.join(output_dir, f\"generated_samples_epoch_{epoch + 1}\")\n",
    "        classes_to_generate = [3, 4]\n",
    "        num_samples = 500\n",
    "        with torch.no_grad():\n",
    "            for class_label in classes_to_generate:\n",
    "                label_tensor = torch.tensor([class_label]).repeat(num_samples).to(device)\n",
    "                z = torch.randn(num_samples, latent_dim).to(device)\n",
    "                # Use realistic dummy skip connections\n",
    "                dummy_skips = [\n",
    "                    torch.randn(num_samples, 32, 64, 64).to(device) * 0.1,\n",
    "                    torch.randn(num_samples, 64, 32, 32).to(device) * 0.1,\n",
    "                    torch.randn(num_samples, 128, 16, 16).to(device) * 0.1,\n",
    "                    torch.randn(num_samples, 256, 8, 8).to(device) * 0.1,\n",
    "                    torch.randn(num_samples, 512, 4, 4).to(device) * 0.1\n",
    "                ]\n",
    "                generated_samples = cvae.decoder(z, label_tensor, dummy_skips)\n",
    "                class_dir = os.path.join(base_dir, str(class_label))\n",
    "                os.makedirs(class_dir, exist_ok=True)\n",
    "                for idx, sample in enumerate(generated_samples):\n",
    "                    sample = adjust_sharpness(sample, sharpness_factor=2.0)\n",
    "                    save_image(sample, os.path.join(class_dir, f\"sample_{idx}.png\"))\n",
    "                print(f\"Generated {num_samples} samples for Class {class_label} ({dataset.class_names[class_label]}) at epoch {epoch + 1}.\")\n",
    "\n",
    "        # Plot samples\n",
    "        fig, axs = plt.subplots(len(classes_to_generate), 10, figsize=(20, 4))\n",
    "        for row, class_label in enumerate(classes_to_generate):\n",
    "            class_dir = os.path.join(base_dir, str(class_label))\n",
    "            sample_files = os.listdir(class_dir)\n",
    "            random_samples = np.random.choice(sample_files, 10, replace=False)\n",
    "            for col, sample_file in enumerate(random_samples):\n",
    "                sample_path = os.path.join(class_dir, sample_file)\n",
    "                sample_image = Image.open(sample_path).convert(\"RGB\")\n",
    "                sample_image = sample_image.resize((128, 128), Image.LANCZOS)\n",
    "                sample_image = np.array(sample_image) / 255.0\n",
    "                ax = axs[row, col] if len(classes_to_generate) > 1 else axs[col]\n",
    "                ax.imshow(sample_image)\n",
    "                ax.axis('off')\n",
    "                if col == 0:\n",
    "                    ax.set_ylabel(dataset.class_names[class_label], rotation=90, labelpad=10)\n",
    "        plt.tight_layout()\n",
    "        plot_path = os.path.join(output_dir, f\"synthetic_samples_classes_3_4_epoch_{epoch + 1}.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        print(f\"Saved synthetic samples plot at epoch {epoch + 1} to {plot_path}\")\n",
    "        cvae.train()\n",
    "\n",
    "# Final model save\n",
    "final_model_path = os.path.join(output_dir, \"cvae_vehicle_final.pth\")\n",
    "torch.save(cvae.state_dict(), final_model_path)\n",
    "print(f\"Saved final CVAE model to {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ca5051-f21d-425c-a5c0-dd0ddc244b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "##RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae721154-c3c2-474c-806b-713c2609b4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Searching for images in C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Found 4793 images across 5 classes.\n",
      "Classes: ['Hatchback', 'Other', 'Pickup', 'Seden', 'SUV']\n",
      "Number of classes in dataset: 5\n",
      "Classes to generate: [3, 4]\n",
      "Epoch 1/1000, Beta: 0.00, Total Loss: 14687.9958, Recon Loss: 29375.7637, KL Loss: 399.6378, Percep Loss: 0.1140\n",
      "Epoch 2/1000, Beta: 0.10, Total Loss: 12661.4874, Recon Loss: 25322.2727, KL Loss: 2.7964, Percep Loss: 0.0714\n",
      "Epoch 3/1000, Beta: 0.20, Total Loss: 12599.1264, Recon Loss: 25198.1397, KL Loss: 0.0146, Percep Loss: 0.0536\n",
      "Epoch 4/1000, Beta: 0.30, Total Loss: 12569.7557, Recon Loss: 25139.4285, KL Loss: 0.0043, Percep Loss: 0.0402\n",
      "Epoch 5/1000, Beta: 0.40, Total Loss: 12550.0420, Recon Loss: 25100.0218, KL Loss: 0.0011, Percep Loss: 0.0308\n",
      "Epoch 6/1000, Beta: 0.50, Total Loss: 12543.0925, Recon Loss: 25086.1328, KL Loss: 0.0008, Percep Loss: 0.0257\n",
      "Epoch 7/1000, Beta: 0.60, Total Loss: 12533.1656, Recon Loss: 25066.2868, KL Loss: 0.0002, Percep Loss: 0.0222\n",
      "Epoch 8/1000, Beta: 0.70, Total Loss: 12528.9651, Recon Loss: 25057.8914, KL Loss: 0.0002, Percep Loss: 0.0193\n",
      "Epoch 9/1000, Beta: 0.80, Total Loss: 12525.1641, Recon Loss: 25050.2931, KL Loss: 0.0001, Percep Loss: 0.0175\n",
      "Epoch 10/1000, Beta: 0.90, Total Loss: 12523.3518, Recon Loss: 25046.6696, KL Loss: 0.0002, Percep Loss: 0.0169\n",
      "Epoch 11/1000, Beta: 1.00, Total Loss: 12519.2300, Recon Loss: 25038.4314, KL Loss: 0.0001, Percep Loss: 0.0142\n",
      "Epoch 12/1000, Beta: 1.10, Total Loss: 12518.7010, Recon Loss: 25037.3762, KL Loss: 0.0001, Percep Loss: 0.0129\n",
      "Epoch 13/1000, Beta: 1.20, Total Loss: 12515.8179, Recon Loss: 25031.6097, KL Loss: 0.0001, Percep Loss: 0.0130\n",
      "Epoch 14/1000, Beta: 1.30, Total Loss: 12514.3617, Recon Loss: 25028.7006, KL Loss: 0.0001, Percep Loss: 0.0114\n",
      "Epoch 15/1000, Beta: 1.40, Total Loss: 12512.6833, Recon Loss: 25025.3447, KL Loss: 0.0001, Percep Loss: 0.0109\n",
      "Epoch 16/1000, Beta: 1.50, Total Loss: 12511.7663, Recon Loss: 25023.5115, KL Loss: 0.0001, Percep Loss: 0.0105\n",
      "Epoch 17/1000, Beta: 1.60, Total Loss: 12510.4057, Recon Loss: 25020.7925, KL Loss: 0.0001, Percep Loss: 0.0094\n",
      "Epoch 18/1000, Beta: 1.70, Total Loss: 12509.1804, Recon Loss: 25018.3420, KL Loss: 0.0000, Percep Loss: 0.0094\n",
      "Epoch 19/1000, Beta: 1.80, Total Loss: 12508.3532, Recon Loss: 25016.6858, KL Loss: 0.0000, Percep Loss: 0.0102\n",
      "Epoch 20/1000, Beta: 1.90, Total Loss: 12507.4277, Recon Loss: 25014.8403, KL Loss: 0.0000, Percep Loss: 0.0075\n",
      "Epoch 21/1000, Beta: 2.00, Total Loss: 12506.7847, Recon Loss: 25013.5521, KL Loss: 0.0000, Percep Loss: 0.0086\n",
      "Epoch 22/1000, Beta: 2.10, Total Loss: 12506.4308, Recon Loss: 25012.8465, KL Loss: 0.0000, Percep Loss: 0.0075\n",
      "Epoch 23/1000, Beta: 2.20, Total Loss: 12506.0382, Recon Loss: 25012.0611, KL Loss: 0.0000, Percep Loss: 0.0076\n",
      "Epoch 24/1000, Beta: 2.30, Total Loss: 12505.0265, Recon Loss: 25010.0383, KL Loss: 0.0000, Percep Loss: 0.0072\n",
      "Epoch 25/1000, Beta: 2.40, Total Loss: 12504.4941, Recon Loss: 25008.9726, KL Loss: 0.0000, Percep Loss: 0.0078\n",
      "Epoch 26/1000, Beta: 2.50, Total Loss: 12504.0045, Recon Loss: 25007.9954, KL Loss: 0.0000, Percep Loss: 0.0068\n",
      "Epoch 27/1000, Beta: 2.60, Total Loss: 12503.3033, Recon Loss: 25006.5939, KL Loss: 0.0000, Percep Loss: 0.0063\n",
      "Epoch 28/1000, Beta: 2.70, Total Loss: 12503.0741, Recon Loss: 25006.1359, KL Loss: 0.0000, Percep Loss: 0.0062\n",
      "Epoch 29/1000, Beta: 2.80, Total Loss: 12502.6771, Recon Loss: 25005.3419, KL Loss: 0.0000, Percep Loss: 0.0061\n",
      "Epoch 30/1000, Beta: 2.90, Total Loss: 12502.3581, Recon Loss: 25004.7047, KL Loss: 0.0000, Percep Loss: 0.0057\n",
      "Epoch 31/1000, Beta: 3.00, Total Loss: 12502.1140, Recon Loss: 25004.2170, KL Loss: 0.0000, Percep Loss: 0.0055\n",
      "Epoch 32/1000, Beta: 3.10, Total Loss: 12501.5782, Recon Loss: 25003.1457, KL Loss: 0.0000, Percep Loss: 0.0054\n",
      "Epoch 33/1000, Beta: 3.20, Total Loss: 12501.3138, Recon Loss: 25002.6162, KL Loss: 0.0000, Percep Loss: 0.0057\n",
      "Epoch 34/1000, Beta: 3.30, Total Loss: 12501.0332, Recon Loss: 25002.0567, KL Loss: 0.0000, Percep Loss: 0.0048\n",
      "Epoch 35/1000, Beta: 3.40, Total Loss: 12500.6857, Recon Loss: 25001.3613, KL Loss: 0.0000, Percep Loss: 0.0051\n",
      "Epoch 36/1000, Beta: 3.50, Total Loss: 12500.6301, Recon Loss: 25001.2495, KL Loss: 0.0000, Percep Loss: 0.0053\n",
      "Epoch 37/1000, Beta: 3.60, Total Loss: 12500.2391, Recon Loss: 25000.4691, KL Loss: 0.0000, Percep Loss: 0.0045\n",
      "Epoch 38/1000, Beta: 3.70, Total Loss: 12500.1779, Recon Loss: 25000.3460, KL Loss: 0.0000, Percep Loss: 0.0049\n",
      "Epoch 39/1000, Beta: 3.80, Total Loss: 12499.8740, Recon Loss: 24999.7385, KL Loss: 0.0000, Percep Loss: 0.0048\n",
      "Epoch 40/1000, Beta: 3.90, Total Loss: 12499.6481, Recon Loss: 24999.2879, KL Loss: 0.0000, Percep Loss: 0.0042\n",
      "Epoch 41/1000, Beta: 4.00, Total Loss: 12499.3648, Recon Loss: 24998.7212, KL Loss: 0.0000, Percep Loss: 0.0042\n",
      "Epoch 42/1000, Beta: 4.10, Total Loss: 12499.4362, Recon Loss: 24998.8627, KL Loss: 0.0000, Percep Loss: 0.0049\n",
      "Epoch 43/1000, Beta: 4.20, Total Loss: 12499.1849, Recon Loss: 24998.3616, KL Loss: 0.0000, Percep Loss: 0.0041\n",
      "Epoch 44/1000, Beta: 4.30, Total Loss: 12498.9387, Recon Loss: 24997.8675, KL Loss: 0.0000, Percep Loss: 0.0049\n",
      "Epoch 45/1000, Beta: 4.40, Total Loss: 12498.7728, Recon Loss: 24997.5378, KL Loss: 0.0000, Percep Loss: 0.0040\n",
      "Epoch 46/1000, Beta: 4.50, Total Loss: 12498.5368, Recon Loss: 24997.0664, KL Loss: 0.0000, Percep Loss: 0.0036\n",
      "Epoch 47/1000, Beta: 4.60, Total Loss: 12498.5081, Recon Loss: 24997.0084, KL Loss: 0.0000, Percep Loss: 0.0039\n",
      "Epoch 48/1000, Beta: 4.70, Total Loss: 12498.1890, Recon Loss: 24996.3707, KL Loss: 0.0000, Percep Loss: 0.0037\n",
      "Epoch 49/1000, Beta: 4.80, Total Loss: 12497.9762, Recon Loss: 24995.9456, KL Loss: 0.0000, Percep Loss: 0.0034\n",
      "Epoch 50/1000, Beta: 4.90, Total Loss: 12497.9585, Recon Loss: 24995.9099, KL Loss: 0.0000, Percep Loss: 0.0035\n",
      "Epoch 51/1000, Beta: 5.00, Total Loss: 12497.8884, Recon Loss: 24995.7697, KL Loss: 0.0000, Percep Loss: 0.0036\n",
      "Epoch 52/1000, Beta: 5.00, Total Loss: 12497.7674, Recon Loss: 24995.5280, KL Loss: 0.0000, Percep Loss: 0.0035\n",
      "Epoch 53/1000, Beta: 5.00, Total Loss: 12497.7386, Recon Loss: 24995.4698, KL Loss: 0.0000, Percep Loss: 0.0037\n",
      "Epoch 54/1000, Beta: 5.00, Total Loss: 12497.4470, Recon Loss: 24994.8875, KL Loss: 0.0000, Percep Loss: 0.0033\n",
      "Epoch 55/1000, Beta: 5.00, Total Loss: 12497.4824, Recon Loss: 24994.9579, KL Loss: 0.0000, Percep Loss: 0.0034\n",
      "Epoch 56/1000, Beta: 5.00, Total Loss: 12497.2189, Recon Loss: 24994.4313, KL Loss: 0.0000, Percep Loss: 0.0033\n",
      "Epoch 57/1000, Beta: 5.00, Total Loss: 12497.0942, Recon Loss: 24994.1823, KL Loss: 0.0000, Percep Loss: 0.0031\n",
      "Epoch 58/1000, Beta: 5.00, Total Loss: 12497.1934, Recon Loss: 24994.3795, KL Loss: 0.0000, Percep Loss: 0.0036\n",
      "Epoch 59/1000, Beta: 5.00, Total Loss: 12497.0121, Recon Loss: 24994.0177, KL Loss: 0.0000, Percep Loss: 0.0032\n",
      "Epoch 60/1000, Beta: 5.00, Total Loss: 12496.9620, Recon Loss: 24993.9174, KL Loss: 0.0000, Percep Loss: 0.0033\n",
      "Epoch 61/1000, Beta: 5.00, Total Loss: 12496.6919, Recon Loss: 24993.3780, KL Loss: 0.0000, Percep Loss: 0.0030\n",
      "Epoch 62/1000, Beta: 5.00, Total Loss: 12496.8300, Recon Loss: 24993.6534, KL Loss: 0.0000, Percep Loss: 0.0033\n",
      "Epoch 63/1000, Beta: 5.00, Total Loss: 12496.6483, Recon Loss: 24993.2903, KL Loss: 0.0000, Percep Loss: 0.0032\n",
      "Epoch 64/1000, Beta: 5.00, Total Loss: 12496.4874, Recon Loss: 24992.9692, KL Loss: 0.0000, Percep Loss: 0.0028\n",
      "Epoch 65/1000, Beta: 5.00, Total Loss: 12496.4371, Recon Loss: 24992.8685, KL Loss: 0.0000, Percep Loss: 0.0029\n",
      "Epoch 66/1000, Beta: 5.00, Total Loss: 12496.3265, Recon Loss: 24992.6463, KL Loss: 0.0000, Percep Loss: 0.0033\n",
      "Epoch 67/1000, Beta: 5.00, Total Loss: 12496.4489, Recon Loss: 24992.8921, KL Loss: 0.0000, Percep Loss: 0.0029\n",
      "Epoch 68/1000, Beta: 5.00, Total Loss: 12496.2449, Recon Loss: 24992.4840, KL Loss: 0.0000, Percep Loss: 0.0029\n",
      "Epoch 69/1000, Beta: 5.00, Total Loss: 12496.1367, Recon Loss: 24992.2677, KL Loss: 0.0000, Percep Loss: 0.0028\n",
      "Epoch 70/1000, Beta: 5.00, Total Loss: 12496.1192, Recon Loss: 24992.2323, KL Loss: 0.0000, Percep Loss: 0.0031\n",
      "Epoch 71/1000, Beta: 5.00, Total Loss: 12496.1087, Recon Loss: 24992.2109, KL Loss: 0.0000, Percep Loss: 0.0033\n",
      "Epoch 72/1000, Beta: 5.00, Total Loss: 12495.9458, Recon Loss: 24991.8860, KL Loss: 0.0000, Percep Loss: 0.0028\n",
      "Epoch 73/1000, Beta: 5.00, Total Loss: 12495.8641, Recon Loss: 24991.7227, KL Loss: 0.0000, Percep Loss: 0.0028\n",
      "Epoch 74/1000, Beta: 5.00, Total Loss: 12495.7972, Recon Loss: 24991.5893, KL Loss: 0.0000, Percep Loss: 0.0025\n",
      "Epoch 75/1000, Beta: 5.00, Total Loss: 12495.7355, Recon Loss: 24991.4657, KL Loss: 0.0000, Percep Loss: 0.0027\n",
      "Epoch 76/1000, Beta: 5.00, Total Loss: 12495.6763, Recon Loss: 24991.3473, KL Loss: 0.0000, Percep Loss: 0.0026\n",
      "Epoch 77/1000, Beta: 5.00, Total Loss: 12495.7768, Recon Loss: 24991.5484, KL Loss: 0.0000, Percep Loss: 0.0026\n",
      "Epoch 78/1000, Beta: 5.00, Total Loss: 12495.6652, Recon Loss: 24991.3245, KL Loss: 0.0000, Percep Loss: 0.0030\n",
      "Epoch 79/1000, Beta: 5.00, Total Loss: 12495.5016, Recon Loss: 24990.9982, KL Loss: 0.0000, Percep Loss: 0.0025\n",
      "Epoch 80/1000, Beta: 5.00, Total Loss: 12495.4781, Recon Loss: 24990.9509, KL Loss: 0.0000, Percep Loss: 0.0027\n",
      "Epoch 81/1000, Beta: 5.00, Total Loss: 12495.3716, Recon Loss: 24990.7383, KL Loss: 0.0000, Percep Loss: 0.0025\n",
      "Epoch 82/1000, Beta: 5.00, Total Loss: 12495.5260, Recon Loss: 24991.0468, KL Loss: 0.0000, Percep Loss: 0.0025\n",
      "Epoch 83/1000, Beta: 5.00, Total Loss: 12495.3689, Recon Loss: 24990.7331, KL Loss: 0.0000, Percep Loss: 0.0024\n",
      "Epoch 84/1000, Beta: 5.00, Total Loss: 12495.3828, Recon Loss: 24990.7603, KL Loss: 0.0000, Percep Loss: 0.0026\n",
      "Epoch 85/1000, Beta: 5.00, Total Loss: 12495.1757, Recon Loss: 24990.3467, KL Loss: 0.0000, Percep Loss: 0.0024\n",
      "Epoch 86/1000, Beta: 5.00, Total Loss: 12495.1786, Recon Loss: 24990.3527, KL Loss: 0.0000, Percep Loss: 0.0023\n",
      "Epoch 87/1000, Beta: 5.00, Total Loss: 12495.3252, Recon Loss: 24990.6455, KL Loss: 0.0000, Percep Loss: 0.0025\n",
      "Epoch 88/1000, Beta: 5.00, Total Loss: 12495.2965, Recon Loss: 24990.5879, KL Loss: 0.0000, Percep Loss: 0.0026\n",
      "Epoch 89/1000, Beta: 5.00, Total Loss: 12495.0943, Recon Loss: 24990.1835, KL Loss: 0.0000, Percep Loss: 0.0026\n",
      "Epoch 90/1000, Beta: 5.00, Total Loss: 12495.0172, Recon Loss: 24990.0298, KL Loss: 0.0000, Percep Loss: 0.0023\n",
      "Epoch 91/1000, Beta: 5.00, Total Loss: 12494.9388, Recon Loss: 24989.8725, KL Loss: 0.0000, Percep Loss: 0.0025\n",
      "Epoch 92/1000, Beta: 5.00, Total Loss: 12495.0593, Recon Loss: 24990.1139, KL Loss: 0.0000, Percep Loss: 0.0024\n",
      "Epoch 93/1000, Beta: 5.00, Total Loss: 12494.8582, Recon Loss: 24989.7116, KL Loss: 0.0000, Percep Loss: 0.0024\n",
      "Epoch 94/1000, Beta: 5.00, Total Loss: 12494.9311, Recon Loss: 24989.8577, KL Loss: 0.0000, Percep Loss: 0.0022\n",
      "Epoch 95/1000, Beta: 5.00, Total Loss: 12494.7692, Recon Loss: 24989.5338, KL Loss: 0.0000, Percep Loss: 0.0023\n",
      "Epoch 96/1000, Beta: 5.00, Total Loss: 12494.7481, Recon Loss: 24989.4915, KL Loss: 0.0000, Percep Loss: 0.0023\n",
      "Epoch 97/1000, Beta: 5.00, Total Loss: 12494.8500, Recon Loss: 24989.6958, KL Loss: 0.0000, Percep Loss: 0.0021\n",
      "Epoch 98/1000, Beta: 5.00, Total Loss: 12494.7949, Recon Loss: 24989.5852, KL Loss: 0.0000, Percep Loss: 0.0023\n",
      "Epoch 99/1000, Beta: 5.00, Total Loss: 12494.8840, Recon Loss: 24989.7633, KL Loss: 0.0000, Percep Loss: 0.0023\n",
      "Epoch 100/1000, Beta: 5.00, Total Loss: 12494.7407, Recon Loss: 24989.4770, KL Loss: 0.0000, Percep Loss: 0.0022\n",
      "Saved checkpoint at epoch 100 to FL_CVAE\\cvae_vehicle_epoch_100.pth\n",
      "Generated 100 samples for Class 3 (Seden) at epoch 100.\n",
      "Generated 100 samples for Class 4 (SUV) at epoch 100.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms.functional import adjust_sharpness\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import kagglehub\n",
    "from PIL import Image\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Set CUDA_LAUNCH_BLOCKING for debugging\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 128\n",
    "batch_size = 16\n",
    "epochs = 1000\n",
    "learning_rate = 5e-4\n",
    "image_size = 128\n",
    "channels = 3\n",
    "output_dir = \"FL_CVAE\"\n",
    "beta_max = 5.0\n",
    "annealing_epochs = 50\n",
    "perceptual_weight = 1.0\n",
    "recon_weight = 0.5\n",
    "\n",
    "# Download Vehicle Type Image Dataset from Kaggle\n",
    "try:\n",
    "    path = kagglehub.dataset_download(\"sujaykapadnis/vehicle-type-image-dataset\")\n",
    "    print(\"Path to dataset files:\", path)\n",
    "    dataset_path = path\n",
    "except Exception as e:\n",
    "    print(f\"Failed to download dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Define the VehicleTypeDataset class\n",
    "class VehicleTypeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_names = []\n",
    "        self.class_to_idx = {}\n",
    "\n",
    "        print(f\"Searching for images in {root_dir}\")\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            image_files = [f for f in files if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "            if image_files:\n",
    "                class_name = os.path.basename(root)\n",
    "                if class_name not in self.class_to_idx:\n",
    "                    self.class_names.append(class_name)\n",
    "                    self.class_to_idx[class_name] = len(self.class_names) - 1\n",
    "                for img_file in image_files:\n",
    "                    img_path = os.path.join(root, img_file)\n",
    "                    self.images.append(img_path)\n",
    "                    self.labels.append(self.class_to_idx[class_name])\n",
    "\n",
    "        if not self.images:\n",
    "            raise ValueError(\n",
    "                f\"No images found in {root_dir}. \"\n",
    "                \"Expected class folders containing .jpg, .png, or .jpeg images.\"\n",
    "            )\n",
    "\n",
    "        print(f\"Found {len(self.images)} images across {len(self.class_names)} classes.\")\n",
    "        print(f\"Classes: {self.class_names}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = VehicleTypeDataset(root_dir=dataset_path, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Dynamically set num_classes based on the dataset\n",
    "num_classes = len(dataset.class_names)\n",
    "print(f\"Number of classes in dataset: {num_classes}\")\n",
    "\n",
    "# Define classes to generate (choose the last two classes if possible)\n",
    "if num_classes >= 2:\n",
    "    classes_to_generate = [num_classes - 2, num_classes - 1]  # Last two classes\n",
    "else:\n",
    "    classes_to_generate = [0]  # Fallback to the first class if fewer than 2 classes\n",
    "print(f\"Classes to generate: {classes_to_generate}\")\n",
    "\n",
    "# Define the Encoder network\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels + num_classes, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.fc_mean = nn.Linear(512 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(512 * 4 * 4, latent_dim)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        y = F.one_hot(y, num_classes=num_classes).float()\n",
    "        y = y.unsqueeze(-1).unsqueeze(-1)\n",
    "        y = y.expand(-1, -1, x.size(2), x.size(3))\n",
    "        x_with_y = torch.cat([x, y], dim=1)\n",
    "        \n",
    "        h1 = F.relu(self.conv1(x_with_y))\n",
    "        if torch.isnan(h1).any() or torch.isinf(h1).any():\n",
    "            print(\"NaN or Inf in h1\")\n",
    "        h2 = F.relu(self.conv2(h1))\n",
    "        if torch.isnan(h2).any() or torch.isinf(h2).any():\n",
    "            print(\"NaN or Inf in h2\")\n",
    "        h3 = F.relu(self.conv3(h2))\n",
    "        if torch.isnan(h3).any() or torch.isinf(h3).any():\n",
    "            print(\"NaN or Inf in h3\")\n",
    "        h4 = F.relu(self.conv4(h3))\n",
    "        if torch.isnan(h4).any() or torch.isinf(h4).any():\n",
    "            print(\"NaN or Inf in h4\")\n",
    "        h5 = F.relu(self.conv5(h4))\n",
    "        if torch.isnan(h5).any() or torch.isinf(h5).any():\n",
    "            print(\"NaN or Inf in h5\")\n",
    "        h = h5.view(h5.size(0), -1)\n",
    "        z_mean = self.fc_mean(h)\n",
    "        z_logvar = self.fc_logvar(h)\n",
    "        return z_mean, z_logvar, (h1, h2, h3, h4, h5)\n",
    "\n",
    "# Define the Decoder network\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim + num_classes, 512 * 4 * 4)\n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(512, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(256, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(128, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv5 = nn.ConvTranspose2d(64, channels, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, z, y, skip_connections):\n",
    "        h1, h2, h3, h4, h5 = skip_connections\n",
    "        y = F.one_hot(y, num_classes=num_classes).float()\n",
    "        z_with_y = torch.cat([z, y], dim=-1)\n",
    "        h = F.relu(self.fc(z_with_y))\n",
    "        h = h.view(h.size(0), 512, 4, 4)\n",
    "        \n",
    "        h = F.relu(self.deconv1(h))\n",
    "        h = torch.cat([h, h4], dim=1)\n",
    "        h = F.relu(self.deconv2(h))\n",
    "        h = torch.cat([h, h3], dim=1)\n",
    "        h = F.relu(self.deconv3(h))\n",
    "        h = torch.cat([h, h2], dim=1)\n",
    "        h = F.relu(self.deconv4(h))\n",
    "        h = torch.cat([h, h1], dim=1)\n",
    "        x_reconstructed = torch.sigmoid(self.deconv5(h))\n",
    "        if torch.isnan(x_reconstructed).any() or torch.isinf(x_reconstructed).any():\n",
    "            print(\"NaN or Inf in x_reconstructed\")\n",
    "        return x_reconstructed\n",
    "\n",
    "# Conditional VAE model\n",
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def reparameterize(self, z_mean, z_logvar):\n",
    "        std = torch.exp(0.5 * z_logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return z_mean + eps * std\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        z_mean, z_logvar, skip_connections = self.encoder(x, y)\n",
    "        z = self.reparameterize(z_mean, z_logvar)\n",
    "        x_reconstructed = self.decoder(z, y, skip_connections)\n",
    "        return x_reconstructed, z_mean, z_logvar\n",
    "\n",
    "# Load pretrained VGG16 for perceptual loss\n",
    "vgg = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features.to(device).eval()\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "def perceptual_loss(x, x_reconstructed):\n",
    "    x_subset = x[:8]\n",
    "    x_reconstructed_subset = x_reconstructed[:8]\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(x.device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(x.device)\n",
    "    x_normalized = (x_subset - mean) / std\n",
    "    x_reconstructed_normalized = (x_reconstructed_subset - mean) / std\n",
    "    x_features = vgg(x_normalized)\n",
    "    x_recon_features = vgg(x_reconstructed_normalized)\n",
    "    return F.mse_loss(x_features, x_recon_features)\n",
    "\n",
    "# Instantiate Encoder, Decoder, and CVAE\n",
    "encoder = Encoder(latent_dim, num_classes).to(device)\n",
    "decoder = Decoder(latent_dim, num_classes).to(device)\n",
    "cvae = ConditionalVAE(encoder, decoder).to(device)\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "optimizer = optim.Adam(cvae.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.5)\n",
    "\n",
    "# Define loss function with beta annealing\n",
    "def cvae_loss(x, x_reconstructed, z_mean, z_logvar, beta=1.0, recon_weight=1.0, perceptual_weight=1.0):\n",
    "    if torch.isnan(x).any() or torch.isinf(x).any():\n",
    "        print(\"NaN or Inf detected in x\")\n",
    "    if torch.isnan(x_reconstructed).any() or torch.isinf(x_reconstructed).any():\n",
    "        print(\"NaN or Inf detected in x_reconstructed\")\n",
    "    if torch.isnan(z_mean).any() or torch.isinf(z_mean).any():\n",
    "        print(\"NaN or Inf detected in z_mean\")\n",
    "    if torch.isnan(z_logvar).any() or torch.isinf(z_logvar).any():\n",
    "        print(\"NaN or Inf detected in z_logvar\")\n",
    "\n",
    "    recon_loss = F.binary_cross_entropy(x_reconstructed, x, reduction='sum')\n",
    "    kl_loss = -0.5 * torch.sum(1 + torch.clamp(z_logvar, -5, 5) - z_mean.pow(2) - torch.clamp(z_logvar, -5, 5).exp())\n",
    "    percep_loss = perceptual_loss(x, x_reconstructed) * perceptual_weight\n",
    "    total_loss = recon_weight * recon_loss + beta * kl_loss + percep_loss\n",
    "    return total_loss, recon_loss, kl_loss, percep_loss\n",
    "\n",
    "# Training loop with checkpointing\n",
    "cvae.train()\n",
    "for epoch in range(epochs):\n",
    "    if epoch < annealing_epochs:\n",
    "        beta = beta_max * (epoch / annealing_epochs)\n",
    "    else:\n",
    "        beta = beta_max\n",
    "\n",
    "    train_loss = 0\n",
    "    train_recon_loss = 0\n",
    "    train_kl_loss = 0\n",
    "    train_percep_loss = 0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        x_reconstructed, z_mean, z_logvar = cvae(data, labels)\n",
    "        total_loss, recon_loss, kl_loss, percep_loss = cvae_loss(\n",
    "            data, x_reconstructed, z_mean, z_logvar, \n",
    "            beta=beta, recon_weight=recon_weight, perceptual_weight=perceptual_weight\n",
    "        )\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(cvae.parameters(), max_norm=0.5)\n",
    "        train_loss += total_loss.item()\n",
    "        train_recon_loss += recon_loss.item()\n",
    "        train_kl_loss += kl_loss.item()\n",
    "        train_percep_loss += percep_loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    avg_recon_loss = train_recon_loss / len(train_loader.dataset)\n",
    "    avg_kl_loss = train_kl_loss / len(train_loader.dataset)\n",
    "    avg_percep_loss = train_percep_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Beta: {beta:.2f}, Total Loss: {avg_loss:.4f}, '\n",
    "          f'Recon Loss: {avg_recon_loss:.4f}, KL Loss: {avg_kl_loss:.4f}, '\n",
    "          f'Percep Loss: {avg_percep_loss:.4f}')\n",
    "\n",
    "    # Save checkpoint every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        checkpoint_path = os.path.join(output_dir, f\"cvae_vehicle_epoch_{epoch + 1}.pth\")\n",
    "        torch.save(cvae.state_dict(), checkpoint_path)\n",
    "        print(f\"Saved checkpoint at epoch {epoch + 1} to {checkpoint_path}\")\n",
    "\n",
    "        # Generate and visualize samples\n",
    "        cvae.eval()\n",
    "        base_dir = os.path.join(output_dir, f\"generated_samples_epoch_{epoch + 1}\")\n",
    "        num_samples = 100\n",
    "        with torch.no_grad():\n",
    "            for class_label in classes_to_generate:\n",
    "                label_tensor = torch.tensor([class_label]).repeat(num_samples).to(device)\n",
    "                z = torch.randn(num_samples, latent_dim).to(device)\n",
    "                dummy_skips = [\n",
    "                    torch.randn(num_samples, 32, 64, 64).to(device) * 0.1,\n",
    "                    torch.randn(num_samples, 64, 32, 32).to(device) * 0.1,\n",
    "                    torch.randn(num_samples, 128, 16, 16).to(device) * 0.1,\n",
    "                    torch.randn(num_samples, 256, 8, 8).to(device) * 0.1,\n",
    "                    torch.randn(num_samples, 512, 4, 4).to(device) * 0.1\n",
    "                ]\n",
    "                generated_samples = cvae.decoder(z, label_tensor, dummy_skips)\n",
    "                class_dir = os.path.join(base_dir, str(class_label))\n",
    "                os.makedirs(class_dir, exist_ok=True)\n",
    "                for idx, sample in enumerate(generated_samples):\n",
    "                    sample = adjust_sharpness(sample, sharpness_factor=2.0)\n",
    "                    save_image(sample, os.path.join(class_dir, f\"sample_{idx}.png\"))\n",
    "                # Safely access class name\n",
    "                class_name = dataset.class_names[class_label] if class_label < len(dataset.class_names) else f\"Class_{class_label}\"\n",
    "                print(f\"Generated {num_samples} samples for Class {class_label} ({class_name}) at epoch {epoch + 1}.\")\n",
    "\n",
    "        # Plot samples\n",
    "        fig, axs = plt.subplots(len(classes_to_generate), 10, figsize=(20, 4))\n",
    "        for row, class_label in enumerate(classes_to_generate):\n",
    "            class_dir = os.path.join(base_dir, str(class_label))\n",
    "            sample_files = os.listdir(class_dir)\n",
    "            random_samples = np.random.choice(sample_files, 10, replace=False)\n",
    "            for col, sample_file in enumerate(random_samples):\n",
    "                sample_path = os.path.join(class_dir, sample_file)\n",
    "                sample_image = Image.open(sample_path).convert(\"RGB\")\n",
    "                sample_image = sample_image.resize((128, 128), Image.LANCZOS)\n",
    "                sample_image = np.array(sample_image) / 255.0\n",
    "                ax = axs[row, col] if len(classes_to_generate) > 1 else axs[col]\n",
    "                ax.imshow(sample_image)\n",
    "                ax.axis('off')\n",
    "                if col == 0:\n",
    "                    # Safely access class name for plotting\n",
    "                    class_name = dataset.class_names[class_label] if class_label < len(dataset.class_names) else f\"Class_{class_label}\"\n",
    "                    ax.set_ylabel(class_name, rotation=90, labelpad=10)\n",
    "        plt.tight_layout()\n",
    "        plot_path = os.path.join(output_dir, f\"synthetic_samples_classes_{'_'.join(map(str, classes_to_generate))}_epoch_{epoch + 1}.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        print(f\"Saved synthetic samples plot at epoch {epoch + 1} to {plot_path}\")\n",
    "        cvae.train()\n",
    "\n",
    "# Final model save\n",
    "final_model_path = os.path.join(output_dir, \"cvae_vehicle_final.pth\")\n",
    "torch.save(cvae.state_dict(), final_model_path)\n",
    "print(f\"Saved final CVAE model to {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46544546-ef2e-4ace-a784-cb96fefafcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##NOT SAVING AT EVERY 100 RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d0025b-3d9c-45c3-b188-97450c5032a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Searching for images in C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Found 4793 images across 5 classes.\n",
      "Classes: ['Hatchback', 'Other', 'Pickup', 'Seden', 'SUV']\n",
      "Number of classes in dataset: 5\n",
      "Classes to generate: [3, 4]\n",
      "Epoch 1/1000, Beta: 0.00, Total Loss: 21212.9881, Recon Loss: 42425.7422, KL Loss: 568.2313, Percep Loss: 0.1169\n",
      "Epoch 2/1000, Beta: 0.10, Total Loss: 12676.3793, Recon Loss: 25352.1531, KL Loss: 2.2939, Percep Loss: 0.0733\n",
      "Epoch 3/1000, Beta: 0.20, Total Loss: 12612.4983, Recon Loss: 25224.8883, KL Loss: 0.0052, Percep Loss: 0.0532\n",
      "Epoch 4/1000, Beta: 0.30, Total Loss: 12579.3610, Recon Loss: 25158.6395, KL Loss: 0.0021, Percep Loss: 0.0407\n",
      "Epoch 5/1000, Beta: 0.40, Total Loss: 12554.8546, Recon Loss: 25109.6483, KL Loss: 0.0007, Percep Loss: 0.0303\n",
      "Epoch 6/1000, Beta: 0.50, Total Loss: 12542.9959, Recon Loss: 25085.9422, KL Loss: 0.0004, Percep Loss: 0.0247\n",
      "Epoch 7/1000, Beta: 0.60, Total Loss: 12536.2430, Recon Loss: 25072.4419, KL Loss: 0.0002, Percep Loss: 0.0220\n",
      "Epoch 8/1000, Beta: 0.70, Total Loss: 12532.6217, Recon Loss: 25065.2051, KL Loss: 0.0001, Percep Loss: 0.0191\n",
      "Epoch 9/1000, Beta: 0.80, Total Loss: 12527.6013, Recon Loss: 25055.1673, KL Loss: 0.0002, Percep Loss: 0.0175\n",
      "Epoch 10/1000, Beta: 0.90, Total Loss: 12527.7256, Recon Loss: 25055.4193, KL Loss: 0.0004, Percep Loss: 0.0156\n",
      "Epoch 11/1000, Beta: 1.00, Total Loss: 12521.5289, Recon Loss: 25043.0269, KL Loss: 0.0001, Percep Loss: 0.0154\n",
      "Epoch 12/1000, Beta: 1.10, Total Loss: 12518.6915, Recon Loss: 25037.3543, KL Loss: 0.0001, Percep Loss: 0.0143\n",
      "Epoch 13/1000, Beta: 1.20, Total Loss: 12516.4298, Recon Loss: 25032.8352, KL Loss: 0.0001, Percep Loss: 0.0121\n",
      "Epoch 14/1000, Beta: 1.30, Total Loss: 12515.0470, Recon Loss: 25030.0688, KL Loss: 0.0000, Percep Loss: 0.0126\n",
      "Epoch 15/1000, Beta: 1.40, Total Loss: 12513.3428, Recon Loss: 25026.6632, KL Loss: 0.0000, Percep Loss: 0.0112\n",
      "Epoch 16/1000, Beta: 1.50, Total Loss: 12511.8368, Recon Loss: 25023.6527, KL Loss: 0.0000, Percep Loss: 0.0104\n",
      "Epoch 17/1000, Beta: 1.60, Total Loss: 12510.8908, Recon Loss: 25021.7611, KL Loss: 0.0000, Percep Loss: 0.0102\n",
      "Epoch 18/1000, Beta: 1.70, Total Loss: 12510.4705, Recon Loss: 25020.9228, KL Loss: 0.0001, Percep Loss: 0.0089\n",
      "Epoch 19/1000, Beta: 1.80, Total Loss: 12509.0570, Recon Loss: 25018.0958, KL Loss: 0.0000, Percep Loss: 0.0091\n",
      "Epoch 20/1000, Beta: 1.90, Total Loss: 12508.3714, Recon Loss: 25016.7254, KL Loss: 0.0000, Percep Loss: 0.0086\n",
      "Epoch 21/1000, Beta: 2.00, Total Loss: 12507.5027, Recon Loss: 25014.9877, KL Loss: 0.0000, Percep Loss: 0.0088\n",
      "Epoch 22/1000, Beta: 2.10, Total Loss: 12507.5925, Recon Loss: 25015.1690, KL Loss: 0.0000, Percep Loss: 0.0079\n",
      "Epoch 23/1000, Beta: 2.20, Total Loss: 12506.1841, Recon Loss: 25012.3523, KL Loss: 0.0000, Percep Loss: 0.0079\n",
      "Epoch 24/1000, Beta: 2.30, Total Loss: 12505.5322, Recon Loss: 25011.0493, KL Loss: 0.0000, Percep Loss: 0.0075\n",
      "Epoch 25/1000, Beta: 2.40, Total Loss: 12504.9457, Recon Loss: 25009.8779, KL Loss: 0.0000, Percep Loss: 0.0067\n",
      "Epoch 26/1000, Beta: 2.50, Total Loss: 12504.6497, Recon Loss: 25009.2850, KL Loss: 0.0000, Percep Loss: 0.0071\n",
      "Epoch 27/1000, Beta: 2.60, Total Loss: 12504.1487, Recon Loss: 25008.2847, KL Loss: 0.0000, Percep Loss: 0.0063\n",
      "Epoch 28/1000, Beta: 2.70, Total Loss: 12503.9023, Recon Loss: 25007.7917, KL Loss: 0.0000, Percep Loss: 0.0064\n",
      "Epoch 29/1000, Beta: 2.80, Total Loss: 12503.4481, Recon Loss: 25006.8829, KL Loss: 0.0000, Percep Loss: 0.0065\n",
      "Epoch 30/1000, Beta: 2.90, Total Loss: 12502.9323, Recon Loss: 25005.8525, KL Loss: 0.0000, Percep Loss: 0.0060\n",
      "Epoch 31/1000, Beta: 3.00, Total Loss: 12502.6542, Recon Loss: 25005.2955, KL Loss: 0.0000, Percep Loss: 0.0064\n",
      "Epoch 32/1000, Beta: 3.10, Total Loss: 12502.1125, Recon Loss: 25004.2139, KL Loss: 0.0000, Percep Loss: 0.0055\n",
      "Epoch 33/1000, Beta: 3.20, Total Loss: 12501.8268, Recon Loss: 25003.6428, KL Loss: 0.0000, Percep Loss: 0.0054\n",
      "Epoch 34/1000, Beta: 3.30, Total Loss: 12501.7676, Recon Loss: 25003.5235, KL Loss: 0.0000, Percep Loss: 0.0058\n",
      "Epoch 35/1000, Beta: 3.40, Total Loss: 12501.5169, Recon Loss: 25003.0227, KL Loss: 0.0000, Percep Loss: 0.0055\n",
      "Epoch 36/1000, Beta: 3.50, Total Loss: 12501.0837, Recon Loss: 25002.1569, KL Loss: 0.0000, Percep Loss: 0.0051\n",
      "Epoch 37/1000, Beta: 3.60, Total Loss: 12500.7728, Recon Loss: 25001.5356, KL Loss: 0.0000, Percep Loss: 0.0050\n",
      "Epoch 38/1000, Beta: 3.70, Total Loss: 12500.7057, Recon Loss: 25001.4006, KL Loss: 0.0000, Percep Loss: 0.0053\n",
      "Epoch 39/1000, Beta: 3.80, Total Loss: 12500.5454, Recon Loss: 25001.0804, KL Loss: 0.0000, Percep Loss: 0.0052\n",
      "Epoch 40/1000, Beta: 3.90, Total Loss: 12500.4387, Recon Loss: 25000.8677, KL Loss: 0.0000, Percep Loss: 0.0048\n",
      "Epoch 41/1000, Beta: 4.00, Total Loss: 12500.0699, Recon Loss: 25000.1300, KL Loss: 0.0000, Percep Loss: 0.0049\n",
      "Epoch 42/1000, Beta: 4.10, Total Loss: 12500.0823, Recon Loss: 25000.1552, KL Loss: 0.0000, Percep Loss: 0.0047\n",
      "Epoch 43/1000, Beta: 4.20, Total Loss: 12499.8162, Recon Loss: 24999.6227, KL Loss: 0.0000, Percep Loss: 0.0048\n",
      "Epoch 44/1000, Beta: 4.30, Total Loss: 12499.6021, Recon Loss: 24999.1954, KL Loss: 0.0000, Percep Loss: 0.0044\n",
      "Epoch 45/1000, Beta: 4.40, Total Loss: 12499.3670, Recon Loss: 24998.7253, KL Loss: 0.0000, Percep Loss: 0.0044\n",
      "Epoch 46/1000, Beta: 4.50, Total Loss: 12499.3493, Recon Loss: 24998.6900, KL Loss: 0.0000, Percep Loss: 0.0043\n",
      "Epoch 47/1000, Beta: 4.60, Total Loss: 12499.1747, Recon Loss: 24998.3406, KL Loss: 0.0000, Percep Loss: 0.0044\n",
      "Epoch 48/1000, Beta: 4.70, Total Loss: 12498.9361, Recon Loss: 24997.8632, KL Loss: 0.0000, Percep Loss: 0.0045\n",
      "Epoch 49/1000, Beta: 4.80, Total Loss: 12498.8373, Recon Loss: 24997.6658, KL Loss: 0.0000, Percep Loss: 0.0044\n",
      "Epoch 50/1000, Beta: 4.90, Total Loss: 12498.6162, Recon Loss: 24997.2245, KL Loss: 0.0000, Percep Loss: 0.0040\n",
      "Epoch 51/1000, Beta: 5.00, Total Loss: 12498.6110, Recon Loss: 24997.2137, KL Loss: 0.0000, Percep Loss: 0.0041\n",
      "Epoch 52/1000, Beta: 5.00, Total Loss: 12498.5332, Recon Loss: 24997.0579, KL Loss: 0.0000, Percep Loss: 0.0042\n",
      "Epoch 53/1000, Beta: 5.00, Total Loss: 12498.3369, Recon Loss: 24996.6660, KL Loss: 0.0000, Percep Loss: 0.0039\n",
      "Epoch 54/1000, Beta: 5.00, Total Loss: 12498.2821, Recon Loss: 24996.5562, KL Loss: 0.0000, Percep Loss: 0.0040\n",
      "Epoch 55/1000, Beta: 5.00, Total Loss: 12498.2096, Recon Loss: 24996.4114, KL Loss: 0.0000, Percep Loss: 0.0039\n",
      "Epoch 56/1000, Beta: 5.00, Total Loss: 12498.2271, Recon Loss: 24996.4459, KL Loss: 0.0000, Percep Loss: 0.0042\n",
      "Epoch 57/1000, Beta: 5.00, Total Loss: 12498.0793, Recon Loss: 24996.1502, KL Loss: 0.0000, Percep Loss: 0.0041\n",
      "Epoch 58/1000, Beta: 5.00, Total Loss: 12497.9182, Recon Loss: 24995.8290, KL Loss: 0.0000, Percep Loss: 0.0036\n",
      "Epoch 59/1000, Beta: 5.00, Total Loss: 12497.8142, Recon Loss: 24995.6205, KL Loss: 0.0000, Percep Loss: 0.0039\n",
      "Epoch 60/1000, Beta: 5.00, Total Loss: 12497.6228, Recon Loss: 24995.2380, KL Loss: 0.0000, Percep Loss: 0.0038\n",
      "Epoch 61/1000, Beta: 5.00, Total Loss: 12497.5772, Recon Loss: 24995.1466, KL Loss: 0.0000, Percep Loss: 0.0039\n",
      "Epoch 62/1000, Beta: 5.00, Total Loss: 12497.4301, Recon Loss: 24994.8535, KL Loss: 0.0000, Percep Loss: 0.0034\n",
      "Epoch 63/1000, Beta: 5.00, Total Loss: 12497.3631, Recon Loss: 24994.7193, KL Loss: 0.0000, Percep Loss: 0.0035\n",
      "Epoch 64/1000, Beta: 5.00, Total Loss: 12497.2400, Recon Loss: 24994.4733, KL Loss: 0.0000, Percep Loss: 0.0033\n",
      "Epoch 65/1000, Beta: 5.00, Total Loss: 12497.0819, Recon Loss: 24994.1567, KL Loss: 0.0000, Percep Loss: 0.0034\n",
      "Epoch 66/1000, Beta: 5.00, Total Loss: 12497.1127, Recon Loss: 24994.2187, KL Loss: 0.0000, Percep Loss: 0.0034\n",
      "Epoch 67/1000, Beta: 5.00, Total Loss: 12497.0787, Recon Loss: 24994.1503, KL Loss: 0.0000, Percep Loss: 0.0035\n",
      "Epoch 68/1000, Beta: 5.00, Total Loss: 12497.0289, Recon Loss: 24994.0508, KL Loss: 0.0000, Percep Loss: 0.0034\n",
      "Epoch 69/1000, Beta: 5.00, Total Loss: 12496.9835, Recon Loss: 24993.9601, KL Loss: 0.0000, Percep Loss: 0.0034\n",
      "Epoch 70/1000, Beta: 5.00, Total Loss: 12496.8597, Recon Loss: 24993.7122, KL Loss: 0.0000, Percep Loss: 0.0036\n",
      "Epoch 71/1000, Beta: 5.00, Total Loss: 12496.7717, Recon Loss: 24993.5365, KL Loss: 0.0000, Percep Loss: 0.0035\n",
      "Epoch 72/1000, Beta: 5.00, Total Loss: 12496.9069, Recon Loss: 24993.8070, KL Loss: 0.0000, Percep Loss: 0.0034\n",
      "Epoch 73/1000, Beta: 5.00, Total Loss: 12496.6309, Recon Loss: 24993.2548, KL Loss: 0.0000, Percep Loss: 0.0034\n",
      "Epoch 74/1000, Beta: 5.00, Total Loss: 12496.5737, Recon Loss: 24993.1410, KL Loss: 0.0000, Percep Loss: 0.0032\n",
      "Epoch 75/1000, Beta: 5.00, Total Loss: 12496.5870, Recon Loss: 24993.1678, KL Loss: 0.0000, Percep Loss: 0.0031\n",
      "Epoch 76/1000, Beta: 5.00, Total Loss: 12496.6878, Recon Loss: 24993.3688, KL Loss: 0.0000, Percep Loss: 0.0034\n",
      "Epoch 77/1000, Beta: 5.00, Total Loss: 12496.4740, Recon Loss: 24992.9410, KL Loss: 0.0000, Percep Loss: 0.0036\n",
      "Epoch 78/1000, Beta: 5.00, Total Loss: 12496.3928, Recon Loss: 24992.7790, KL Loss: 0.0000, Percep Loss: 0.0032\n",
      "Epoch 79/1000, Beta: 5.00, Total Loss: 12496.3614, Recon Loss: 24992.7166, KL Loss: 0.0000, Percep Loss: 0.0031\n",
      "Epoch 80/1000, Beta: 5.00, Total Loss: 12496.2433, Recon Loss: 24992.4809, KL Loss: 0.0000, Percep Loss: 0.0029\n",
      "Epoch 81/1000, Beta: 5.00, Total Loss: 12496.2534, Recon Loss: 24992.5003, KL Loss: 0.0000, Percep Loss: 0.0032\n",
      "Epoch 82/1000, Beta: 5.00, Total Loss: 12496.2559, Recon Loss: 24992.5057, KL Loss: 0.0000, Percep Loss: 0.0030\n",
      "Epoch 83/1000, Beta: 5.00, Total Loss: 12496.1458, Recon Loss: 24992.2853, KL Loss: 0.0000, Percep Loss: 0.0031\n",
      "Epoch 84/1000, Beta: 5.00, Total Loss: 12496.1173, Recon Loss: 24992.2284, KL Loss: 0.0000, Percep Loss: 0.0030\n",
      "Epoch 85/1000, Beta: 5.00, Total Loss: 12496.0497, Recon Loss: 24992.0927, KL Loss: 0.0000, Percep Loss: 0.0033\n",
      "Epoch 86/1000, Beta: 5.00, Total Loss: 12496.0842, Recon Loss: 24992.1623, KL Loss: 0.0000, Percep Loss: 0.0030\n",
      "Epoch 87/1000, Beta: 5.00, Total Loss: 12496.1630, Recon Loss: 24992.3202, KL Loss: 0.0000, Percep Loss: 0.0029\n",
      "Epoch 88/1000, Beta: 5.00, Total Loss: 12495.8705, Recon Loss: 24991.7351, KL Loss: 0.0000, Percep Loss: 0.0030\n",
      "Epoch 89/1000, Beta: 5.00, Total Loss: 12496.0489, Recon Loss: 24992.0918, KL Loss: 0.0000, Percep Loss: 0.0029\n",
      "Epoch 90/1000, Beta: 5.00, Total Loss: 12495.7637, Recon Loss: 24991.5221, KL Loss: 0.0000, Percep Loss: 0.0027\n",
      "Epoch 91/1000, Beta: 5.00, Total Loss: 12495.8881, Recon Loss: 24991.7702, KL Loss: 0.0000, Percep Loss: 0.0030\n",
      "Epoch 92/1000, Beta: 5.00, Total Loss: 12495.7646, Recon Loss: 24991.5234, KL Loss: 0.0000, Percep Loss: 0.0029\n",
      "Epoch 93/1000, Beta: 5.00, Total Loss: 12495.8616, Recon Loss: 24991.7173, KL Loss: 0.0000, Percep Loss: 0.0029\n",
      "Epoch 94/1000, Beta: 5.00, Total Loss: 12495.7827, Recon Loss: 24991.5593, KL Loss: 0.0000, Percep Loss: 0.0030\n",
      "Epoch 95/1000, Beta: 5.00, Total Loss: 12495.7806, Recon Loss: 24991.5552, KL Loss: 0.0000, Percep Loss: 0.0030\n",
      "Epoch 96/1000, Beta: 5.00, Total Loss: 12495.6571, Recon Loss: 24991.3079, KL Loss: 0.0000, Percep Loss: 0.0031\n",
      "Epoch 97/1000, Beta: 5.00, Total Loss: 12495.7513, Recon Loss: 24991.4964, KL Loss: 0.0000, Percep Loss: 0.0031\n",
      "Epoch 98/1000, Beta: 5.00, Total Loss: 12495.5814, Recon Loss: 24991.1571, KL Loss: 0.0000, Percep Loss: 0.0029\n",
      "Epoch 99/1000, Beta: 5.00, Total Loss: 12495.4855, Recon Loss: 24990.9658, KL Loss: 0.0000, Percep Loss: 0.0026\n",
      "Epoch 100/1000, Beta: 5.00, Total Loss: 12495.6567, Recon Loss: 24991.3078, KL Loss: 0.0000, Percep Loss: 0.0028\n",
      "Epoch 101/1000, Beta: 5.00, Total Loss: 12495.5392, Recon Loss: 24991.0723, KL Loss: 0.0000, Percep Loss: 0.0030\n",
      "Epoch 102/1000, Beta: 5.00, Total Loss: 12495.3953, Recon Loss: 24990.7855, KL Loss: 0.0000, Percep Loss: 0.0026\n",
      "Epoch 103/1000, Beta: 5.00, Total Loss: 12495.4330, Recon Loss: 24990.8608, KL Loss: 0.0000, Percep Loss: 0.0026\n",
      "Epoch 104/1000, Beta: 5.00, Total Loss: 12495.4263, Recon Loss: 24990.8473, KL Loss: 0.0000, Percep Loss: 0.0027\n",
      "Epoch 105/1000, Beta: 5.00, Total Loss: 12495.3142, Recon Loss: 24990.6233, KL Loss: 0.0000, Percep Loss: 0.0025\n",
      "Epoch 106/1000, Beta: 5.00, Total Loss: 12495.3777, Recon Loss: 24990.7498, KL Loss: 0.0000, Percep Loss: 0.0028\n",
      "Epoch 107/1000, Beta: 5.00, Total Loss: 12495.3152, Recon Loss: 24990.6253, KL Loss: 0.0000, Percep Loss: 0.0026\n",
      "Epoch 108/1000, Beta: 5.00, Total Loss: 12495.3336, Recon Loss: 24990.6620, KL Loss: 0.0000, Percep Loss: 0.0026\n",
      "Epoch 109/1000, Beta: 5.00, Total Loss: 12495.2140, Recon Loss: 24990.4231, KL Loss: 0.0000, Percep Loss: 0.0025\n",
      "Epoch 110/1000, Beta: 5.00, Total Loss: 12495.2172, Recon Loss: 24990.4291, KL Loss: 0.0000, Percep Loss: 0.0027\n",
      "Epoch 111/1000, Beta: 5.00, Total Loss: 12495.1952, Recon Loss: 24990.3848, KL Loss: 0.0000, Percep Loss: 0.0029\n",
      "Epoch 112/1000, Beta: 5.00, Total Loss: 12495.1463, Recon Loss: 24990.2874, KL Loss: 0.0000, Percep Loss: 0.0027\n",
      "Epoch 113/1000, Beta: 5.00, Total Loss: 12495.0799, Recon Loss: 24990.1549, KL Loss: 0.0000, Percep Loss: 0.0024\n",
      "Epoch 114/1000, Beta: 5.00, Total Loss: 12495.0348, Recon Loss: 24990.0650, KL Loss: 0.0000, Percep Loss: 0.0024\n",
      "Epoch 115/1000, Beta: 5.00, Total Loss: 12495.0221, Recon Loss: 24990.0393, KL Loss: 0.0000, Percep Loss: 0.0025\n",
      "Epoch 116/1000, Beta: 5.00, Total Loss: 12495.0142, Recon Loss: 24990.0235, KL Loss: 0.0000, Percep Loss: 0.0025\n",
      "Epoch 117/1000, Beta: 5.00, Total Loss: 12495.0009, Recon Loss: 24989.9969, KL Loss: 0.0000, Percep Loss: 0.0024\n",
      "Epoch 118/1000, Beta: 5.00, Total Loss: 12494.9625, Recon Loss: 24989.9195, KL Loss: 0.0000, Percep Loss: 0.0028\n",
      "Epoch 119/1000, Beta: 5.00, Total Loss: 12494.9470, Recon Loss: 24989.8892, KL Loss: 0.0000, Percep Loss: 0.0024\n",
      "Epoch 120/1000, Beta: 5.00, Total Loss: 12495.0371, Recon Loss: 24990.0695, KL Loss: 0.0000, Percep Loss: 0.0024\n",
      "Epoch 121/1000, Beta: 5.00, Total Loss: 12495.0273, Recon Loss: 24990.0497, KL Loss: 0.0000, Percep Loss: 0.0024\n",
      "Epoch 122/1000, Beta: 5.00, Total Loss: 12494.8255, Recon Loss: 24989.6463, KL Loss: 0.0000, Percep Loss: 0.0024\n",
      "Epoch 123/1000, Beta: 5.00, Total Loss: 12494.8191, Recon Loss: 24989.6331, KL Loss: 0.0000, Percep Loss: 0.0025\n",
      "Epoch 124/1000, Beta: 5.00, Total Loss: 12494.8217, Recon Loss: 24989.6386, KL Loss: 0.0000, Percep Loss: 0.0024\n",
      "Epoch 125/1000, Beta: 5.00, Total Loss: 12494.7058, Recon Loss: 24989.4068, KL Loss: 0.0000, Percep Loss: 0.0025\n",
      "Epoch 126/1000, Beta: 5.00, Total Loss: 12494.8478, Recon Loss: 24989.6909, KL Loss: 0.0000, Percep Loss: 0.0025\n",
      "Epoch 127/1000, Beta: 5.00, Total Loss: 12494.8261, Recon Loss: 24989.6474, KL Loss: 0.0000, Percep Loss: 0.0024\n",
      "Epoch 128/1000, Beta: 5.00, Total Loss: 12494.7289, Recon Loss: 24989.4526, KL Loss: 0.0000, Percep Loss: 0.0027\n",
      "Epoch 129/1000, Beta: 5.00, Total Loss: 12494.7925, Recon Loss: 24989.5802, KL Loss: 0.0000, Percep Loss: 0.0024\n",
      "Epoch 130/1000, Beta: 5.00, Total Loss: 12494.6768, Recon Loss: 24989.3483, KL Loss: 0.0000, Percep Loss: 0.0026\n",
      "Epoch 131/1000, Beta: 5.00, Total Loss: 12494.6563, Recon Loss: 24989.3077, KL Loss: 0.0000, Percep Loss: 0.0025\n",
      "Epoch 132/1000, Beta: 5.00, Total Loss: 12494.5725, Recon Loss: 24989.1407, KL Loss: 0.0000, Percep Loss: 0.0021\n",
      "Epoch 133/1000, Beta: 5.00, Total Loss: 12494.7476, Recon Loss: 24989.4905, KL Loss: 0.0000, Percep Loss: 0.0024\n",
      "Epoch 134/1000, Beta: 5.00, Total Loss: 12494.6372, Recon Loss: 24989.2698, KL Loss: 0.0000, Percep Loss: 0.0022\n",
      "Epoch 135/1000, Beta: 5.00, Total Loss: 12494.4673, Recon Loss: 24988.9302, KL Loss: 0.0000, Percep Loss: 0.0022\n",
      "Epoch 136/1000, Beta: 5.00, Total Loss: 12494.6459, Recon Loss: 24989.2867, KL Loss: 0.0000, Percep Loss: 0.0023\n",
      "Epoch 137/1000, Beta: 5.00, Total Loss: 12494.5275, Recon Loss: 24989.0504, KL Loss: 0.0000, Percep Loss: 0.0022\n",
      "Epoch 138/1000, Beta: 5.00, Total Loss: 12494.4286, Recon Loss: 24988.8529, KL Loss: 0.0000, Percep Loss: 0.0021\n",
      "Epoch 139/1000, Beta: 5.00, Total Loss: 12494.4877, Recon Loss: 24988.9704, KL Loss: 0.0000, Percep Loss: 0.0025\n",
      "Epoch 140/1000, Beta: 5.00, Total Loss: 12494.3786, Recon Loss: 24988.7529, KL Loss: 0.0000, Percep Loss: 0.0021\n",
      "Epoch 141/1000, Beta: 5.00, Total Loss: 12494.4338, Recon Loss: 24988.8632, KL Loss: 0.0000, Percep Loss: 0.0022\n",
      "Epoch 142/1000, Beta: 5.00, Total Loss: 12494.3399, Recon Loss: 24988.6754, KL Loss: 0.0000, Percep Loss: 0.0022\n",
      "Epoch 143/1000, Beta: 5.00, Total Loss: 12494.2810, Recon Loss: 24988.5579, KL Loss: 0.0000, Percep Loss: 0.0020\n",
      "Epoch 144/1000, Beta: 5.00, Total Loss: 12494.3149, Recon Loss: 24988.6254, KL Loss: 0.0000, Percep Loss: 0.0022\n",
      "Epoch 145/1000, Beta: 5.00, Total Loss: 12494.2919, Recon Loss: 24988.5793, KL Loss: 0.0000, Percep Loss: 0.0022\n",
      "Epoch 146/1000, Beta: 5.00, Total Loss: 12494.2170, Recon Loss: 24988.4299, KL Loss: 0.0000, Percep Loss: 0.0021\n",
      "Epoch 147/1000, Beta: 5.00, Total Loss: 12494.3533, Recon Loss: 24988.7021, KL Loss: 0.0000, Percep Loss: 0.0022\n",
      "Epoch 148/1000, Beta: 5.00, Total Loss: 12494.2088, Recon Loss: 24988.4136, KL Loss: 0.0000, Percep Loss: 0.0020\n",
      "Epoch 149/1000, Beta: 5.00, Total Loss: 12494.1732, Recon Loss: 24988.3418, KL Loss: 0.0000, Percep Loss: 0.0022\n",
      "Epoch 150/1000, Beta: 5.00, Total Loss: 12494.1338, Recon Loss: 24988.2630, KL Loss: 0.0000, Percep Loss: 0.0023\n",
      "Epoch 151/1000, Beta: 5.00, Total Loss: 12494.1515, Recon Loss: 24988.2987, KL Loss: 0.0000, Percep Loss: 0.0021\n",
      "Epoch 152/1000, Beta: 5.00, Total Loss: 12494.1395, Recon Loss: 24988.2751, KL Loss: 0.0000, Percep Loss: 0.0019\n",
      "Epoch 153/1000, Beta: 5.00, Total Loss: 12494.2122, Recon Loss: 24988.4202, KL Loss: 0.0000, Percep Loss: 0.0021\n",
      "Epoch 154/1000, Beta: 5.00, Total Loss: 12494.1123, Recon Loss: 24988.2208, KL Loss: 0.0000, Percep Loss: 0.0019\n",
      "Epoch 155/1000, Beta: 5.00, Total Loss: 12494.1061, Recon Loss: 24988.2082, KL Loss: 0.0000, Percep Loss: 0.0019\n",
      "Epoch 156/1000, Beta: 5.00, Total Loss: 12494.1332, Recon Loss: 24988.2623, KL Loss: 0.0000, Percep Loss: 0.0020\n",
      "Epoch 157/1000, Beta: 5.00, Total Loss: 12494.0168, Recon Loss: 24988.0294, KL Loss: 0.0000, Percep Loss: 0.0021\n",
      "Epoch 158/1000, Beta: 5.00, Total Loss: 12493.9568, Recon Loss: 24987.9096, KL Loss: 0.0000, Percep Loss: 0.0020\n",
      "Epoch 159/1000, Beta: 5.00, Total Loss: 12493.9813, Recon Loss: 24987.9585, KL Loss: 0.0000, Percep Loss: 0.0020\n",
      "Epoch 160/1000, Beta: 5.00, Total Loss: 12494.0049, Recon Loss: 24988.0051, KL Loss: 0.0000, Percep Loss: 0.0024\n",
      "Epoch 161/1000, Beta: 5.00, Total Loss: 12493.9800, Recon Loss: 24987.9561, KL Loss: 0.0000, Percep Loss: 0.0019\n",
      "Epoch 162/1000, Beta: 5.00, Total Loss: 12494.0060, Recon Loss: 24988.0078, KL Loss: 0.0000, Percep Loss: 0.0021\n",
      "Epoch 163/1000, Beta: 5.00, Total Loss: 12493.9562, Recon Loss: 24987.9085, KL Loss: 0.0000, Percep Loss: 0.0019\n",
      "Epoch 164/1000, Beta: 5.00, Total Loss: 12493.9417, Recon Loss: 24987.8796, KL Loss: 0.0000, Percep Loss: 0.0019\n",
      "Epoch 165/1000, Beta: 5.00, Total Loss: 12494.1414, Recon Loss: 24988.2777, KL Loss: 0.0000, Percep Loss: 0.0025\n",
      "Epoch 166/1000, Beta: 5.00, Total Loss: 12493.8456, Recon Loss: 24987.6870, KL Loss: 0.0000, Percep Loss: 0.0021\n",
      "Epoch 167/1000, Beta: 5.00, Total Loss: 12493.8777, Recon Loss: 24987.7513, KL Loss: 0.0000, Percep Loss: 0.0020\n",
      "Epoch 168/1000, Beta: 5.00, Total Loss: 12493.8469, Recon Loss: 24987.6892, KL Loss: 0.0000, Percep Loss: 0.0023\n",
      "Epoch 169/1000, Beta: 5.00, Total Loss: 12493.8279, Recon Loss: 24987.6521, KL Loss: 0.0000, Percep Loss: 0.0019\n",
      "Epoch 170/1000, Beta: 5.00, Total Loss: 12493.8450, Recon Loss: 24987.6858, KL Loss: 0.0000, Percep Loss: 0.0021\n",
      "Epoch 171/1000, Beta: 5.00, Total Loss: 12493.7675, Recon Loss: 24987.5311, KL Loss: 0.0000, Percep Loss: 0.0019\n",
      "Epoch 172/1000, Beta: 5.00, Total Loss: 12493.8876, Recon Loss: 24987.7711, KL Loss: 0.0000, Percep Loss: 0.0020\n",
      "Epoch 173/1000, Beta: 5.00, Total Loss: 12493.7814, Recon Loss: 24987.5592, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 174/1000, Beta: 5.00, Total Loss: 12493.7676, Recon Loss: 24987.5311, KL Loss: 0.0000, Percep Loss: 0.0020\n",
      "Epoch 175/1000, Beta: 5.00, Total Loss: 12493.6980, Recon Loss: 24987.3919, KL Loss: 0.0000, Percep Loss: 0.0020\n",
      "Epoch 176/1000, Beta: 5.00, Total Loss: 12493.8236, Recon Loss: 24987.6438, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 177/1000, Beta: 5.00, Total Loss: 12493.7243, Recon Loss: 24987.4450, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 178/1000, Beta: 5.00, Total Loss: 12493.7617, Recon Loss: 24987.5198, KL Loss: 0.0000, Percep Loss: 0.0018\n",
      "Epoch 179/1000, Beta: 5.00, Total Loss: 12493.7486, Recon Loss: 24987.4935, KL Loss: 0.0000, Percep Loss: 0.0018\n",
      "Epoch 180/1000, Beta: 5.00, Total Loss: 12493.6793, Recon Loss: 24987.3549, KL Loss: 0.0000, Percep Loss: 0.0018\n",
      "Epoch 181/1000, Beta: 5.00, Total Loss: 12493.7616, Recon Loss: 24987.5192, KL Loss: 0.0000, Percep Loss: 0.0020\n",
      "Epoch 182/1000, Beta: 5.00, Total Loss: 12493.6734, Recon Loss: 24987.3425, KL Loss: 0.0000, Percep Loss: 0.0021\n",
      "Epoch 183/1000, Beta: 5.00, Total Loss: 12493.6897, Recon Loss: 24987.3756, KL Loss: 0.0000, Percep Loss: 0.0018\n",
      "Epoch 184/1000, Beta: 5.00, Total Loss: 12493.6030, Recon Loss: 24987.2025, KL Loss: 0.0000, Percep Loss: 0.0018\n",
      "Epoch 185/1000, Beta: 5.00, Total Loss: 12493.6719, Recon Loss: 24987.3396, KL Loss: 0.0000, Percep Loss: 0.0022\n",
      "Epoch 186/1000, Beta: 5.00, Total Loss: 12493.6361, Recon Loss: 24987.2687, KL Loss: 0.0000, Percep Loss: 0.0018\n",
      "Epoch 187/1000, Beta: 5.00, Total Loss: 12493.5484, Recon Loss: 24987.0928, KL Loss: 0.0000, Percep Loss: 0.0020\n",
      "Epoch 188/1000, Beta: 5.00, Total Loss: 12493.5406, Recon Loss: 24987.0778, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 189/1000, Beta: 5.00, Total Loss: 12493.5160, Recon Loss: 24987.0283, KL Loss: 0.0000, Percep Loss: 0.0018\n",
      "Epoch 190/1000, Beta: 5.00, Total Loss: 12493.5982, Recon Loss: 24987.1926, KL Loss: 0.0000, Percep Loss: 0.0020\n",
      "Epoch 191/1000, Beta: 5.00, Total Loss: 12493.6462, Recon Loss: 24987.2888, KL Loss: 0.0000, Percep Loss: 0.0018\n",
      "Epoch 192/1000, Beta: 5.00, Total Loss: 12493.5600, Recon Loss: 24987.1167, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 193/1000, Beta: 5.00, Total Loss: 12493.3991, Recon Loss: 24986.7943, KL Loss: 0.0000, Percep Loss: 0.0019\n",
      "Epoch 194/1000, Beta: 5.00, Total Loss: 12493.4559, Recon Loss: 24986.9084, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 195/1000, Beta: 5.00, Total Loss: 12493.4669, Recon Loss: 24986.9303, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 196/1000, Beta: 5.00, Total Loss: 12493.4765, Recon Loss: 24986.9492, KL Loss: 0.0000, Percep Loss: 0.0019\n",
      "Epoch 197/1000, Beta: 5.00, Total Loss: 12493.3680, Recon Loss: 24986.7324, KL Loss: 0.0000, Percep Loss: 0.0018\n",
      "Epoch 198/1000, Beta: 5.00, Total Loss: 12493.4411, Recon Loss: 24986.8788, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 199/1000, Beta: 5.00, Total Loss: 12493.4013, Recon Loss: 24986.7992, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 200/1000, Beta: 5.00, Total Loss: 12493.4057, Recon Loss: 24986.8077, KL Loss: 0.0000, Percep Loss: 0.0018\n",
      "Epoch 201/1000, Beta: 5.00, Total Loss: 12492.8509, Recon Loss: 24985.6993, KL Loss: 0.0000, Percep Loss: 0.0013\n",
      "Epoch 202/1000, Beta: 5.00, Total Loss: 12492.7728, Recon Loss: 24985.5432, KL Loss: 0.0000, Percep Loss: 0.0013\n",
      "Epoch 203/1000, Beta: 5.00, Total Loss: 12492.7470, Recon Loss: 24985.4916, KL Loss: 0.0000, Percep Loss: 0.0013\n",
      "Epoch 204/1000, Beta: 5.00, Total Loss: 12492.7323, Recon Loss: 24985.4621, KL Loss: 0.0000, Percep Loss: 0.0013\n",
      "Epoch 205/1000, Beta: 5.00, Total Loss: 12492.7135, Recon Loss: 24985.4246, KL Loss: 0.0000, Percep Loss: 0.0013\n",
      "Epoch 206/1000, Beta: 5.00, Total Loss: 12492.6813, Recon Loss: 24985.3604, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 207/1000, Beta: 5.00, Total Loss: 12492.6659, Recon Loss: 24985.3295, KL Loss: 0.0000, Percep Loss: 0.0013\n",
      "Epoch 208/1000, Beta: 5.00, Total Loss: 12492.6596, Recon Loss: 24985.3170, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 209/1000, Beta: 5.00, Total Loss: 12492.6932, Recon Loss: 24985.3841, KL Loss: 0.0000, Percep Loss: 0.0013\n",
      "Epoch 210/1000, Beta: 5.00, Total Loss: 12492.6803, Recon Loss: 24985.3582, KL Loss: 0.0000, Percep Loss: 0.0013\n",
      "Epoch 211/1000, Beta: 5.00, Total Loss: 12492.6232, Recon Loss: 24985.2441, KL Loss: 0.0000, Percep Loss: 0.0013\n",
      "Epoch 212/1000, Beta: 5.00, Total Loss: 12492.6258, Recon Loss: 24985.2493, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 213/1000, Beta: 5.00, Total Loss: 12492.5826, Recon Loss: 24985.1629, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 214/1000, Beta: 5.00, Total Loss: 12492.5924, Recon Loss: 24985.1827, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 215/1000, Beta: 5.00, Total Loss: 12492.5697, Recon Loss: 24985.1372, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 216/1000, Beta: 5.00, Total Loss: 12492.5617, Recon Loss: 24985.1213, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 217/1000, Beta: 5.00, Total Loss: 12492.5450, Recon Loss: 24985.0877, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 218/1000, Beta: 5.00, Total Loss: 12492.5593, Recon Loss: 24985.1164, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 219/1000, Beta: 5.00, Total Loss: 12492.5440, Recon Loss: 24985.0858, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 220/1000, Beta: 5.00, Total Loss: 12492.5528, Recon Loss: 24985.1033, KL Loss: 0.0000, Percep Loss: 0.0013\n",
      "Epoch 221/1000, Beta: 5.00, Total Loss: 12492.4902, Recon Loss: 24984.9783, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 222/1000, Beta: 5.00, Total Loss: 12492.5383, Recon Loss: 24985.0744, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 223/1000, Beta: 5.00, Total Loss: 12492.5122, Recon Loss: 24985.0221, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 224/1000, Beta: 5.00, Total Loss: 12492.4939, Recon Loss: 24984.9856, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 225/1000, Beta: 5.00, Total Loss: 12492.4700, Recon Loss: 24984.9379, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 226/1000, Beta: 5.00, Total Loss: 12492.4726, Recon Loss: 24984.9431, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 227/1000, Beta: 5.00, Total Loss: 12492.4507, Recon Loss: 24984.8991, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 228/1000, Beta: 5.00, Total Loss: 12492.4434, Recon Loss: 24984.8846, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 229/1000, Beta: 5.00, Total Loss: 12492.4373, Recon Loss: 24984.8724, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 230/1000, Beta: 5.00, Total Loss: 12492.4551, Recon Loss: 24984.9079, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 231/1000, Beta: 5.00, Total Loss: 12492.4278, Recon Loss: 24984.8536, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 232/1000, Beta: 5.00, Total Loss: 12492.4039, Recon Loss: 24984.8057, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 233/1000, Beta: 5.00, Total Loss: 12492.4075, Recon Loss: 24984.8128, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 234/1000, Beta: 5.00, Total Loss: 12492.3865, Recon Loss: 24984.7709, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 235/1000, Beta: 5.00, Total Loss: 12492.3974, Recon Loss: 24984.7925, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 236/1000, Beta: 5.00, Total Loss: 12492.3954, Recon Loss: 24984.7888, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 237/1000, Beta: 5.00, Total Loss: 12492.3712, Recon Loss: 24984.7403, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 238/1000, Beta: 5.00, Total Loss: 12492.3628, Recon Loss: 24984.7235, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 239/1000, Beta: 5.00, Total Loss: 12492.3706, Recon Loss: 24984.7391, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 240/1000, Beta: 5.00, Total Loss: 12492.3599, Recon Loss: 24984.7175, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 241/1000, Beta: 5.00, Total Loss: 12492.3653, Recon Loss: 24984.7285, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 242/1000, Beta: 5.00, Total Loss: 12492.3314, Recon Loss: 24984.6607, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 243/1000, Beta: 5.00, Total Loss: 12492.3473, Recon Loss: 24984.6924, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 244/1000, Beta: 5.00, Total Loss: 12492.3203, Recon Loss: 24984.6386, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 245/1000, Beta: 5.00, Total Loss: 12492.3202, Recon Loss: 24984.6384, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 246/1000, Beta: 5.00, Total Loss: 12492.2968, Recon Loss: 24984.5915, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 247/1000, Beta: 5.00, Total Loss: 12492.3117, Recon Loss: 24984.6214, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 248/1000, Beta: 5.00, Total Loss: 12492.2798, Recon Loss: 24984.5575, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 249/1000, Beta: 5.00, Total Loss: 12492.2684, Recon Loss: 24984.5348, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 250/1000, Beta: 5.00, Total Loss: 12492.2764, Recon Loss: 24984.5507, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 251/1000, Beta: 5.00, Total Loss: 12492.2911, Recon Loss: 24984.5802, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 252/1000, Beta: 5.00, Total Loss: 12492.2729, Recon Loss: 24984.5437, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 253/1000, Beta: 5.00, Total Loss: 12492.2809, Recon Loss: 24984.5596, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 254/1000, Beta: 5.00, Total Loss: 12492.2559, Recon Loss: 24984.5097, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 255/1000, Beta: 5.00, Total Loss: 12492.2657, Recon Loss: 24984.5294, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 256/1000, Beta: 5.00, Total Loss: 12492.2502, Recon Loss: 24984.4982, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 257/1000, Beta: 5.00, Total Loss: 12492.2209, Recon Loss: 24984.4398, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 258/1000, Beta: 5.00, Total Loss: 12492.2237, Recon Loss: 24984.4453, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 259/1000, Beta: 5.00, Total Loss: 12492.2490, Recon Loss: 24984.4958, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 260/1000, Beta: 5.00, Total Loss: 12492.2043, Recon Loss: 24984.4065, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 261/1000, Beta: 5.00, Total Loss: 12492.2408, Recon Loss: 24984.4794, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 262/1000, Beta: 5.00, Total Loss: 12492.1988, Recon Loss: 24984.3955, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 263/1000, Beta: 5.00, Total Loss: 12492.1953, Recon Loss: 24984.3886, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 264/1000, Beta: 5.00, Total Loss: 12492.1980, Recon Loss: 24984.3938, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 265/1000, Beta: 5.00, Total Loss: 12492.2029, Recon Loss: 24984.4037, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 266/1000, Beta: 5.00, Total Loss: 12492.1795, Recon Loss: 24984.3569, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 267/1000, Beta: 5.00, Total Loss: 12492.1799, Recon Loss: 24984.3577, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 268/1000, Beta: 5.00, Total Loss: 12492.1763, Recon Loss: 24984.3505, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 269/1000, Beta: 5.00, Total Loss: 12492.1725, Recon Loss: 24984.3428, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 270/1000, Beta: 5.00, Total Loss: 12492.1782, Recon Loss: 24984.3543, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 271/1000, Beta: 5.00, Total Loss: 12492.1711, Recon Loss: 24984.3401, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 272/1000, Beta: 5.00, Total Loss: 12492.1575, Recon Loss: 24984.3129, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 273/1000, Beta: 5.00, Total Loss: 12492.1354, Recon Loss: 24984.2687, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 274/1000, Beta: 5.00, Total Loss: 12492.1496, Recon Loss: 24984.2972, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 275/1000, Beta: 5.00, Total Loss: 12492.1357, Recon Loss: 24984.2694, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 276/1000, Beta: 5.00, Total Loss: 12492.1305, Recon Loss: 24984.2589, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 277/1000, Beta: 5.00, Total Loss: 12492.1193, Recon Loss: 24984.2366, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 278/1000, Beta: 5.00, Total Loss: 12492.1146, Recon Loss: 24984.2271, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 279/1000, Beta: 5.00, Total Loss: 12492.1287, Recon Loss: 24984.2554, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 280/1000, Beta: 5.00, Total Loss: 12492.0847, Recon Loss: 24984.1672, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 281/1000, Beta: 5.00, Total Loss: 12492.1146, Recon Loss: 24984.2271, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 282/1000, Beta: 5.00, Total Loss: 12492.1248, Recon Loss: 24984.2474, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 283/1000, Beta: 5.00, Total Loss: 12492.1126, Recon Loss: 24984.2231, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 284/1000, Beta: 5.00, Total Loss: 12492.0793, Recon Loss: 24984.1566, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 285/1000, Beta: 5.00, Total Loss: 12492.1268, Recon Loss: 24984.2514, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 286/1000, Beta: 5.00, Total Loss: 12492.1116, Recon Loss: 24984.2210, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 287/1000, Beta: 5.00, Total Loss: 12492.0754, Recon Loss: 24984.1487, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 288/1000, Beta: 5.00, Total Loss: 12492.0737, Recon Loss: 24984.1453, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 289/1000, Beta: 5.00, Total Loss: 12492.0736, Recon Loss: 24984.1452, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 290/1000, Beta: 5.00, Total Loss: 12492.0733, Recon Loss: 24984.1444, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 291/1000, Beta: 5.00, Total Loss: 12492.0733, Recon Loss: 24984.1444, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 292/1000, Beta: 5.00, Total Loss: 12492.0687, Recon Loss: 24984.1352, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 293/1000, Beta: 5.00, Total Loss: 12492.0291, Recon Loss: 24984.0560, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 294/1000, Beta: 5.00, Total Loss: 12492.0872, Recon Loss: 24984.1723, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 295/1000, Beta: 5.00, Total Loss: 12492.0424, Recon Loss: 24984.0828, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 296/1000, Beta: 5.00, Total Loss: 12492.0695, Recon Loss: 24984.1369, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 297/1000, Beta: 5.00, Total Loss: 12492.0465, Recon Loss: 24984.0910, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 298/1000, Beta: 5.00, Total Loss: 12492.0266, Recon Loss: 24984.0512, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 299/1000, Beta: 5.00, Total Loss: 12492.0352, Recon Loss: 24984.0683, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 300/1000, Beta: 5.00, Total Loss: 12492.0519, Recon Loss: 24984.1017, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 301/1000, Beta: 5.00, Total Loss: 12492.0293, Recon Loss: 24984.0566, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 302/1000, Beta: 5.00, Total Loss: 12491.9938, Recon Loss: 24983.9855, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 303/1000, Beta: 5.00, Total Loss: 12492.0145, Recon Loss: 24984.0269, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 304/1000, Beta: 5.00, Total Loss: 12492.0085, Recon Loss: 24984.0150, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 305/1000, Beta: 5.00, Total Loss: 12492.0074, Recon Loss: 24984.0127, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 306/1000, Beta: 5.00, Total Loss: 12491.9727, Recon Loss: 24983.9433, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 307/1000, Beta: 5.00, Total Loss: 12492.0077, Recon Loss: 24984.0133, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 308/1000, Beta: 5.00, Total Loss: 12491.9811, Recon Loss: 24983.9602, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 309/1000, Beta: 5.00, Total Loss: 12491.9738, Recon Loss: 24983.9455, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 310/1000, Beta: 5.00, Total Loss: 12491.9990, Recon Loss: 24983.9960, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 311/1000, Beta: 5.00, Total Loss: 12491.9612, Recon Loss: 24983.9205, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 312/1000, Beta: 5.00, Total Loss: 12491.9287, Recon Loss: 24983.8555, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 313/1000, Beta: 5.00, Total Loss: 12491.9968, Recon Loss: 24983.9915, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 314/1000, Beta: 5.00, Total Loss: 12491.9510, Recon Loss: 24983.9000, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 315/1000, Beta: 5.00, Total Loss: 12492.0028, Recon Loss: 24984.0034, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 316/1000, Beta: 5.00, Total Loss: 12491.9266, Recon Loss: 24983.8511, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 317/1000, Beta: 5.00, Total Loss: 12491.9567, Recon Loss: 24983.9113, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 318/1000, Beta: 5.00, Total Loss: 12491.9499, Recon Loss: 24983.8978, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 319/1000, Beta: 5.00, Total Loss: 12491.9366, Recon Loss: 24983.8711, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 320/1000, Beta: 5.00, Total Loss: 12491.9366, Recon Loss: 24983.8711, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 321/1000, Beta: 5.00, Total Loss: 12491.9365, Recon Loss: 24983.8709, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 322/1000, Beta: 5.00, Total Loss: 12491.9186, Recon Loss: 24983.8352, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 323/1000, Beta: 5.00, Total Loss: 12491.9176, Recon Loss: 24983.8332, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 324/1000, Beta: 5.00, Total Loss: 12491.8939, Recon Loss: 24983.7858, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 325/1000, Beta: 5.00, Total Loss: 12491.9196, Recon Loss: 24983.8372, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 326/1000, Beta: 5.00, Total Loss: 12491.9090, Recon Loss: 24983.8159, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 327/1000, Beta: 5.00, Total Loss: 12491.8884, Recon Loss: 24983.7747, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 328/1000, Beta: 5.00, Total Loss: 12491.9081, Recon Loss: 24983.8143, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 329/1000, Beta: 5.00, Total Loss: 12491.8965, Recon Loss: 24983.7909, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 330/1000, Beta: 5.00, Total Loss: 12491.8998, Recon Loss: 24983.7977, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 331/1000, Beta: 5.00, Total Loss: 12491.9024, Recon Loss: 24983.8027, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 332/1000, Beta: 5.00, Total Loss: 12491.8835, Recon Loss: 24983.7649, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 333/1000, Beta: 5.00, Total Loss: 12491.8667, Recon Loss: 24983.7313, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 334/1000, Beta: 5.00, Total Loss: 12491.8655, Recon Loss: 24983.7290, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 335/1000, Beta: 5.00, Total Loss: 12491.8507, Recon Loss: 24983.6993, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 336/1000, Beta: 5.00, Total Loss: 12491.8599, Recon Loss: 24983.7178, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 337/1000, Beta: 5.00, Total Loss: 12491.8809, Recon Loss: 24983.7599, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 338/1000, Beta: 5.00, Total Loss: 12491.8445, Recon Loss: 24983.6870, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 339/1000, Beta: 5.00, Total Loss: 12491.8579, Recon Loss: 24983.7137, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 340/1000, Beta: 5.00, Total Loss: 12491.8507, Recon Loss: 24983.6993, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 341/1000, Beta: 5.00, Total Loss: 12491.8264, Recon Loss: 24983.6509, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 342/1000, Beta: 5.00, Total Loss: 12491.8534, Recon Loss: 24983.7047, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 343/1000, Beta: 5.00, Total Loss: 12491.8287, Recon Loss: 24983.6554, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 344/1000, Beta: 5.00, Total Loss: 12491.7996, Recon Loss: 24983.5971, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 345/1000, Beta: 5.00, Total Loss: 12491.8491, Recon Loss: 24983.6962, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 346/1000, Beta: 5.00, Total Loss: 12491.8316, Recon Loss: 24983.6611, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 347/1000, Beta: 5.00, Total Loss: 12491.8582, Recon Loss: 24983.7144, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 348/1000, Beta: 5.00, Total Loss: 12491.8486, Recon Loss: 24983.6951, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 349/1000, Beta: 5.00, Total Loss: 12491.8023, Recon Loss: 24983.6026, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 350/1000, Beta: 5.00, Total Loss: 12491.8254, Recon Loss: 24983.6487, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 351/1000, Beta: 5.00, Total Loss: 12491.8067, Recon Loss: 24983.6114, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 352/1000, Beta: 5.00, Total Loss: 12491.8080, Recon Loss: 24983.6141, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 353/1000, Beta: 5.00, Total Loss: 12491.8059, Recon Loss: 24983.6098, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 354/1000, Beta: 5.00, Total Loss: 12491.7947, Recon Loss: 24983.5875, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 355/1000, Beta: 5.00, Total Loss: 12491.8086, Recon Loss: 24983.6151, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 356/1000, Beta: 5.00, Total Loss: 12491.7713, Recon Loss: 24983.5407, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 357/1000, Beta: 5.00, Total Loss: 12491.7770, Recon Loss: 24983.5519, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 358/1000, Beta: 5.00, Total Loss: 12491.7967, Recon Loss: 24983.5913, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 359/1000, Beta: 5.00, Total Loss: 12491.7784, Recon Loss: 24983.5547, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 360/1000, Beta: 5.00, Total Loss: 12491.7692, Recon Loss: 24983.5364, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 361/1000, Beta: 5.00, Total Loss: 12491.7590, Recon Loss: 24983.5159, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 362/1000, Beta: 5.00, Total Loss: 12491.7660, Recon Loss: 24983.5299, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 363/1000, Beta: 5.00, Total Loss: 12491.7725, Recon Loss: 24983.5430, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 364/1000, Beta: 5.00, Total Loss: 12491.7592, Recon Loss: 24983.5164, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 365/1000, Beta: 5.00, Total Loss: 12491.7447, Recon Loss: 24983.4874, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 366/1000, Beta: 5.00, Total Loss: 12491.7729, Recon Loss: 24983.5438, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 367/1000, Beta: 5.00, Total Loss: 12491.7464, Recon Loss: 24983.4908, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 368/1000, Beta: 5.00, Total Loss: 12491.7608, Recon Loss: 24983.5195, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 369/1000, Beta: 5.00, Total Loss: 12491.7587, Recon Loss: 24983.5154, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 370/1000, Beta: 5.00, Total Loss: 12491.7557, Recon Loss: 24983.5094, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 371/1000, Beta: 5.00, Total Loss: 12491.7351, Recon Loss: 24983.4682, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 372/1000, Beta: 5.00, Total Loss: 12491.7139, Recon Loss: 24983.4258, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 373/1000, Beta: 5.00, Total Loss: 12491.7472, Recon Loss: 24983.4924, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 374/1000, Beta: 5.00, Total Loss: 12491.7164, Recon Loss: 24983.4309, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 375/1000, Beta: 5.00, Total Loss: 12491.7477, Recon Loss: 24983.4934, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 376/1000, Beta: 5.00, Total Loss: 12491.7256, Recon Loss: 24983.4492, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 377/1000, Beta: 5.00, Total Loss: 12491.7641, Recon Loss: 24983.5260, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 378/1000, Beta: 5.00, Total Loss: 12491.7101, Recon Loss: 24983.4182, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 379/1000, Beta: 5.00, Total Loss: 12491.7448, Recon Loss: 24983.4875, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 380/1000, Beta: 5.00, Total Loss: 12491.7288, Recon Loss: 24983.4557, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 381/1000, Beta: 5.00, Total Loss: 12491.6947, Recon Loss: 24983.3873, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 382/1000, Beta: 5.00, Total Loss: 12491.6869, Recon Loss: 24983.3718, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 383/1000, Beta: 5.00, Total Loss: 12491.6796, Recon Loss: 24983.3571, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 384/1000, Beta: 5.00, Total Loss: 12491.7211, Recon Loss: 24983.4402, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 385/1000, Beta: 5.00, Total Loss: 12491.7163, Recon Loss: 24983.4306, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 386/1000, Beta: 5.00, Total Loss: 12491.6856, Recon Loss: 24983.3692, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 387/1000, Beta: 5.00, Total Loss: 12491.7157, Recon Loss: 24983.4293, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 388/1000, Beta: 5.00, Total Loss: 12491.6882, Recon Loss: 24983.3743, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 389/1000, Beta: 5.00, Total Loss: 12491.6736, Recon Loss: 24983.3451, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 390/1000, Beta: 5.00, Total Loss: 12491.7006, Recon Loss: 24983.3992, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 391/1000, Beta: 5.00, Total Loss: 12491.7002, Recon Loss: 24983.3983, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 392/1000, Beta: 5.00, Total Loss: 12491.6766, Recon Loss: 24983.3512, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 393/1000, Beta: 5.00, Total Loss: 12491.6823, Recon Loss: 24983.3627, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 394/1000, Beta: 5.00, Total Loss: 12491.6667, Recon Loss: 24983.3314, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 395/1000, Beta: 5.00, Total Loss: 12491.6586, Recon Loss: 24983.3152, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 396/1000, Beta: 5.00, Total Loss: 12491.6551, Recon Loss: 24983.3083, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 397/1000, Beta: 5.00, Total Loss: 12491.6614, Recon Loss: 24983.3208, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 398/1000, Beta: 5.00, Total Loss: 12491.6572, Recon Loss: 24983.3124, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 399/1000, Beta: 5.00, Total Loss: 12491.6532, Recon Loss: 24983.3043, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 400/1000, Beta: 5.00, Total Loss: 12491.6525, Recon Loss: 24983.3029, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 401/1000, Beta: 5.00, Total Loss: 12491.4631, Recon Loss: 24982.9242, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 402/1000, Beta: 5.00, Total Loss: 12491.4472, Recon Loss: 24982.8924, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 403/1000, Beta: 5.00, Total Loss: 12491.4417, Recon Loss: 24982.8814, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 404/1000, Beta: 5.00, Total Loss: 12491.4354, Recon Loss: 24982.8687, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 405/1000, Beta: 5.00, Total Loss: 12491.4281, Recon Loss: 24982.8542, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 406/1000, Beta: 5.00, Total Loss: 12491.4315, Recon Loss: 24982.8611, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 407/1000, Beta: 5.00, Total Loss: 12491.4236, Recon Loss: 24982.8452, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 408/1000, Beta: 5.00, Total Loss: 12491.4218, Recon Loss: 24982.8416, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 409/1000, Beta: 5.00, Total Loss: 12491.4168, Recon Loss: 24982.8316, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 410/1000, Beta: 5.00, Total Loss: 12491.4108, Recon Loss: 24982.8196, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 411/1000, Beta: 5.00, Total Loss: 12491.4055, Recon Loss: 24982.8091, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 412/1000, Beta: 5.00, Total Loss: 12491.4173, Recon Loss: 24982.8327, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 413/1000, Beta: 5.00, Total Loss: 12491.3939, Recon Loss: 24982.7858, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 414/1000, Beta: 5.00, Total Loss: 12491.3890, Recon Loss: 24982.7759, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 415/1000, Beta: 5.00, Total Loss: 12491.3923, Recon Loss: 24982.7826, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 416/1000, Beta: 5.00, Total Loss: 12491.3960, Recon Loss: 24982.7900, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 417/1000, Beta: 5.00, Total Loss: 12491.3955, Recon Loss: 24982.7891, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 418/1000, Beta: 5.00, Total Loss: 12491.3953, Recon Loss: 24982.7886, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 419/1000, Beta: 5.00, Total Loss: 12491.3901, Recon Loss: 24982.7782, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 420/1000, Beta: 5.00, Total Loss: 12491.3813, Recon Loss: 24982.7607, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 421/1000, Beta: 5.00, Total Loss: 12491.3805, Recon Loss: 24982.7590, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 422/1000, Beta: 5.00, Total Loss: 12491.3803, Recon Loss: 24982.7587, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 423/1000, Beta: 5.00, Total Loss: 12491.3750, Recon Loss: 24982.7481, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 424/1000, Beta: 5.00, Total Loss: 12491.3849, Recon Loss: 24982.7679, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 425/1000, Beta: 5.00, Total Loss: 12491.3737, Recon Loss: 24982.7454, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 426/1000, Beta: 5.00, Total Loss: 12491.3789, Recon Loss: 24982.7558, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 427/1000, Beta: 5.00, Total Loss: 12491.3632, Recon Loss: 24982.7245, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 428/1000, Beta: 5.00, Total Loss: 12491.3636, Recon Loss: 24982.7253, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 429/1000, Beta: 5.00, Total Loss: 12491.3694, Recon Loss: 24982.7369, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 430/1000, Beta: 5.00, Total Loss: 12491.3653, Recon Loss: 24982.7286, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 431/1000, Beta: 5.00, Total Loss: 12491.3602, Recon Loss: 24982.7185, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 432/1000, Beta: 5.00, Total Loss: 12491.3528, Recon Loss: 24982.7036, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 433/1000, Beta: 5.00, Total Loss: 12491.3600, Recon Loss: 24982.7181, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 434/1000, Beta: 5.00, Total Loss: 12491.3483, Recon Loss: 24982.6947, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 435/1000, Beta: 5.00, Total Loss: 12491.3487, Recon Loss: 24982.6955, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 436/1000, Beta: 5.00, Total Loss: 12491.3500, Recon Loss: 24982.6980, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 437/1000, Beta: 5.00, Total Loss: 12491.3409, Recon Loss: 24982.6799, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 438/1000, Beta: 5.00, Total Loss: 12491.3394, Recon Loss: 24982.6768, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 439/1000, Beta: 5.00, Total Loss: 12491.3474, Recon Loss: 24982.6928, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 440/1000, Beta: 5.00, Total Loss: 12491.3347, Recon Loss: 24982.6674, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 441/1000, Beta: 5.00, Total Loss: 12491.3362, Recon Loss: 24982.6705, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 442/1000, Beta: 5.00, Total Loss: 12491.3323, Recon Loss: 24982.6627, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 443/1000, Beta: 5.00, Total Loss: 12491.3238, Recon Loss: 24982.6458, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 444/1000, Beta: 5.00, Total Loss: 12491.3235, Recon Loss: 24982.6451, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 445/1000, Beta: 5.00, Total Loss: 12491.3239, Recon Loss: 24982.6458, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 446/1000, Beta: 5.00, Total Loss: 12491.3212, Recon Loss: 24982.6406, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 447/1000, Beta: 5.00, Total Loss: 12491.3179, Recon Loss: 24982.6339, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 448/1000, Beta: 5.00, Total Loss: 12491.3242, Recon Loss: 24982.6464, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 449/1000, Beta: 5.00, Total Loss: 12491.3250, Recon Loss: 24982.6481, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 450/1000, Beta: 5.00, Total Loss: 12491.3132, Recon Loss: 24982.6245, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 451/1000, Beta: 5.00, Total Loss: 12491.3130, Recon Loss: 24982.6241, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 452/1000, Beta: 5.00, Total Loss: 12491.3154, Recon Loss: 24982.6288, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 453/1000, Beta: 5.00, Total Loss: 12491.3090, Recon Loss: 24982.6160, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 454/1000, Beta: 5.00, Total Loss: 12491.3129, Recon Loss: 24982.6239, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 455/1000, Beta: 5.00, Total Loss: 12491.3121, Recon Loss: 24982.6222, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 456/1000, Beta: 5.00, Total Loss: 12491.2989, Recon Loss: 24982.5958, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 457/1000, Beta: 5.00, Total Loss: 12491.2986, Recon Loss: 24982.5953, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 458/1000, Beta: 5.00, Total Loss: 12491.3026, Recon Loss: 24982.6033, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 459/1000, Beta: 5.00, Total Loss: 12491.2963, Recon Loss: 24982.5906, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 460/1000, Beta: 5.00, Total Loss: 12491.3058, Recon Loss: 24982.6096, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 461/1000, Beta: 5.00, Total Loss: 12491.2964, Recon Loss: 24982.5908, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 462/1000, Beta: 5.00, Total Loss: 12491.2912, Recon Loss: 24982.5806, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 463/1000, Beta: 5.00, Total Loss: 12491.2939, Recon Loss: 24982.5860, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 464/1000, Beta: 5.00, Total Loss: 12491.2893, Recon Loss: 24982.5768, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 465/1000, Beta: 5.00, Total Loss: 12491.2823, Recon Loss: 24982.5627, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 466/1000, Beta: 5.00, Total Loss: 12491.2963, Recon Loss: 24982.5908, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 467/1000, Beta: 5.00, Total Loss: 12491.2886, Recon Loss: 24982.5753, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 468/1000, Beta: 5.00, Total Loss: 12491.2945, Recon Loss: 24982.5871, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 469/1000, Beta: 5.00, Total Loss: 12491.2784, Recon Loss: 24982.5549, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 470/1000, Beta: 5.00, Total Loss: 12491.2867, Recon Loss: 24982.5714, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 471/1000, Beta: 5.00, Total Loss: 12491.2761, Recon Loss: 24982.5503, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 472/1000, Beta: 5.00, Total Loss: 12491.2774, Recon Loss: 24982.5529, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 473/1000, Beta: 5.00, Total Loss: 12491.2777, Recon Loss: 24982.5534, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 474/1000, Beta: 5.00, Total Loss: 12491.2770, Recon Loss: 24982.5521, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 475/1000, Beta: 5.00, Total Loss: 12491.2699, Recon Loss: 24982.5379, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 476/1000, Beta: 5.00, Total Loss: 12491.2641, Recon Loss: 24982.5263, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 477/1000, Beta: 5.00, Total Loss: 12491.2659, Recon Loss: 24982.5299, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 478/1000, Beta: 5.00, Total Loss: 12491.2698, Recon Loss: 24982.5377, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 479/1000, Beta: 5.00, Total Loss: 12491.2656, Recon Loss: 24982.5292, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 480/1000, Beta: 5.00, Total Loss: 12491.2617, Recon Loss: 24982.5216, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 481/1000, Beta: 5.00, Total Loss: 12491.2697, Recon Loss: 24982.5375, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 482/1000, Beta: 5.00, Total Loss: 12491.2599, Recon Loss: 24982.5179, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 483/1000, Beta: 5.00, Total Loss: 12491.2549, Recon Loss: 24982.5079, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 484/1000, Beta: 5.00, Total Loss: 12491.2516, Recon Loss: 24982.5013, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 485/1000, Beta: 5.00, Total Loss: 12491.2547, Recon Loss: 24982.5076, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 486/1000, Beta: 5.00, Total Loss: 12491.2611, Recon Loss: 24982.5203, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 487/1000, Beta: 5.00, Total Loss: 12491.2609, Recon Loss: 24982.5199, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 488/1000, Beta: 5.00, Total Loss: 12491.2558, Recon Loss: 24982.5096, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 489/1000, Beta: 5.00, Total Loss: 12491.2433, Recon Loss: 24982.4846, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 490/1000, Beta: 5.00, Total Loss: 12491.2513, Recon Loss: 24982.5007, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 491/1000, Beta: 5.00, Total Loss: 12491.2437, Recon Loss: 24982.4856, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 492/1000, Beta: 5.00, Total Loss: 12491.2353, Recon Loss: 24982.4687, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 493/1000, Beta: 5.00, Total Loss: 12491.2391, Recon Loss: 24982.4763, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 494/1000, Beta: 5.00, Total Loss: 12491.2378, Recon Loss: 24982.4737, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 495/1000, Beta: 5.00, Total Loss: 12491.2381, Recon Loss: 24982.4743, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 496/1000, Beta: 5.00, Total Loss: 12491.2441, Recon Loss: 24982.4862, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 497/1000, Beta: 5.00, Total Loss: 12491.2350, Recon Loss: 24982.4681, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 498/1000, Beta: 5.00, Total Loss: 12491.2508, Recon Loss: 24982.4997, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 499/1000, Beta: 5.00, Total Loss: 12491.2403, Recon Loss: 24982.4786, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 500/1000, Beta: 5.00, Total Loss: 12491.2357, Recon Loss: 24982.4695, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 501/1000, Beta: 5.00, Total Loss: 12491.2264, Recon Loss: 24982.4510, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 502/1000, Beta: 5.00, Total Loss: 12491.2332, Recon Loss: 24982.4644, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 503/1000, Beta: 5.00, Total Loss: 12491.2191, Recon Loss: 24982.4362, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 504/1000, Beta: 5.00, Total Loss: 12491.2324, Recon Loss: 24982.4629, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 505/1000, Beta: 5.00, Total Loss: 12491.2243, Recon Loss: 24982.4467, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 506/1000, Beta: 5.00, Total Loss: 12491.2314, Recon Loss: 24982.4609, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 507/1000, Beta: 5.00, Total Loss: 12491.2164, Recon Loss: 24982.4308, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 508/1000, Beta: 5.00, Total Loss: 12491.2215, Recon Loss: 24982.4411, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 509/1000, Beta: 5.00, Total Loss: 12491.2160, Recon Loss: 24982.4302, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 510/1000, Beta: 5.00, Total Loss: 12491.2121, Recon Loss: 24982.4223, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 511/1000, Beta: 5.00, Total Loss: 12491.2170, Recon Loss: 24982.4321, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 512/1000, Beta: 5.00, Total Loss: 12491.2108, Recon Loss: 24982.4197, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 513/1000, Beta: 5.00, Total Loss: 12491.2158, Recon Loss: 24982.4297, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 514/1000, Beta: 5.00, Total Loss: 12491.2101, Recon Loss: 24982.4184, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 515/1000, Beta: 5.00, Total Loss: 12491.2120, Recon Loss: 24982.4221, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 516/1000, Beta: 5.00, Total Loss: 12491.2131, Recon Loss: 24982.4243, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 517/1000, Beta: 5.00, Total Loss: 12491.2166, Recon Loss: 24982.4313, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 518/1000, Beta: 5.00, Total Loss: 12491.2091, Recon Loss: 24982.4162, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 519/1000, Beta: 5.00, Total Loss: 12491.2073, Recon Loss: 24982.4126, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 520/1000, Beta: 5.00, Total Loss: 12491.2005, Recon Loss: 24982.3992, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 521/1000, Beta: 5.00, Total Loss: 12491.2052, Recon Loss: 24982.4085, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 522/1000, Beta: 5.00, Total Loss: 12491.2023, Recon Loss: 24982.4027, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 523/1000, Beta: 5.00, Total Loss: 12491.1913, Recon Loss: 24982.3808, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 524/1000, Beta: 5.00, Total Loss: 12491.1954, Recon Loss: 24982.3889, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 525/1000, Beta: 5.00, Total Loss: 12491.1937, Recon Loss: 24982.3855, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 526/1000, Beta: 5.00, Total Loss: 12491.1899, Recon Loss: 24982.3779, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 527/1000, Beta: 5.00, Total Loss: 12491.1915, Recon Loss: 24982.3811, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 528/1000, Beta: 5.00, Total Loss: 12491.1957, Recon Loss: 24982.3896, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 529/1000, Beta: 5.00, Total Loss: 12491.2035, Recon Loss: 24982.4051, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 530/1000, Beta: 5.00, Total Loss: 12491.1810, Recon Loss: 24982.3602, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 531/1000, Beta: 5.00, Total Loss: 12491.1939, Recon Loss: 24982.3859, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 532/1000, Beta: 5.00, Total Loss: 12491.1935, Recon Loss: 24982.3851, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 533/1000, Beta: 5.00, Total Loss: 12491.1805, Recon Loss: 24982.3592, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 534/1000, Beta: 5.00, Total Loss: 12491.1850, Recon Loss: 24982.3680, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 535/1000, Beta: 5.00, Total Loss: 12491.1924, Recon Loss: 24982.3829, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 536/1000, Beta: 5.00, Total Loss: 12491.1879, Recon Loss: 24982.3740, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 537/1000, Beta: 5.00, Total Loss: 12491.1986, Recon Loss: 24982.3952, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 538/1000, Beta: 5.00, Total Loss: 12491.1721, Recon Loss: 24982.3423, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 539/1000, Beta: 5.00, Total Loss: 12491.1810, Recon Loss: 24982.3601, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 540/1000, Beta: 5.00, Total Loss: 12491.1793, Recon Loss: 24982.3567, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 541/1000, Beta: 5.00, Total Loss: 12491.1752, Recon Loss: 24982.3485, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 542/1000, Beta: 5.00, Total Loss: 12491.1776, Recon Loss: 24982.3533, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 543/1000, Beta: 5.00, Total Loss: 12491.1766, Recon Loss: 24982.3513, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 544/1000, Beta: 5.00, Total Loss: 12491.1734, Recon Loss: 24982.3450, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 545/1000, Beta: 5.00, Total Loss: 12491.1773, Recon Loss: 24982.3528, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 546/1000, Beta: 5.00, Total Loss: 12491.1706, Recon Loss: 24982.3393, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 547/1000, Beta: 5.00, Total Loss: 12491.1745, Recon Loss: 24982.3472, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 548/1000, Beta: 5.00, Total Loss: 12491.1658, Recon Loss: 24982.3297, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 549/1000, Beta: 5.00, Total Loss: 12491.1553, Recon Loss: 24982.3088, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 550/1000, Beta: 5.00, Total Loss: 12491.1677, Recon Loss: 24982.3335, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 551/1000, Beta: 5.00, Total Loss: 12491.1688, Recon Loss: 24982.3359, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 552/1000, Beta: 5.00, Total Loss: 12491.1601, Recon Loss: 24982.3183, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 553/1000, Beta: 5.00, Total Loss: 12491.1562, Recon Loss: 24982.3105, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 554/1000, Beta: 5.00, Total Loss: 12491.1706, Recon Loss: 24982.3393, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 555/1000, Beta: 5.00, Total Loss: 12491.1739, Recon Loss: 24982.3459, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 556/1000, Beta: 5.00, Total Loss: 12491.1553, Recon Loss: 24982.3087, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 557/1000, Beta: 5.00, Total Loss: 12491.1611, Recon Loss: 24982.3203, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 558/1000, Beta: 5.00, Total Loss: 12491.1552, Recon Loss: 24982.3085, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 559/1000, Beta: 5.00, Total Loss: 12491.1580, Recon Loss: 24982.3141, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 560/1000, Beta: 5.00, Total Loss: 12491.1507, Recon Loss: 24982.2994, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 561/1000, Beta: 5.00, Total Loss: 12491.1567, Recon Loss: 24982.3116, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 562/1000, Beta: 5.00, Total Loss: 12491.1535, Recon Loss: 24982.3052, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 563/1000, Beta: 5.00, Total Loss: 12491.1473, Recon Loss: 24982.2927, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 564/1000, Beta: 5.00, Total Loss: 12491.1599, Recon Loss: 24982.3179, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 565/1000, Beta: 5.00, Total Loss: 12491.1478, Recon Loss: 24982.2937, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 566/1000, Beta: 5.00, Total Loss: 12491.1553, Recon Loss: 24982.3087, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 567/1000, Beta: 5.00, Total Loss: 12491.1381, Recon Loss: 24982.2743, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 568/1000, Beta: 5.00, Total Loss: 12491.1372, Recon Loss: 24982.2727, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 569/1000, Beta: 5.00, Total Loss: 12491.1522, Recon Loss: 24982.3025, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 570/1000, Beta: 5.00, Total Loss: 12491.1366, Recon Loss: 24982.2713, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 571/1000, Beta: 5.00, Total Loss: 12491.1371, Recon Loss: 24982.2724, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 572/1000, Beta: 5.00, Total Loss: 12491.1331, Recon Loss: 24982.2644, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 573/1000, Beta: 5.00, Total Loss: 12491.1409, Recon Loss: 24982.2800, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 574/1000, Beta: 5.00, Total Loss: 12491.1485, Recon Loss: 24982.2951, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 575/1000, Beta: 5.00, Total Loss: 12491.1394, Recon Loss: 24982.2769, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 576/1000, Beta: 5.00, Total Loss: 12491.1292, Recon Loss: 24982.2565, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 577/1000, Beta: 5.00, Total Loss: 12491.1362, Recon Loss: 24982.2705, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 578/1000, Beta: 5.00, Total Loss: 12491.1264, Recon Loss: 24982.2510, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 579/1000, Beta: 5.00, Total Loss: 12491.1396, Recon Loss: 24982.2774, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 580/1000, Beta: 5.00, Total Loss: 12491.1258, Recon Loss: 24982.2497, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 581/1000, Beta: 5.00, Total Loss: 12491.1239, Recon Loss: 24982.2460, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 582/1000, Beta: 5.00, Total Loss: 12491.1193, Recon Loss: 24982.2368, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 583/1000, Beta: 5.00, Total Loss: 12491.1196, Recon Loss: 24982.2374, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 584/1000, Beta: 5.00, Total Loss: 12491.1251, Recon Loss: 24982.2484, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 585/1000, Beta: 5.00, Total Loss: 12491.1327, Recon Loss: 24982.2635, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 586/1000, Beta: 5.00, Total Loss: 12491.1361, Recon Loss: 24982.2703, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 587/1000, Beta: 5.00, Total Loss: 12491.1253, Recon Loss: 24982.2487, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 588/1000, Beta: 5.00, Total Loss: 12491.1127, Recon Loss: 24982.2235, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 589/1000, Beta: 5.00, Total Loss: 12491.1199, Recon Loss: 24982.2379, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 590/1000, Beta: 5.00, Total Loss: 12491.1289, Recon Loss: 24982.2558, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 591/1000, Beta: 5.00, Total Loss: 12491.1188, Recon Loss: 24982.2357, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 592/1000, Beta: 5.00, Total Loss: 12491.1235, Recon Loss: 24982.2452, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 593/1000, Beta: 5.00, Total Loss: 12491.1165, Recon Loss: 24982.2311, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 594/1000, Beta: 5.00, Total Loss: 12491.1116, Recon Loss: 24982.2212, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 595/1000, Beta: 5.00, Total Loss: 12491.1125, Recon Loss: 24982.2232, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 596/1000, Beta: 5.00, Total Loss: 12491.1202, Recon Loss: 24982.2386, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 597/1000, Beta: 5.00, Total Loss: 12491.1077, Recon Loss: 24982.2135, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 598/1000, Beta: 5.00, Total Loss: 12491.1064, Recon Loss: 24982.2110, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 599/1000, Beta: 5.00, Total Loss: 12491.1088, Recon Loss: 24982.2157, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 600/1000, Beta: 5.00, Total Loss: 12491.1118, Recon Loss: 24982.2218, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 601/1000, Beta: 5.00, Total Loss: 12491.0449, Recon Loss: 24982.0881, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 602/1000, Beta: 5.00, Total Loss: 12491.0352, Recon Loss: 24982.0687, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 603/1000, Beta: 5.00, Total Loss: 12491.0320, Recon Loss: 24982.0624, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 604/1000, Beta: 5.00, Total Loss: 12491.0328, Recon Loss: 24982.0638, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 605/1000, Beta: 5.00, Total Loss: 12491.0296, Recon Loss: 24982.0574, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 606/1000, Beta: 5.00, Total Loss: 12491.0270, Recon Loss: 24982.0522, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 607/1000, Beta: 5.00, Total Loss: 12491.0240, Recon Loss: 24982.0464, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 608/1000, Beta: 5.00, Total Loss: 12491.0221, Recon Loss: 24982.0426, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 609/1000, Beta: 5.00, Total Loss: 12491.0256, Recon Loss: 24982.0495, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 610/1000, Beta: 5.00, Total Loss: 12491.0221, Recon Loss: 24982.0426, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 611/1000, Beta: 5.00, Total Loss: 12491.0198, Recon Loss: 24982.0379, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 612/1000, Beta: 5.00, Total Loss: 12491.0213, Recon Loss: 24982.0409, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 613/1000, Beta: 5.00, Total Loss: 12491.0194, Recon Loss: 24982.0370, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 614/1000, Beta: 5.00, Total Loss: 12491.0166, Recon Loss: 24982.0314, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 615/1000, Beta: 5.00, Total Loss: 12491.0172, Recon Loss: 24982.0328, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 616/1000, Beta: 5.00, Total Loss: 12491.0171, Recon Loss: 24982.0324, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 617/1000, Beta: 5.00, Total Loss: 12491.0153, Recon Loss: 24982.0290, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 618/1000, Beta: 5.00, Total Loss: 12491.0136, Recon Loss: 24982.0255, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 619/1000, Beta: 5.00, Total Loss: 12491.0141, Recon Loss: 24982.0265, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 620/1000, Beta: 5.00, Total Loss: 12491.0127, Recon Loss: 24982.0237, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 621/1000, Beta: 5.00, Total Loss: 12491.0088, Recon Loss: 24982.0160, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 622/1000, Beta: 5.00, Total Loss: 12491.0093, Recon Loss: 24982.0169, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 623/1000, Beta: 5.00, Total Loss: 12491.0089, Recon Loss: 24982.0162, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 624/1000, Beta: 5.00, Total Loss: 12491.0091, Recon Loss: 24982.0164, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 625/1000, Beta: 5.00, Total Loss: 12491.0056, Recon Loss: 24982.0095, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 626/1000, Beta: 5.00, Total Loss: 12491.0060, Recon Loss: 24982.0102, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 627/1000, Beta: 5.00, Total Loss: 12491.0075, Recon Loss: 24982.0133, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 628/1000, Beta: 5.00, Total Loss: 12491.0046, Recon Loss: 24982.0075, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 629/1000, Beta: 5.00, Total Loss: 12491.0036, Recon Loss: 24982.0055, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 630/1000, Beta: 5.00, Total Loss: 12491.0021, Recon Loss: 24982.0026, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 631/1000, Beta: 5.00, Total Loss: 12491.0039, Recon Loss: 24982.0061, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 632/1000, Beta: 5.00, Total Loss: 12491.0015, Recon Loss: 24982.0013, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 633/1000, Beta: 5.00, Total Loss: 12491.0016, Recon Loss: 24982.0016, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 634/1000, Beta: 5.00, Total Loss: 12491.0000, Recon Loss: 24981.9984, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 635/1000, Beta: 5.00, Total Loss: 12490.9983, Recon Loss: 24981.9949, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 636/1000, Beta: 5.00, Total Loss: 12490.9983, Recon Loss: 24981.9949, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 637/1000, Beta: 5.00, Total Loss: 12490.9961, Recon Loss: 24981.9905, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 638/1000, Beta: 5.00, Total Loss: 12490.9940, Recon Loss: 24981.9863, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 639/1000, Beta: 5.00, Total Loss: 12490.9941, Recon Loss: 24981.9866, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 640/1000, Beta: 5.00, Total Loss: 12490.9930, Recon Loss: 24981.9845, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 641/1000, Beta: 5.00, Total Loss: 12490.9944, Recon Loss: 24981.9871, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 642/1000, Beta: 5.00, Total Loss: 12490.9937, Recon Loss: 24981.9857, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 643/1000, Beta: 5.00, Total Loss: 12490.9916, Recon Loss: 24981.9816, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 644/1000, Beta: 5.00, Total Loss: 12490.9915, Recon Loss: 24981.9813, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 645/1000, Beta: 5.00, Total Loss: 12490.9915, Recon Loss: 24981.9813, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 646/1000, Beta: 5.00, Total Loss: 12490.9891, Recon Loss: 24981.9765, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 647/1000, Beta: 5.00, Total Loss: 12490.9894, Recon Loss: 24981.9771, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 648/1000, Beta: 5.00, Total Loss: 12490.9859, Recon Loss: 24981.9700, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 649/1000, Beta: 5.00, Total Loss: 12490.9866, Recon Loss: 24981.9714, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 650/1000, Beta: 5.00, Total Loss: 12490.9868, Recon Loss: 24981.9719, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 651/1000, Beta: 5.00, Total Loss: 12490.9861, Recon Loss: 24981.9706, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 652/1000, Beta: 5.00, Total Loss: 12490.9851, Recon Loss: 24981.9685, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 653/1000, Beta: 5.00, Total Loss: 12490.9858, Recon Loss: 24981.9700, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 654/1000, Beta: 5.00, Total Loss: 12490.9838, Recon Loss: 24981.9658, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 655/1000, Beta: 5.00, Total Loss: 12490.9836, Recon Loss: 24981.9656, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 656/1000, Beta: 5.00, Total Loss: 12490.9821, Recon Loss: 24981.9624, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 657/1000, Beta: 5.00, Total Loss: 12490.9785, Recon Loss: 24981.9554, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 658/1000, Beta: 5.00, Total Loss: 12490.9821, Recon Loss: 24981.9625, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 659/1000, Beta: 5.00, Total Loss: 12490.9758, Recon Loss: 24981.9500, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 660/1000, Beta: 5.00, Total Loss: 12490.9789, Recon Loss: 24981.9562, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 661/1000, Beta: 5.00, Total Loss: 12490.9777, Recon Loss: 24981.9538, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 662/1000, Beta: 5.00, Total Loss: 12490.9770, Recon Loss: 24981.9523, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 663/1000, Beta: 5.00, Total Loss: 12490.9742, Recon Loss: 24981.9468, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 664/1000, Beta: 5.00, Total Loss: 12490.9776, Recon Loss: 24981.9537, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 665/1000, Beta: 5.00, Total Loss: 12490.9749, Recon Loss: 24981.9481, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 666/1000, Beta: 5.00, Total Loss: 12490.9742, Recon Loss: 24981.9467, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 667/1000, Beta: 5.00, Total Loss: 12490.9748, Recon Loss: 24981.9480, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 668/1000, Beta: 5.00, Total Loss: 12490.9714, Recon Loss: 24981.9412, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 669/1000, Beta: 5.00, Total Loss: 12490.9741, Recon Loss: 24981.9466, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 670/1000, Beta: 5.00, Total Loss: 12490.9712, Recon Loss: 24981.9408, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 671/1000, Beta: 5.00, Total Loss: 12490.9701, Recon Loss: 24981.9386, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 672/1000, Beta: 5.00, Total Loss: 12490.9702, Recon Loss: 24981.9386, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 673/1000, Beta: 5.00, Total Loss: 12490.9695, Recon Loss: 24981.9373, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 674/1000, Beta: 5.00, Total Loss: 12490.9675, Recon Loss: 24981.9332, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 675/1000, Beta: 5.00, Total Loss: 12490.9672, Recon Loss: 24981.9327, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 676/1000, Beta: 5.00, Total Loss: 12490.9650, Recon Loss: 24981.9282, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 677/1000, Beta: 5.00, Total Loss: 12490.9658, Recon Loss: 24981.9299, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 678/1000, Beta: 5.00, Total Loss: 12490.9666, Recon Loss: 24981.9315, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 679/1000, Beta: 5.00, Total Loss: 12490.9650, Recon Loss: 24981.9283, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 680/1000, Beta: 5.00, Total Loss: 12490.9605, Recon Loss: 24981.9194, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 681/1000, Beta: 5.00, Total Loss: 12490.9639, Recon Loss: 24981.9261, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 682/1000, Beta: 5.00, Total Loss: 12490.9629, Recon Loss: 24981.9242, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 683/1000, Beta: 5.00, Total Loss: 12490.9605, Recon Loss: 24981.9193, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 684/1000, Beta: 5.00, Total Loss: 12490.9598, Recon Loss: 24981.9180, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 685/1000, Beta: 5.00, Total Loss: 12490.9602, Recon Loss: 24981.9187, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 686/1000, Beta: 5.00, Total Loss: 12490.9608, Recon Loss: 24981.9199, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 687/1000, Beta: 5.00, Total Loss: 12490.9616, Recon Loss: 24981.9217, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 688/1000, Beta: 5.00, Total Loss: 12490.9573, Recon Loss: 24981.9129, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 689/1000, Beta: 5.00, Total Loss: 12490.9556, Recon Loss: 24981.9096, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 690/1000, Beta: 5.00, Total Loss: 12490.9572, Recon Loss: 24981.9127, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 691/1000, Beta: 5.00, Total Loss: 12490.9549, Recon Loss: 24981.9081, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 692/1000, Beta: 5.00, Total Loss: 12490.9541, Recon Loss: 24981.9066, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 693/1000, Beta: 5.00, Total Loss: 12490.9547, Recon Loss: 24981.9078, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 694/1000, Beta: 5.00, Total Loss: 12490.9534, Recon Loss: 24981.9052, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 695/1000, Beta: 5.00, Total Loss: 12490.9508, Recon Loss: 24981.9000, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 696/1000, Beta: 5.00, Total Loss: 12490.9521, Recon Loss: 24981.9025, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 697/1000, Beta: 5.00, Total Loss: 12490.9475, Recon Loss: 24981.8933, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 698/1000, Beta: 5.00, Total Loss: 12490.9503, Recon Loss: 24981.8989, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 699/1000, Beta: 5.00, Total Loss: 12490.9504, Recon Loss: 24981.8991, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 700/1000, Beta: 5.00, Total Loss: 12490.9497, Recon Loss: 24981.8978, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 701/1000, Beta: 5.00, Total Loss: 12490.9499, Recon Loss: 24981.8983, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 702/1000, Beta: 5.00, Total Loss: 12490.9449, Recon Loss: 24981.8883, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 703/1000, Beta: 5.00, Total Loss: 12490.9472, Recon Loss: 24981.8929, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 704/1000, Beta: 5.00, Total Loss: 12490.9478, Recon Loss: 24981.8940, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 705/1000, Beta: 5.00, Total Loss: 12490.9440, Recon Loss: 24981.8864, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 706/1000, Beta: 5.00, Total Loss: 12490.9474, Recon Loss: 24981.8932, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 707/1000, Beta: 5.00, Total Loss: 12490.9445, Recon Loss: 24981.8873, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 708/1000, Beta: 5.00, Total Loss: 12490.9429, Recon Loss: 24981.8842, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 709/1000, Beta: 5.00, Total Loss: 12490.9449, Recon Loss: 24981.8882, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 710/1000, Beta: 5.00, Total Loss: 12490.9422, Recon Loss: 24981.8828, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 711/1000, Beta: 5.00, Total Loss: 12490.9404, Recon Loss: 24981.8793, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 712/1000, Beta: 5.00, Total Loss: 12490.9416, Recon Loss: 24981.8817, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 713/1000, Beta: 5.00, Total Loss: 12490.9431, Recon Loss: 24981.8847, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 714/1000, Beta: 5.00, Total Loss: 12490.9398, Recon Loss: 24981.8780, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 715/1000, Beta: 5.00, Total Loss: 12490.9403, Recon Loss: 24981.8791, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 716/1000, Beta: 5.00, Total Loss: 12490.9390, Recon Loss: 24981.8764, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 717/1000, Beta: 5.00, Total Loss: 12490.9385, Recon Loss: 24981.8755, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 718/1000, Beta: 5.00, Total Loss: 12490.9369, Recon Loss: 24981.8722, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 719/1000, Beta: 5.00, Total Loss: 12490.9349, Recon Loss: 24981.8682, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 720/1000, Beta: 5.00, Total Loss: 12490.9340, Recon Loss: 24981.8666, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 721/1000, Beta: 5.00, Total Loss: 12490.9339, Recon Loss: 24981.8661, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 722/1000, Beta: 5.00, Total Loss: 12490.9352, Recon Loss: 24981.8689, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 723/1000, Beta: 5.00, Total Loss: 12490.9315, Recon Loss: 24981.8614, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 724/1000, Beta: 5.00, Total Loss: 12490.9307, Recon Loss: 24981.8599, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 725/1000, Beta: 5.00, Total Loss: 12490.9310, Recon Loss: 24981.8604, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 726/1000, Beta: 5.00, Total Loss: 12490.9318, Recon Loss: 24981.8619, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 727/1000, Beta: 5.00, Total Loss: 12490.9316, Recon Loss: 24981.8616, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 728/1000, Beta: 5.00, Total Loss: 12490.9303, Recon Loss: 24981.8590, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 729/1000, Beta: 5.00, Total Loss: 12490.9304, Recon Loss: 24981.8592, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 730/1000, Beta: 5.00, Total Loss: 12490.9251, Recon Loss: 24981.8486, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 731/1000, Beta: 5.00, Total Loss: 12490.9295, Recon Loss: 24981.8574, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 732/1000, Beta: 5.00, Total Loss: 12490.9273, Recon Loss: 24981.8530, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 733/1000, Beta: 5.00, Total Loss: 12490.9303, Recon Loss: 24981.8591, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 734/1000, Beta: 5.00, Total Loss: 12490.9244, Recon Loss: 24981.8473, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 735/1000, Beta: 5.00, Total Loss: 12490.9243, Recon Loss: 24981.8472, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 736/1000, Beta: 5.00, Total Loss: 12490.9265, Recon Loss: 24981.8514, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 737/1000, Beta: 5.00, Total Loss: 12490.9260, Recon Loss: 24981.8505, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 738/1000, Beta: 5.00, Total Loss: 12490.9233, Recon Loss: 24981.8451, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 739/1000, Beta: 5.00, Total Loss: 12490.9220, Recon Loss: 24981.8425, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 740/1000, Beta: 5.00, Total Loss: 12490.9222, Recon Loss: 24981.8430, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 741/1000, Beta: 5.00, Total Loss: 12490.9238, Recon Loss: 24981.8461, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 742/1000, Beta: 5.00, Total Loss: 12490.9211, Recon Loss: 24981.8407, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 743/1000, Beta: 5.00, Total Loss: 12490.9176, Recon Loss: 24981.8337, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 744/1000, Beta: 5.00, Total Loss: 12490.9187, Recon Loss: 24981.8359, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 745/1000, Beta: 5.00, Total Loss: 12490.9188, Recon Loss: 24981.8361, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 746/1000, Beta: 5.00, Total Loss: 12490.9210, Recon Loss: 24981.8405, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 747/1000, Beta: 5.00, Total Loss: 12490.9245, Recon Loss: 24981.8475, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 748/1000, Beta: 5.00, Total Loss: 12490.9166, Recon Loss: 24981.8317, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 749/1000, Beta: 5.00, Total Loss: 12490.9163, Recon Loss: 24981.8312, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 750/1000, Beta: 5.00, Total Loss: 12490.9143, Recon Loss: 24981.8272, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 751/1000, Beta: 5.00, Total Loss: 12490.9125, Recon Loss: 24981.8236, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 752/1000, Beta: 5.00, Total Loss: 12490.9135, Recon Loss: 24981.8253, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 753/1000, Beta: 5.00, Total Loss: 12490.9133, Recon Loss: 24981.8251, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 754/1000, Beta: 5.00, Total Loss: 12490.9150, Recon Loss: 24981.8285, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 755/1000, Beta: 5.00, Total Loss: 12490.9114, Recon Loss: 24981.8215, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 756/1000, Beta: 5.00, Total Loss: 12490.9142, Recon Loss: 24981.8267, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 757/1000, Beta: 5.00, Total Loss: 12490.9120, Recon Loss: 24981.8226, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 758/1000, Beta: 5.00, Total Loss: 12490.9085, Recon Loss: 24981.8154, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 759/1000, Beta: 5.00, Total Loss: 12490.9092, Recon Loss: 24981.8169, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 760/1000, Beta: 5.00, Total Loss: 12490.9092, Recon Loss: 24981.8169, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 761/1000, Beta: 5.00, Total Loss: 12490.9102, Recon Loss: 24981.8189, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 762/1000, Beta: 5.00, Total Loss: 12490.9097, Recon Loss: 24981.8179, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 763/1000, Beta: 5.00, Total Loss: 12490.9064, Recon Loss: 24981.8114, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 764/1000, Beta: 5.00, Total Loss: 12490.9054, Recon Loss: 24981.8092, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 765/1000, Beta: 5.00, Total Loss: 12490.9059, Recon Loss: 24981.8102, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 766/1000, Beta: 5.00, Total Loss: 12490.9056, Recon Loss: 24981.8097, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 767/1000, Beta: 5.00, Total Loss: 12490.9049, Recon Loss: 24981.8083, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 768/1000, Beta: 5.00, Total Loss: 12490.9038, Recon Loss: 24981.8061, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 769/1000, Beta: 5.00, Total Loss: 12490.9063, Recon Loss: 24981.8111, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 770/1000, Beta: 5.00, Total Loss: 12490.9068, Recon Loss: 24981.8120, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 771/1000, Beta: 5.00, Total Loss: 12490.9038, Recon Loss: 24981.8061, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 772/1000, Beta: 5.00, Total Loss: 12490.9003, Recon Loss: 24981.7990, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 773/1000, Beta: 5.00, Total Loss: 12490.9027, Recon Loss: 24981.8040, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 774/1000, Beta: 5.00, Total Loss: 12490.9007, Recon Loss: 24981.8000, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 775/1000, Beta: 5.00, Total Loss: 12490.9029, Recon Loss: 24981.8043, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 776/1000, Beta: 5.00, Total Loss: 12490.9005, Recon Loss: 24981.7994, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 777/1000, Beta: 5.00, Total Loss: 12490.8999, Recon Loss: 24981.7984, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 778/1000, Beta: 5.00, Total Loss: 12490.8989, Recon Loss: 24981.7962, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 779/1000, Beta: 5.00, Total Loss: 12490.8984, Recon Loss: 24981.7952, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 780/1000, Beta: 5.00, Total Loss: 12490.8958, Recon Loss: 24981.7901, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 781/1000, Beta: 5.00, Total Loss: 12490.8973, Recon Loss: 24981.7930, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 782/1000, Beta: 5.00, Total Loss: 12490.8970, Recon Loss: 24981.7925, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 783/1000, Beta: 5.00, Total Loss: 12490.8982, Recon Loss: 24981.7949, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 784/1000, Beta: 5.00, Total Loss: 12490.8930, Recon Loss: 24981.7845, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 785/1000, Beta: 5.00, Total Loss: 12490.8948, Recon Loss: 24981.7880, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 786/1000, Beta: 5.00, Total Loss: 12490.8928, Recon Loss: 24981.7842, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 787/1000, Beta: 5.00, Total Loss: 12490.8934, Recon Loss: 24981.7852, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 788/1000, Beta: 5.00, Total Loss: 12490.8929, Recon Loss: 24981.7843, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 789/1000, Beta: 5.00, Total Loss: 12490.8928, Recon Loss: 24981.7841, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 790/1000, Beta: 5.00, Total Loss: 12490.8895, Recon Loss: 24981.7776, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 791/1000, Beta: 5.00, Total Loss: 12490.8942, Recon Loss: 24981.7870, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 792/1000, Beta: 5.00, Total Loss: 12490.8905, Recon Loss: 24981.7795, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 793/1000, Beta: 5.00, Total Loss: 12490.8911, Recon Loss: 24981.7807, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 794/1000, Beta: 5.00, Total Loss: 12490.8889, Recon Loss: 24981.7764, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 795/1000, Beta: 5.00, Total Loss: 12490.8875, Recon Loss: 24981.7737, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 796/1000, Beta: 5.00, Total Loss: 12490.8867, Recon Loss: 24981.7718, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 797/1000, Beta: 5.00, Total Loss: 12490.8898, Recon Loss: 24981.7781, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 798/1000, Beta: 5.00, Total Loss: 12490.8862, Recon Loss: 24981.7710, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 799/1000, Beta: 5.00, Total Loss: 12490.8881, Recon Loss: 24981.7746, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 800/1000, Beta: 5.00, Total Loss: 12490.8861, Recon Loss: 24981.7707, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 801/1000, Beta: 5.00, Total Loss: 12490.8647, Recon Loss: 24981.7279, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 802/1000, Beta: 5.00, Total Loss: 12490.8609, Recon Loss: 24981.7204, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 803/1000, Beta: 5.00, Total Loss: 12490.8587, Recon Loss: 24981.7160, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 804/1000, Beta: 5.00, Total Loss: 12490.8581, Recon Loss: 24981.7149, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 805/1000, Beta: 5.00, Total Loss: 12490.8570, Recon Loss: 24981.7126, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 806/1000, Beta: 5.00, Total Loss: 12490.8556, Recon Loss: 24981.7097, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 807/1000, Beta: 5.00, Total Loss: 12490.8557, Recon Loss: 24981.7100, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 808/1000, Beta: 5.00, Total Loss: 12490.8550, Recon Loss: 24981.7085, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 809/1000, Beta: 5.00, Total Loss: 12490.8547, Recon Loss: 24981.7080, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 810/1000, Beta: 5.00, Total Loss: 12490.8537, Recon Loss: 24981.7060, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 811/1000, Beta: 5.00, Total Loss: 12490.8537, Recon Loss: 24981.7059, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 812/1000, Beta: 5.00, Total Loss: 12490.8528, Recon Loss: 24981.7041, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 813/1000, Beta: 5.00, Total Loss: 12490.8523, Recon Loss: 24981.7033, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 814/1000, Beta: 5.00, Total Loss: 12490.8519, Recon Loss: 24981.7025, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 815/1000, Beta: 5.00, Total Loss: 12490.8507, Recon Loss: 24981.7000, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 816/1000, Beta: 5.00, Total Loss: 12490.8511, Recon Loss: 24981.7007, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 817/1000, Beta: 5.00, Total Loss: 12490.8498, Recon Loss: 24981.6983, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 818/1000, Beta: 5.00, Total Loss: 12490.8497, Recon Loss: 24981.6982, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 819/1000, Beta: 5.00, Total Loss: 12490.8492, Recon Loss: 24981.6969, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 820/1000, Beta: 5.00, Total Loss: 12490.8490, Recon Loss: 24981.6965, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 821/1000, Beta: 5.00, Total Loss: 12490.8476, Recon Loss: 24981.6938, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 822/1000, Beta: 5.00, Total Loss: 12490.8478, Recon Loss: 24981.6942, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 823/1000, Beta: 5.00, Total Loss: 12490.8475, Recon Loss: 24981.6935, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 824/1000, Beta: 5.00, Total Loss: 12490.8473, Recon Loss: 24981.6932, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 825/1000, Beta: 5.00, Total Loss: 12490.8467, Recon Loss: 24981.6920, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 826/1000, Beta: 5.00, Total Loss: 12490.8459, Recon Loss: 24981.6903, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 827/1000, Beta: 5.00, Total Loss: 12490.8457, Recon Loss: 24981.6901, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 828/1000, Beta: 5.00, Total Loss: 12490.8451, Recon Loss: 24981.6889, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 829/1000, Beta: 5.00, Total Loss: 12490.8437, Recon Loss: 24981.6859, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 830/1000, Beta: 5.00, Total Loss: 12490.8442, Recon Loss: 24981.6870, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 831/1000, Beta: 5.00, Total Loss: 12490.8433, Recon Loss: 24981.6852, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 832/1000, Beta: 5.00, Total Loss: 12490.8439, Recon Loss: 24981.6863, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 833/1000, Beta: 5.00, Total Loss: 12490.8426, Recon Loss: 24981.6839, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 834/1000, Beta: 5.00, Total Loss: 12490.8424, Recon Loss: 24981.6835, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 835/1000, Beta: 5.00, Total Loss: 12490.8415, Recon Loss: 24981.6817, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 836/1000, Beta: 5.00, Total Loss: 12490.8418, Recon Loss: 24981.6821, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 837/1000, Beta: 5.00, Total Loss: 12490.8413, Recon Loss: 24981.6813, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 838/1000, Beta: 5.00, Total Loss: 12490.8405, Recon Loss: 24981.6797, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 839/1000, Beta: 5.00, Total Loss: 12490.8402, Recon Loss: 24981.6791, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 840/1000, Beta: 5.00, Total Loss: 12490.8395, Recon Loss: 24981.6776, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 841/1000, Beta: 5.00, Total Loss: 12490.8394, Recon Loss: 24981.6773, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 842/1000, Beta: 5.00, Total Loss: 12490.8386, Recon Loss: 24981.6758, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 843/1000, Beta: 5.00, Total Loss: 12490.8385, Recon Loss: 24981.6757, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 844/1000, Beta: 5.00, Total Loss: 12490.8369, Recon Loss: 24981.6725, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 845/1000, Beta: 5.00, Total Loss: 12490.8373, Recon Loss: 24981.6733, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 846/1000, Beta: 5.00, Total Loss: 12490.8368, Recon Loss: 24981.6723, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 847/1000, Beta: 5.00, Total Loss: 12490.8363, Recon Loss: 24981.6712, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 848/1000, Beta: 5.00, Total Loss: 12490.8358, Recon Loss: 24981.6703, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 849/1000, Beta: 5.00, Total Loss: 12490.8354, Recon Loss: 24981.6696, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 850/1000, Beta: 5.00, Total Loss: 12490.8350, Recon Loss: 24981.6687, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 851/1000, Beta: 5.00, Total Loss: 12490.8352, Recon Loss: 24981.6690, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 852/1000, Beta: 5.00, Total Loss: 12490.8336, Recon Loss: 24981.6658, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 853/1000, Beta: 5.00, Total Loss: 12490.8341, Recon Loss: 24981.6668, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 854/1000, Beta: 5.00, Total Loss: 12490.8335, Recon Loss: 24981.6657, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 855/1000, Beta: 5.00, Total Loss: 12490.8333, Recon Loss: 24981.6651, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 856/1000, Beta: 5.00, Total Loss: 12490.8330, Recon Loss: 24981.6646, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 857/1000, Beta: 5.00, Total Loss: 12490.8321, Recon Loss: 24981.6629, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 858/1000, Beta: 5.00, Total Loss: 12490.8316, Recon Loss: 24981.6619, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 859/1000, Beta: 5.00, Total Loss: 12490.8306, Recon Loss: 24981.6599, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 860/1000, Beta: 5.00, Total Loss: 12490.8311, Recon Loss: 24981.6608, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 861/1000, Beta: 5.00, Total Loss: 12490.8303, Recon Loss: 24981.6591, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 862/1000, Beta: 5.00, Total Loss: 12490.8303, Recon Loss: 24981.6592, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 863/1000, Beta: 5.00, Total Loss: 12490.8297, Recon Loss: 24981.6580, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 864/1000, Beta: 5.00, Total Loss: 12490.8291, Recon Loss: 24981.6568, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 865/1000, Beta: 5.00, Total Loss: 12490.8286, Recon Loss: 24981.6559, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 866/1000, Beta: 5.00, Total Loss: 12490.8289, Recon Loss: 24981.6565, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 867/1000, Beta: 5.00, Total Loss: 12490.8276, Recon Loss: 24981.6539, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 868/1000, Beta: 5.00, Total Loss: 12490.8279, Recon Loss: 24981.6545, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 869/1000, Beta: 5.00, Total Loss: 12490.8266, Recon Loss: 24981.6520, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 870/1000, Beta: 5.00, Total Loss: 12490.8264, Recon Loss: 24981.6514, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 871/1000, Beta: 5.00, Total Loss: 12490.8268, Recon Loss: 24981.6523, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 872/1000, Beta: 5.00, Total Loss: 12490.8255, Recon Loss: 24981.6498, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 873/1000, Beta: 5.00, Total Loss: 12490.8253, Recon Loss: 24981.6492, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 874/1000, Beta: 5.00, Total Loss: 12490.8242, Recon Loss: 24981.6471, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 875/1000, Beta: 5.00, Total Loss: 12490.8242, Recon Loss: 24981.6471, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 876/1000, Beta: 5.00, Total Loss: 12490.8237, Recon Loss: 24981.6461, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 877/1000, Beta: 5.00, Total Loss: 12490.8238, Recon Loss: 24981.6464, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 878/1000, Beta: 5.00, Total Loss: 12490.8236, Recon Loss: 24981.6460, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 879/1000, Beta: 5.00, Total Loss: 12490.8224, Recon Loss: 24981.6434, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 880/1000, Beta: 5.00, Total Loss: 12490.8214, Recon Loss: 24981.6416, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 881/1000, Beta: 5.00, Total Loss: 12490.8218, Recon Loss: 24981.6423, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 882/1000, Beta: 5.00, Total Loss: 12490.8209, Recon Loss: 24981.6406, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 883/1000, Beta: 5.00, Total Loss: 12490.8206, Recon Loss: 24981.6399, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 884/1000, Beta: 5.00, Total Loss: 12490.8210, Recon Loss: 24981.6406, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 885/1000, Beta: 5.00, Total Loss: 12490.8211, Recon Loss: 24981.6409, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 886/1000, Beta: 5.00, Total Loss: 12490.8191, Recon Loss: 24981.6369, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 887/1000, Beta: 5.00, Total Loss: 12490.8190, Recon Loss: 24981.6367, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 888/1000, Beta: 5.00, Total Loss: 12490.8199, Recon Loss: 24981.6384, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 889/1000, Beta: 5.00, Total Loss: 12490.8181, Recon Loss: 24981.6349, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 890/1000, Beta: 5.00, Total Loss: 12490.8186, Recon Loss: 24981.6358, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 891/1000, Beta: 5.00, Total Loss: 12490.8179, Recon Loss: 24981.6345, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 892/1000, Beta: 5.00, Total Loss: 12490.8174, Recon Loss: 24981.6335, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 893/1000, Beta: 5.00, Total Loss: 12490.8169, Recon Loss: 24981.6324, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 894/1000, Beta: 5.00, Total Loss: 12490.8165, Recon Loss: 24981.6316, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 895/1000, Beta: 5.00, Total Loss: 12490.8165, Recon Loss: 24981.6317, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 896/1000, Beta: 5.00, Total Loss: 12490.8162, Recon Loss: 24981.6310, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 897/1000, Beta: 5.00, Total Loss: 12490.8158, Recon Loss: 24981.6303, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 898/1000, Beta: 5.00, Total Loss: 12490.8158, Recon Loss: 24981.6304, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 899/1000, Beta: 5.00, Total Loss: 12490.8139, Recon Loss: 24981.6266, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 900/1000, Beta: 5.00, Total Loss: 12490.8143, Recon Loss: 24981.6273, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 901/1000, Beta: 5.00, Total Loss: 12490.8136, Recon Loss: 24981.6260, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 902/1000, Beta: 5.00, Total Loss: 12490.8126, Recon Loss: 24981.6239, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 903/1000, Beta: 5.00, Total Loss: 12490.8125, Recon Loss: 24981.6237, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 904/1000, Beta: 5.00, Total Loss: 12490.8127, Recon Loss: 24981.6241, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 905/1000, Beta: 5.00, Total Loss: 12490.8125, Recon Loss: 24981.6237, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 906/1000, Beta: 5.00, Total Loss: 12490.8116, Recon Loss: 24981.6220, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 907/1000, Beta: 5.00, Total Loss: 12490.8117, Recon Loss: 24981.6221, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 908/1000, Beta: 5.00, Total Loss: 12490.8109, Recon Loss: 24981.6205, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 909/1000, Beta: 5.00, Total Loss: 12490.8101, Recon Loss: 24981.6189, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 910/1000, Beta: 5.00, Total Loss: 12490.8101, Recon Loss: 24981.6189, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 911/1000, Beta: 5.00, Total Loss: 12490.8098, Recon Loss: 24981.6183, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 912/1000, Beta: 5.00, Total Loss: 12490.8093, Recon Loss: 24981.6171, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 913/1000, Beta: 5.00, Total Loss: 12490.8088, Recon Loss: 24981.6163, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 914/1000, Beta: 5.00, Total Loss: 12490.8095, Recon Loss: 24981.6176, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 915/1000, Beta: 5.00, Total Loss: 12490.8075, Recon Loss: 24981.6138, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 916/1000, Beta: 5.00, Total Loss: 12490.8085, Recon Loss: 24981.6158, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 917/1000, Beta: 5.00, Total Loss: 12490.8071, Recon Loss: 24981.6129, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 918/1000, Beta: 5.00, Total Loss: 12490.8078, Recon Loss: 24981.6142, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 919/1000, Beta: 5.00, Total Loss: 12490.8067, Recon Loss: 24981.6121, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 920/1000, Beta: 5.00, Total Loss: 12490.8065, Recon Loss: 24981.6118, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 921/1000, Beta: 5.00, Total Loss: 12490.8062, Recon Loss: 24981.6111, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 922/1000, Beta: 5.00, Total Loss: 12490.8056, Recon Loss: 24981.6100, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 923/1000, Beta: 5.00, Total Loss: 12490.8049, Recon Loss: 24981.6084, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 924/1000, Beta: 5.00, Total Loss: 12490.8045, Recon Loss: 24981.6078, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 925/1000, Beta: 5.00, Total Loss: 12490.8042, Recon Loss: 24981.6072, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 926/1000, Beta: 5.00, Total Loss: 12490.8038, Recon Loss: 24981.6064, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 927/1000, Beta: 5.00, Total Loss: 12490.8033, Recon Loss: 24981.6053, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 928/1000, Beta: 5.00, Total Loss: 12490.8031, Recon Loss: 24981.6048, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 929/1000, Beta: 5.00, Total Loss: 12490.8039, Recon Loss: 24981.6065, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 930/1000, Beta: 5.00, Total Loss: 12490.8023, Recon Loss: 24981.6033, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 931/1000, Beta: 5.00, Total Loss: 12490.8017, Recon Loss: 24981.6020, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 932/1000, Beta: 5.00, Total Loss: 12490.8018, Recon Loss: 24981.6024, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 933/1000, Beta: 5.00, Total Loss: 12490.8009, Recon Loss: 24981.6006, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 934/1000, Beta: 5.00, Total Loss: 12490.8014, Recon Loss: 24981.6016, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 935/1000, Beta: 5.00, Total Loss: 12490.8003, Recon Loss: 24981.5993, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 936/1000, Beta: 5.00, Total Loss: 12490.8002, Recon Loss: 24981.5991, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 937/1000, Beta: 5.00, Total Loss: 12490.7998, Recon Loss: 24981.5982, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 938/1000, Beta: 5.00, Total Loss: 12490.7988, Recon Loss: 24981.5964, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 939/1000, Beta: 5.00, Total Loss: 12490.7988, Recon Loss: 24981.5963, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 940/1000, Beta: 5.00, Total Loss: 12490.7984, Recon Loss: 24981.5956, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 941/1000, Beta: 5.00, Total Loss: 12490.7981, Recon Loss: 24981.5948, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 942/1000, Beta: 5.00, Total Loss: 12490.7977, Recon Loss: 24981.5940, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 943/1000, Beta: 5.00, Total Loss: 12490.7970, Recon Loss: 24981.5928, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 944/1000, Beta: 5.00, Total Loss: 12490.7964, Recon Loss: 24981.5916, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 945/1000, Beta: 5.00, Total Loss: 12490.7980, Recon Loss: 24981.5949, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 946/1000, Beta: 5.00, Total Loss: 12490.7968, Recon Loss: 24981.5923, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 947/1000, Beta: 5.00, Total Loss: 12490.7966, Recon Loss: 24981.5920, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 948/1000, Beta: 5.00, Total Loss: 12490.7957, Recon Loss: 24981.5902, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 949/1000, Beta: 5.00, Total Loss: 12490.7948, Recon Loss: 24981.5883, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 950/1000, Beta: 5.00, Total Loss: 12490.7943, Recon Loss: 24981.5873, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 951/1000, Beta: 5.00, Total Loss: 12490.7942, Recon Loss: 24981.5870, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 952/1000, Beta: 5.00, Total Loss: 12490.7946, Recon Loss: 24981.5880, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 953/1000, Beta: 5.00, Total Loss: 12490.7936, Recon Loss: 24981.5861, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 954/1000, Beta: 5.00, Total Loss: 12490.7935, Recon Loss: 24981.5857, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 955/1000, Beta: 5.00, Total Loss: 12490.7929, Recon Loss: 24981.5845, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 956/1000, Beta: 5.00, Total Loss: 12490.7923, Recon Loss: 24981.5833, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 957/1000, Beta: 5.00, Total Loss: 12490.7925, Recon Loss: 24981.5837, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 958/1000, Beta: 5.00, Total Loss: 12490.7925, Recon Loss: 24981.5836, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 959/1000, Beta: 5.00, Total Loss: 12490.7907, Recon Loss: 24981.5802, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 960/1000, Beta: 5.00, Total Loss: 12490.7919, Recon Loss: 24981.5826, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 961/1000, Beta: 5.00, Total Loss: 12490.7902, Recon Loss: 24981.5791, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 962/1000, Beta: 5.00, Total Loss: 12490.7900, Recon Loss: 24981.5786, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 963/1000, Beta: 5.00, Total Loss: 12490.7895, Recon Loss: 24981.5777, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 964/1000, Beta: 5.00, Total Loss: 12490.7896, Recon Loss: 24981.5780, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 965/1000, Beta: 5.00, Total Loss: 12490.7892, Recon Loss: 24981.5770, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 966/1000, Beta: 5.00, Total Loss: 12490.7890, Recon Loss: 24981.5768, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 967/1000, Beta: 5.00, Total Loss: 12490.7888, Recon Loss: 24981.5765, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 968/1000, Beta: 5.00, Total Loss: 12490.7880, Recon Loss: 24981.5748, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 969/1000, Beta: 5.00, Total Loss: 12490.7878, Recon Loss: 24981.5744, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 970/1000, Beta: 5.00, Total Loss: 12490.7872, Recon Loss: 24981.5732, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 971/1000, Beta: 5.00, Total Loss: 12490.7872, Recon Loss: 24981.5731, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 972/1000, Beta: 5.00, Total Loss: 12490.7880, Recon Loss: 24981.5748, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 973/1000, Beta: 5.00, Total Loss: 12490.7861, Recon Loss: 24981.5709, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 974/1000, Beta: 5.00, Total Loss: 12490.7864, Recon Loss: 24981.5716, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 975/1000, Beta: 5.00, Total Loss: 12490.7857, Recon Loss: 24981.5703, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 976/1000, Beta: 5.00, Total Loss: 12490.7860, Recon Loss: 24981.5708, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 977/1000, Beta: 5.00, Total Loss: 12490.7858, Recon Loss: 24981.5703, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 978/1000, Beta: 5.00, Total Loss: 12490.7841, Recon Loss: 24981.5670, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 979/1000, Beta: 5.00, Total Loss: 12490.7843, Recon Loss: 24981.5673, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 980/1000, Beta: 5.00, Total Loss: 12490.7843, Recon Loss: 24981.5674, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 981/1000, Beta: 5.00, Total Loss: 12490.7836, Recon Loss: 24981.5659, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 982/1000, Beta: 5.00, Total Loss: 12490.7833, Recon Loss: 24981.5654, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 983/1000, Beta: 5.00, Total Loss: 12490.7833, Recon Loss: 24981.5654, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 984/1000, Beta: 5.00, Total Loss: 12490.7823, Recon Loss: 24981.5634, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 985/1000, Beta: 5.00, Total Loss: 12490.7824, Recon Loss: 24981.5635, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 986/1000, Beta: 5.00, Total Loss: 12490.7820, Recon Loss: 24981.5628, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 987/1000, Beta: 5.00, Total Loss: 12490.7816, Recon Loss: 24981.5620, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 988/1000, Beta: 5.00, Total Loss: 12490.7811, Recon Loss: 24981.5609, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 989/1000, Beta: 5.00, Total Loss: 12490.7801, Recon Loss: 24981.5591, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 990/1000, Beta: 5.00, Total Loss: 12490.7798, Recon Loss: 24981.5583, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 991/1000, Beta: 5.00, Total Loss: 12490.7798, Recon Loss: 24981.5583, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 992/1000, Beta: 5.00, Total Loss: 12490.7805, Recon Loss: 24981.5597, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 993/1000, Beta: 5.00, Total Loss: 12490.7797, Recon Loss: 24981.5580, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 994/1000, Beta: 5.00, Total Loss: 12490.7791, Recon Loss: 24981.5570, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 995/1000, Beta: 5.00, Total Loss: 12490.7792, Recon Loss: 24981.5573, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 996/1000, Beta: 5.00, Total Loss: 12490.7788, Recon Loss: 24981.5565, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 997/1000, Beta: 5.00, Total Loss: 12490.7781, Recon Loss: 24981.5549, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 998/1000, Beta: 5.00, Total Loss: 12490.7783, Recon Loss: 24981.5554, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Epoch 999/1000, Beta: 5.00, Total Loss: 12490.7767, Recon Loss: 24981.5523, KL Loss: 0.0000, Percep Loss: 0.0006\n",
      "Epoch 1000/1000, Beta: 5.00, Total Loss: 12490.7770, Recon Loss: 24981.5529, KL Loss: 0.0000, Percep Loss: 0.0005\n",
      "Saved final CVAE model to FL_CVAE\\cvae_vehicle_final.pth\n",
      "Generated 100 samples for Class 3 (Seden) at the end of training.\n",
      "Generated 100 samples for Class 4 (SUV) at the end of training.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms.functional import adjust_sharpness\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import kagglehub\n",
    "from PIL import Image\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Set CUDA_LAUNCH_BLOCKING for debugging\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.device_count() > 1 else 'cuda')\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 128\n",
    "batch_size = 16\n",
    "epochs = 1000\n",
    "learning_rate = 5e-4\n",
    "image_size = 128\n",
    "channels = 3\n",
    "output_dir = \"FL_CVAE\"\n",
    "beta_max = 5.0\n",
    "annealing_epochs = 50\n",
    "perceptual_weight = 1.0\n",
    "recon_weight = 0.5\n",
    "\n",
    "# Download Vehicle Type Image Dataset from Kaggle\n",
    "try:\n",
    "    path = kagglehub.dataset_download(\"sujaykapadnis/vehicle-type-image-dataset\")\n",
    "    print(\"Path to dataset files:\", path)\n",
    "    dataset_path = path\n",
    "except Exception as e:\n",
    "    print(f\"Failed to download dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Define the VehicleTypeDataset class\n",
    "class VehicleTypeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_names = []\n",
    "        self.class_to_idx = {}\n",
    "\n",
    "        print(f\"Searching for images in {root_dir}\")\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            image_files = [f for f in files if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "            if image_files:\n",
    "                class_name = os.path.basename(root)\n",
    "                if class_name not in self.class_to_idx:\n",
    "                    self.class_names.append(class_name)\n",
    "                    self.class_to_idx[class_name] = len(self.class_names) - 1\n",
    "                for img_file in image_files:\n",
    "                    img_path = os.path.join(root, img_file)\n",
    "                    self.images.append(img_path)\n",
    "                    self.labels.append(self.class_to_idx[class_name])\n",
    "\n",
    "        if not self.images:\n",
    "            raise ValueError(\n",
    "                f\"No images found in {root_dir}. \"\n",
    "                \"Expected class folders containing .jpg, .png, or .jpeg images.\"\n",
    "            )\n",
    "\n",
    "        print(f\"Found {len(self.images)} images across {len(self.class_names)} classes.\")\n",
    "        print(f\"Classes: {self.class_names}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = VehicleTypeDataset(root_dir=dataset_path, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Dynamically set num_classes based on the dataset\n",
    "num_classes = len(dataset.class_names)\n",
    "print(f\"Number of classes in dataset: {num_classes}\")\n",
    "\n",
    "# Define classes to generate (choose the last two classes if possible)\n",
    "if num_classes >= 2:\n",
    "    classes_to_generate = [num_classes - 2, num_classes - 1]  # Last two classes\n",
    "else:\n",
    "    classes_to_generate = [0]  # Fallback to the first class if fewer than 2 classes\n",
    "print(f\"Classes to generate: {classes_to_generate}\")\n",
    "\n",
    "# Define the Encoder network\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels + num_classes, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.fc_mean = nn.Linear(512 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(512 * 4 * 4, latent_dim)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        y = F.one_hot(y, num_classes=num_classes).float()\n",
    "        y = y.unsqueeze(-1).unsqueeze(-1)\n",
    "        y = y.expand(-1, -1, x.size(2), x.size(3))\n",
    "        x_with_y = torch.cat([x, y], dim=1)\n",
    "        \n",
    "        h1 = F.relu(self.conv1(x_with_y))\n",
    "        if torch.isnan(h1).any() or torch.isinf(h1).any():\n",
    "            print(\"NaN or Inf in h1\")\n",
    "        h2 = F.relu(self.conv2(h1))\n",
    "        if torch.isnan(h2).any() or torch.isinf(h2).any():\n",
    "            print(\"NaN or Inf in h2\")\n",
    "        h3 = F.relu(self.conv3(h2))\n",
    "        if torch.isnan(h3).any() or torch.isinf(h3).any():\n",
    "            print(\"NaN or Inf in h3\")\n",
    "        h4 = F.relu(self.conv4(h3))\n",
    "        if torch.isnan(h4).any() or torch.isinf(h4).any():\n",
    "            print(\"NaN or Inf in h4\")\n",
    "        h5 = F.relu(self.conv5(h4))\n",
    "        if torch.isnan(h5).any() or torch.isinf(h5).any():\n",
    "            print(\"NaN or Inf in h5\")\n",
    "        h = h5.view(h5.size(0), -1)\n",
    "        z_mean = self.fc_mean(h)\n",
    "        z_logvar = self.fc_logvar(h)\n",
    "        return z_mean, z_logvar, (h1, h2, h3, h4, h5)\n",
    "\n",
    "# Define the Decoder network\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim + num_classes, 512 * 4 * 4)\n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(512, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(256, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(128, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv5 = nn.ConvTranspose2d(64, channels, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, z, y, skip_connections):\n",
    "        h1, h2, h3, h4, h5 = skip_connections\n",
    "        y = F.one_hot(y, num_classes=num_classes).float()\n",
    "        z_with_y = torch.cat([z, y], dim=-1)\n",
    "        h = F.relu(self.fc(z_with_y))\n",
    "        h = h.view(h.size(0), 512, 4, 4)\n",
    "        \n",
    "        h = F.relu(self.deconv1(h))\n",
    "        h = torch.cat([h, h4], dim=1)\n",
    "        h = F.relu(self.deconv2(h))\n",
    "        h = torch.cat([h, h3], dim=1)\n",
    "        h = F.relu(self.deconv3(h))\n",
    "        h = torch.cat([h, h2], dim=1)\n",
    "        h = F.relu(self.deconv4(h))\n",
    "        h = torch.cat([h, h1], dim=1)\n",
    "        x_reconstructed = torch.sigmoid(self.deconv5(h))\n",
    "        if torch.isnan(x_reconstructed).any() or torch.isinf(x_reconstructed).any():\n",
    "            print(\"NaN or Inf in x_reconstructed\")\n",
    "        return x_reconstructed\n",
    "\n",
    "# Conditional VAE model\n",
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def reparameterize(self, z_mean, z_logvar):\n",
    "        std = torch.exp(0.5 * z_logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return z_mean + eps * std\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        z_mean, z_logvar, skip_connections = self.encoder(x, y)\n",
    "        z = self.reparameterize(z_mean, z_logvar)\n",
    "        x_reconstructed = self.decoder(z, y, skip_connections)\n",
    "        return x_reconstructed, z_mean, z_logvar\n",
    "\n",
    "# Load pretrained VGG16 for perceptual loss\n",
    "vgg = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features.to(device).eval()\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "def perceptual_loss(x, x_reconstructed):\n",
    "    x_subset = x[:8]\n",
    "    x_reconstructed_subset = x_reconstructed[:8]\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(x.device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(x.device)\n",
    "    x_normalized = (x_subset - mean) / std\n",
    "    x_reconstructed_normalized = (x_reconstructed_subset - mean) / std\n",
    "    x_features = vgg(x_normalized)\n",
    "    x_recon_features = vgg(x_reconstructed_normalized)\n",
    "    return F.mse_loss(x_features, x_recon_features)\n",
    "\n",
    "# Instantiate Encoder, Decoder, and CVAE\n",
    "encoder = Encoder(latent_dim, num_classes).to(device)\n",
    "decoder = Decoder(latent_dim, num_classes).to(device)\n",
    "cvae = ConditionalVAE(encoder, decoder).to(device)\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "optimizer = optim.Adam(cvae.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.5)\n",
    "\n",
    "# Define loss function with beta annealing\n",
    "def cvae_loss(x, x_reconstructed, z_mean, z_logvar, beta=1.0, recon_weight=1.0, perceptual_weight=1.0):\n",
    "    if torch.isnan(x).any() or torch.isinf(x).any():\n",
    "        print(\"NaN or Inf detected in x\")\n",
    "    if torch.isnan(x_reconstructed).any() or torch.isinf(x_reconstructed).any():\n",
    "        print(\"NaN or Inf detected in x_reconstructed\")\n",
    "    if torch.isnan(z_mean).any() or torch.isinf(z_mean).any():\n",
    "        print(\"NaN or Inf detected in z_mean\")\n",
    "    if torch.isnan(z_logvar).any() or torch.isinf(z_logvar).any():\n",
    "        print(\"NaN or Inf detected in z_logvar\")\n",
    "\n",
    "    recon_loss = F.binary_cross_entropy(x_reconstructed, x, reduction='sum')\n",
    "    kl_loss = -0.5 * torch.sum(1 + torch.clamp(z_logvar, -5, 5) - z_mean.pow(2) - torch.clamp(z_logvar, -5, 5).exp())\n",
    "    percep_loss = perceptual_loss(x, x_reconstructed) * perceptual_weight\n",
    "    total_loss = recon_weight * recon_loss + beta * kl_loss + percep_loss\n",
    "    return total_loss, recon_loss, kl_loss, percep_loss\n",
    "\n",
    "# Training loop\n",
    "cvae.train()\n",
    "for epoch in range(epochs):\n",
    "    if epoch < annealing_epochs:\n",
    "        beta = beta_max * (epoch / annealing_epochs)\n",
    "    else:\n",
    "        beta = beta_max\n",
    "\n",
    "    train_loss = 0\n",
    "    train_recon_loss = 0\n",
    "    train_kl_loss = 0\n",
    "    train_percep_loss = 0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        x_reconstructed, z_mean, z_logvar = cvae(data, labels)\n",
    "        total_loss, recon_loss, kl_loss, percep_loss = cvae_loss(\n",
    "            data, x_reconstructed, z_mean, z_logvar, \n",
    "            beta=beta, recon_weight=recon_weight, perceptual_weight=perceptual_weight\n",
    "        )\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(cvae.parameters(), max_norm=0.5)\n",
    "        train_loss += total_loss.item()\n",
    "        train_recon_loss += recon_loss.item()\n",
    "        train_kl_loss += kl_loss.item()\n",
    "        train_percep_loss += percep_loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    avg_recon_loss = train_recon_loss / len(train_loader.dataset)\n",
    "    avg_kl_loss = train_kl_loss / len(train_loader.dataset)\n",
    "    avg_percep_loss = train_percep_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Beta: {beta:.2f}, Total Loss: {avg_loss:.4f}, '\n",
    "          f'Recon Loss: {avg_recon_loss:.4f}, KL Loss: {avg_kl_loss:.4f}, '\n",
    "          f'Percep Loss: {avg_percep_loss:.4f}')\n",
    "\n",
    "# Save final model\n",
    "final_model_path = os.path.join(output_dir, \"cvae_vehicle_final.pth\")\n",
    "torch.save(cvae.state_dict(), final_model_path)\n",
    "print(f\"Saved final CVAE model to {final_model_path}\")\n",
    "\n",
    "# Generate and visualize samples at the end\n",
    "cvae.eval()\n",
    "base_dir = os.path.join(output_dir, \"generated_samples_final\")\n",
    "num_samples = 100\n",
    "with torch.no_grad():\n",
    "    for class_label in classes_to_generate:\n",
    "        label_tensor = torch.tensor([class_label]).repeat(num_samples).to(device)\n",
    "        z = torch.randn(num_samples, latent_dim).to(device)\n",
    "        dummy_skips = [\n",
    "            torch.randn(num_samples, 32, 64, 64).to(device) * 0.1,\n",
    "            torch.randn(num_samples, 64, 32, 32).to(device) * 0.1,\n",
    "            torch.randn(num_samples, 128, 16, 16).to(device) * 0.1,\n",
    "            torch.randn(num_samples, 256, 8, 8).to(device) * 0.1,\n",
    "            torch.randn(num_samples, 512, 4, 4).to(device) * 0.1\n",
    "        ]\n",
    "        generated_samples = cvae.decoder(z, label_tensor, dummy_skips)\n",
    "        class_dir = os.path.join(base_dir, str(class_label))\n",
    "        os.makedirs(class_dir, exist_ok=True)\n",
    "        for idx, sample in enumerate(generated_samples):\n",
    "            sample = adjust_sharpness(sample, sharpness_factor=2.0)\n",
    "            save_image(sample, os.path.join(class_dir, f\"sample_{idx}.png\"))\n",
    "        # Safely access class name\n",
    "        class_name = dataset.class_names[class_label] if class_label < len(dataset.class_names) else f\"Class_{class_label}\"\n",
    "        print(f\"Generated {num_samples} samples for Class {class_label} ({class_name}) at the end of training.\")\n",
    "\n",
    "# Plot samples\n",
    "fig, axs = plt.subplots(len(classes_to_generate), 10, figsize=(20, 4))\n",
    "for row, class_label in enumerate(classes_to_generate):\n",
    "    class_dir = os.path.join(base_dir, str(class_label))\n",
    "    sample_files = os.listdir(class_dir)\n",
    "    random_samples = np.random.choice(sample_files, 10, replace=False)\n",
    "    for col, sample_file in enumerate(random_samples):\n",
    "        sample_path = os.path.join(class_dir, sample_file)\n",
    "        sample_image = Image.open(sample_path).convert(\"RGB\")\n",
    "        sample_image = sample_image.resize((128, 128), Image.LANCZOS)\n",
    "        sample_image = np.array(sample_image) / 255.0\n",
    "        ax = axs[row, col] if len(classes_to_generate) > 1 else axs[col]\n",
    "        ax.imshow(sample_image)\n",
    "        ax.axis('off')\n",
    "        if col == 0:\n",
    "            # Safely access class name for plotting\n",
    "            class_name = dataset.class_names[class_label] if class_label < len(dataset.class_names) else f\"Class_{class_label}\"\n",
    "            ax.set_ylabel(class_name, rotation=90, labelpad=10)\n",
    "plt.tight_layout()\n",
    "plot_path = os.path.join(output_dir, f\"synthetic_samples_classes_{'_'.join(map(str, classes_to_generate))}_final.png\")\n",
    "plt.savefig(plot_path)\n",
    "plt.close()\n",
    "print(f\"Saved synthetic samples plot to {plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21214134-83fc-4697-9141-95019597ff4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
