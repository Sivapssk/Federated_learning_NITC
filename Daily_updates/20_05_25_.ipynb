{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fe82e2-7a0e-49a0-b4ba-4d07dc422173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef64f2d6-26bf-4c1c-864a-301129939b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##RGB--->GREY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc81ec6a-66bd-4745-8eda-68e100150a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Searching for images in C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Found 1000 images across 2 classes.\n",
      "Classes: ['Hatchback', 'Other']\n",
      "Epoch 1/1000, Beta: 0.00, Total Loss: 5096.8752, Recon Loss: 10193.5189, KL Loss: 538.2337, Percep Loss: 0.1158\n",
      "Epoch 2/1000, Beta: 0.10, Total Loss: 4382.3090, Recon Loss: 8762.3615, KL Loss: 10.2121, Percep Loss: 0.1071\n",
      "Epoch 3/1000, Beta: 0.20, Total Loss: 4330.1876, Recon Loss: 8659.9946, KL Loss: 0.4808, Percep Loss: 0.0941\n",
      "Epoch 4/1000, Beta: 0.30, Total Loss: 4303.3290, Recon Loss: 8606.4371, KL Loss: 0.0896, Percep Loss: 0.0837\n",
      "Epoch 5/1000, Beta: 0.40, Total Loss: 4295.4675, Recon Loss: 8590.7705, KL Loss: 0.0263, Percep Loss: 0.0717\n",
      "Epoch 6/1000, Beta: 0.50, Total Loss: 4289.2458, Recon Loss: 8578.3385, KL Loss: 0.0169, Percep Loss: 0.0682\n",
      "Epoch 7/1000, Beta: 0.60, Total Loss: 4281.7255, Recon Loss: 8563.3091, KL Loss: 0.0111, Percep Loss: 0.0642\n",
      "Epoch 8/1000, Beta: 0.70, Total Loss: 4276.2110, Recon Loss: 8552.2813, KL Loss: 0.0068, Percep Loss: 0.0656\n",
      "Epoch 9/1000, Beta: 0.80, Total Loss: 4270.7414, Recon Loss: 8541.3628, KL Loss: 0.0043, Percep Loss: 0.0567\n",
      "Epoch 10/1000, Beta: 0.90, Total Loss: 4267.3076, Recon Loss: 8534.5032, KL Loss: 0.0026, Percep Loss: 0.0537\n",
      "Epoch 11/1000, Beta: 1.00, Total Loss: 4265.9768, Recon Loss: 8531.8434, KL Loss: 0.0027, Percep Loss: 0.0524\n",
      "Epoch 12/1000, Beta: 1.10, Total Loss: 4263.8192, Recon Loss: 8527.5485, KL Loss: 0.0006, Percep Loss: 0.0443\n",
      "Epoch 13/1000, Beta: 1.20, Total Loss: 4262.4164, Recon Loss: 8524.7297, KL Loss: 0.0005, Percep Loss: 0.0510\n",
      "Epoch 14/1000, Beta: 1.30, Total Loss: 4260.4151, Recon Loss: 8520.7424, KL Loss: 0.0002, Percep Loss: 0.0437\n",
      "Epoch 15/1000, Beta: 1.40, Total Loss: 4259.7106, Recon Loss: 8519.3418, KL Loss: 0.0001, Percep Loss: 0.0395\n",
      "Epoch 16/1000, Beta: 1.50, Total Loss: 4258.1080, Recon Loss: 8516.1387, KL Loss: 0.0001, Percep Loss: 0.0385\n",
      "Epoch 17/1000, Beta: 1.60, Total Loss: 4257.7677, Recon Loss: 8515.4611, KL Loss: 0.0001, Percep Loss: 0.0370\n",
      "Epoch 18/1000, Beta: 1.70, Total Loss: 4260.9313, Recon Loss: 8521.7713, KL Loss: 0.0004, Percep Loss: 0.0450\n",
      "Epoch 19/1000, Beta: 1.80, Total Loss: 4255.7686, Recon Loss: 8511.4562, KL Loss: 0.0002, Percep Loss: 0.0402\n",
      "Epoch 20/1000, Beta: 1.90, Total Loss: 4255.5161, Recon Loss: 8510.9612, KL Loss: 0.0001, Percep Loss: 0.0354\n",
      "Epoch 21/1000, Beta: 2.00, Total Loss: 4254.7059, Recon Loss: 8509.3420, KL Loss: 0.0001, Percep Loss: 0.0348\n",
      "Epoch 22/1000, Beta: 2.10, Total Loss: 4255.0187, Recon Loss: 8509.9627, KL Loss: 0.0001, Percep Loss: 0.0371\n",
      "Epoch 23/1000, Beta: 2.20, Total Loss: 4255.0040, Recon Loss: 8509.9447, KL Loss: 0.0000, Percep Loss: 0.0316\n",
      "Epoch 24/1000, Beta: 2.30, Total Loss: 4253.1142, Recon Loss: 8506.1638, KL Loss: 0.0000, Percep Loss: 0.0323\n",
      "Epoch 25/1000, Beta: 2.40, Total Loss: 4253.1837, Recon Loss: 8506.3012, KL Loss: 0.0001, Percep Loss: 0.0330\n",
      "Epoch 26/1000, Beta: 2.50, Total Loss: 4252.5025, Recon Loss: 8504.9266, KL Loss: 0.0001, Percep Loss: 0.0389\n",
      "Epoch 27/1000, Beta: 2.60, Total Loss: 4250.6318, Recon Loss: 8501.2065, KL Loss: 0.0000, Percep Loss: 0.0285\n",
      "Epoch 28/1000, Beta: 2.70, Total Loss: 4250.9748, Recon Loss: 8501.8944, KL Loss: 0.0001, Percep Loss: 0.0273\n",
      "Epoch 29/1000, Beta: 2.80, Total Loss: 4250.1791, Recon Loss: 8500.3053, KL Loss: 0.0000, Percep Loss: 0.0263\n",
      "Epoch 30/1000, Beta: 2.90, Total Loss: 4249.1834, Recon Loss: 8498.3129, KL Loss: 0.0001, Percep Loss: 0.0267\n",
      "Epoch 31/1000, Beta: 3.00, Total Loss: 4248.6182, Recon Loss: 8497.1843, KL Loss: 0.0000, Percep Loss: 0.0260\n",
      "Epoch 32/1000, Beta: 3.10, Total Loss: 4249.3383, Recon Loss: 8498.6200, KL Loss: 0.0000, Percep Loss: 0.0282\n",
      "Epoch 33/1000, Beta: 3.20, Total Loss: 4249.6234, Recon Loss: 8499.1819, KL Loss: 0.0001, Percep Loss: 0.0323\n",
      "Epoch 34/1000, Beta: 3.30, Total Loss: 4248.7831, Recon Loss: 8497.5187, KL Loss: 0.0000, Percep Loss: 0.0238\n",
      "Epoch 35/1000, Beta: 3.40, Total Loss: 4247.9959, Recon Loss: 8495.9434, KL Loss: 0.0000, Percep Loss: 0.0241\n",
      "Epoch 36/1000, Beta: 3.50, Total Loss: 4247.1542, Recon Loss: 8494.2644, KL Loss: 0.0000, Percep Loss: 0.0220\n",
      "Epoch 37/1000, Beta: 3.60, Total Loss: 4246.7389, Recon Loss: 8493.4334, KL Loss: 0.0000, Percep Loss: 0.0221\n",
      "Epoch 38/1000, Beta: 3.70, Total Loss: 4246.3743, Recon Loss: 8492.7048, KL Loss: 0.0000, Percep Loss: 0.0219\n",
      "Epoch 39/1000, Beta: 3.80, Total Loss: 4246.6082, Recon Loss: 8493.1672, KL Loss: 0.0000, Percep Loss: 0.0245\n",
      "Epoch 40/1000, Beta: 3.90, Total Loss: 4246.2862, Recon Loss: 8492.5254, KL Loss: 0.0000, Percep Loss: 0.0235\n",
      "Epoch 41/1000, Beta: 4.00, Total Loss: 4246.1642, Recon Loss: 8492.2904, KL Loss: 0.0000, Percep Loss: 0.0190\n",
      "Epoch 42/1000, Beta: 4.10, Total Loss: 4245.4567, Recon Loss: 8490.8706, KL Loss: 0.0000, Percep Loss: 0.0214\n",
      "Epoch 43/1000, Beta: 4.20, Total Loss: 4245.3982, Recon Loss: 8490.7561, KL Loss: 0.0000, Percep Loss: 0.0201\n",
      "Epoch 44/1000, Beta: 4.30, Total Loss: 4245.5925, Recon Loss: 8491.1505, KL Loss: 0.0000, Percep Loss: 0.0173\n",
      "Epoch 45/1000, Beta: 4.40, Total Loss: 4244.9466, Recon Loss: 8489.8558, KL Loss: 0.0000, Percep Loss: 0.0187\n",
      "Epoch 46/1000, Beta: 4.50, Total Loss: 4244.4465, Recon Loss: 8488.8585, KL Loss: 0.0000, Percep Loss: 0.0173\n",
      "Epoch 47/1000, Beta: 4.60, Total Loss: 4244.6028, Recon Loss: 8489.1713, KL Loss: 0.0000, Percep Loss: 0.0171\n",
      "Epoch 48/1000, Beta: 4.70, Total Loss: 4245.2419, Recon Loss: 8490.4507, KL Loss: 0.0001, Percep Loss: 0.0163\n",
      "Epoch 49/1000, Beta: 4.80, Total Loss: 4244.5330, Recon Loss: 8489.0337, KL Loss: 0.0000, Percep Loss: 0.0162\n",
      "Epoch 50/1000, Beta: 4.90, Total Loss: 4243.9074, Recon Loss: 8487.7820, KL Loss: 0.0000, Percep Loss: 0.0164\n",
      "Epoch 51/1000, Beta: 5.00, Total Loss: 4243.7683, Recon Loss: 8487.4997, KL Loss: 0.0000, Percep Loss: 0.0184\n",
      "Epoch 52/1000, Beta: 5.00, Total Loss: 4243.9226, Recon Loss: 8487.8088, KL Loss: 0.0002, Percep Loss: 0.0175\n",
      "Epoch 53/1000, Beta: 5.00, Total Loss: 4243.8056, Recon Loss: 8487.5796, KL Loss: 0.0000, Percep Loss: 0.0158\n",
      "Epoch 54/1000, Beta: 5.00, Total Loss: 4243.2106, Recon Loss: 8486.3915, KL Loss: 0.0000, Percep Loss: 0.0148\n",
      "Epoch 55/1000, Beta: 5.00, Total Loss: 4243.3438, Recon Loss: 8486.6593, KL Loss: 0.0000, Percep Loss: 0.0141\n",
      "Epoch 56/1000, Beta: 5.00, Total Loss: 4244.1340, Recon Loss: 8488.2379, KL Loss: 0.0000, Percep Loss: 0.0151\n",
      "Epoch 57/1000, Beta: 5.00, Total Loss: 4243.6057, Recon Loss: 8487.1841, KL Loss: 0.0000, Percep Loss: 0.0136\n",
      "Epoch 58/1000, Beta: 5.00, Total Loss: 4243.2785, Recon Loss: 8486.5269, KL Loss: 0.0000, Percep Loss: 0.0150\n",
      "Epoch 59/1000, Beta: 5.00, Total Loss: 4242.7096, Recon Loss: 8485.3905, KL Loss: 0.0000, Percep Loss: 0.0144\n",
      "Epoch 60/1000, Beta: 5.00, Total Loss: 4242.6741, Recon Loss: 8485.3212, KL Loss: 0.0000, Percep Loss: 0.0135\n",
      "Epoch 61/1000, Beta: 5.00, Total Loss: 4242.6246, Recon Loss: 8485.2197, KL Loss: 0.0000, Percep Loss: 0.0147\n",
      "Epoch 62/1000, Beta: 5.00, Total Loss: 4242.6529, Recon Loss: 8485.2783, KL Loss: 0.0000, Percep Loss: 0.0138\n",
      "Epoch 63/1000, Beta: 5.00, Total Loss: 4242.6275, Recon Loss: 8485.2292, KL Loss: 0.0000, Percep Loss: 0.0129\n",
      "Epoch 64/1000, Beta: 5.00, Total Loss: 4242.6198, Recon Loss: 8485.2041, KL Loss: 0.0000, Percep Loss: 0.0177\n",
      "Epoch 65/1000, Beta: 5.00, Total Loss: 4242.4501, Recon Loss: 8484.8726, KL Loss: 0.0000, Percep Loss: 0.0138\n",
      "Epoch 66/1000, Beta: 5.00, Total Loss: 4242.2154, Recon Loss: 8484.4084, KL Loss: 0.0000, Percep Loss: 0.0112\n",
      "Epoch 67/1000, Beta: 5.00, Total Loss: 4242.6572, Recon Loss: 8485.2922, KL Loss: 0.0000, Percep Loss: 0.0111\n",
      "Epoch 68/1000, Beta: 5.00, Total Loss: 4242.0843, Recon Loss: 8484.1425, KL Loss: 0.0000, Percep Loss: 0.0130\n",
      "Epoch 69/1000, Beta: 5.00, Total Loss: 4242.8453, Recon Loss: 8485.6638, KL Loss: 0.0000, Percep Loss: 0.0134\n",
      "Epoch 70/1000, Beta: 5.00, Total Loss: 4242.1382, Recon Loss: 8484.2506, KL Loss: 0.0000, Percep Loss: 0.0129\n",
      "Epoch 71/1000, Beta: 5.00, Total Loss: 4242.1512, Recon Loss: 8484.2702, KL Loss: 0.0000, Percep Loss: 0.0161\n",
      "Epoch 72/1000, Beta: 5.00, Total Loss: 4241.8860, Recon Loss: 8483.7467, KL Loss: 0.0000, Percep Loss: 0.0126\n",
      "Epoch 73/1000, Beta: 5.00, Total Loss: 4242.0349, Recon Loss: 8484.0365, KL Loss: 0.0000, Percep Loss: 0.0166\n",
      "Epoch 74/1000, Beta: 5.00, Total Loss: 4241.4964, Recon Loss: 8482.9694, KL Loss: 0.0000, Percep Loss: 0.0117\n",
      "Epoch 75/1000, Beta: 5.00, Total Loss: 4241.5898, Recon Loss: 8483.1523, KL Loss: 0.0000, Percep Loss: 0.0136\n",
      "Epoch 76/1000, Beta: 5.00, Total Loss: 4241.5056, Recon Loss: 8482.9874, KL Loss: 0.0000, Percep Loss: 0.0118\n",
      "Epoch 77/1000, Beta: 5.00, Total Loss: 4241.4761, Recon Loss: 8482.9294, KL Loss: 0.0000, Percep Loss: 0.0114\n",
      "Epoch 78/1000, Beta: 5.00, Total Loss: 4241.2566, Recon Loss: 8482.4934, KL Loss: 0.0000, Percep Loss: 0.0099\n",
      "Epoch 79/1000, Beta: 5.00, Total Loss: 4241.3142, Recon Loss: 8482.6084, KL Loss: 0.0000, Percep Loss: 0.0100\n",
      "Epoch 80/1000, Beta: 5.00, Total Loss: 4241.2280, Recon Loss: 8482.4356, KL Loss: 0.0000, Percep Loss: 0.0102\n",
      "Epoch 81/1000, Beta: 5.00, Total Loss: 4241.2052, Recon Loss: 8482.3893, KL Loss: 0.0000, Percep Loss: 0.0105\n",
      "Epoch 82/1000, Beta: 5.00, Total Loss: 4241.2592, Recon Loss: 8482.4932, KL Loss: 0.0000, Percep Loss: 0.0126\n",
      "Epoch 83/1000, Beta: 5.00, Total Loss: 4241.5544, Recon Loss: 8483.0817, KL Loss: 0.0000, Percep Loss: 0.0136\n",
      "Epoch 84/1000, Beta: 5.00, Total Loss: 4241.1921, Recon Loss: 8482.3629, KL Loss: 0.0000, Percep Loss: 0.0106\n",
      "Epoch 85/1000, Beta: 5.00, Total Loss: 4240.9208, Recon Loss: 8481.8254, KL Loss: 0.0000, Percep Loss: 0.0081\n",
      "Epoch 86/1000, Beta: 5.00, Total Loss: 4240.8523, Recon Loss: 8481.6873, KL Loss: 0.0000, Percep Loss: 0.0087\n",
      "Epoch 87/1000, Beta: 5.00, Total Loss: 4240.8858, Recon Loss: 8481.7520, KL Loss: 0.0000, Percep Loss: 0.0098\n",
      "Epoch 88/1000, Beta: 5.00, Total Loss: 4240.8104, Recon Loss: 8481.6031, KL Loss: 0.0000, Percep Loss: 0.0088\n",
      "Epoch 89/1000, Beta: 5.00, Total Loss: 4240.7051, Recon Loss: 8481.3937, KL Loss: 0.0000, Percep Loss: 0.0082\n",
      "Epoch 90/1000, Beta: 5.00, Total Loss: 4241.2421, Recon Loss: 8482.4638, KL Loss: 0.0000, Percep Loss: 0.0102\n",
      "Epoch 91/1000, Beta: 5.00, Total Loss: 4241.1311, Recon Loss: 8482.2459, KL Loss: 0.0000, Percep Loss: 0.0081\n",
      "Epoch 92/1000, Beta: 5.00, Total Loss: 4240.8120, Recon Loss: 8481.6069, KL Loss: 0.0000, Percep Loss: 0.0086\n",
      "Epoch 93/1000, Beta: 5.00, Total Loss: 4240.5308, Recon Loss: 8481.0470, KL Loss: 0.0000, Percep Loss: 0.0074\n",
      "Epoch 94/1000, Beta: 5.00, Total Loss: 4240.7073, Recon Loss: 8481.3972, KL Loss: 0.0000, Percep Loss: 0.0086\n",
      "Epoch 95/1000, Beta: 5.00, Total Loss: 4240.5236, Recon Loss: 8481.0304, KL Loss: 0.0000, Percep Loss: 0.0084\n",
      "Epoch 96/1000, Beta: 5.00, Total Loss: 4240.4172, Recon Loss: 8480.8197, KL Loss: 0.0000, Percep Loss: 0.0073\n",
      "Epoch 97/1000, Beta: 5.00, Total Loss: 4240.4874, Recon Loss: 8480.9577, KL Loss: 0.0000, Percep Loss: 0.0085\n",
      "Epoch 98/1000, Beta: 5.00, Total Loss: 4240.5424, Recon Loss: 8481.0666, KL Loss: 0.0000, Percep Loss: 0.0091\n",
      "Epoch 99/1000, Beta: 5.00, Total Loss: 4240.5075, Recon Loss: 8480.9989, KL Loss: 0.0000, Percep Loss: 0.0080\n",
      "Epoch 100/1000, Beta: 5.00, Total Loss: 4240.4529, Recon Loss: 8480.8915, KL Loss: 0.0000, Percep Loss: 0.0072\n",
      "Saved checkpoint at epoch 100 to FL_CVAE\\cvae_vehicle_epoch_100.pth\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 325\u001b[0m\n\u001b[0;32m    323\u001b[0m             sample \u001b[38;5;241m=\u001b[39m adjust_sharpness(sample, sharpness_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m)\n\u001b[0;32m    324\u001b[0m             save_image(sample, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(class_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m--> 325\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples for Class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39mclass_names[class_label]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) at epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    327\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;28mlen\u001b[39m(classes_to_generate), \u001b[38;5;241m10\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row, class_label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(classes_to_generate):\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms.functional import adjust_sharpness\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import kagglehub\n",
    "from PIL import Image\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Set CUDA_LAUNCH_BLOCKING for debugging\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.device_count() > 1 else 'cuda')\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 128\n",
    "num_classes = 5\n",
    "batch_size = 16\n",
    "epochs = 1000\n",
    "learning_rate = 5e-4\n",
    "image_size = 128\n",
    "channels = 1\n",
    "output_dir = \"FL_CVAE\"\n",
    "beta_max = 5.0\n",
    "annealing_epochs = 50\n",
    "perceptual_weight = 1.0\n",
    "recon_weight = 0.5\n",
    "\n",
    "# Download Vehicle Type Image Dataset from Kaggle\n",
    "try:\n",
    "    path = kagglehub.dataset_download(\"sujaykapadnis/vehicle-type-image-dataset\")\n",
    "    print(\"Path to dataset files:\", path)\n",
    "    dataset_path = path\n",
    "except Exception as e:\n",
    "    print(f\"Failed to download dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Define the VehicleTypeDataset class with a limit\n",
    "class VehicleTypeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None, max_images=1000):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_names = []\n",
    "        self.class_to_idx = {}\n",
    "\n",
    "        print(f\"Searching for images in {root_dir}\")\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            image_files = [f for f in files if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "            if image_files:\n",
    "                class_name = os.path.basename(root)\n",
    "                if class_name not in self.class_to_idx:\n",
    "                    self.class_names.append(class_name)\n",
    "                    self.class_to_idx[class_name] = len(self.class_names) - 1\n",
    "                for img_file in image_files:\n",
    "                    img_path = os.path.join(root, img_file)\n",
    "                    self.images.append(img_path)\n",
    "                    self.labels.append(self.class_to_idx[class_name])\n",
    "                    if len(self.images) >= max_images:\n",
    "                        break\n",
    "                if len(self.images) >= max_images:\n",
    "                    break\n",
    "\n",
    "        if not self.images:\n",
    "            raise ValueError(\n",
    "                f\"No images found in {root_dir}. \"\n",
    "                \"Expected class folders containing .jpg, .png, or .jpeg images.\"\n",
    "            )\n",
    "\n",
    "        print(f\"Found {len(self.images)} images across {len(self.class_names)} classes.\")\n",
    "        print(f\"Classes: {self.class_names}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"L\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the dataset with a limit\n",
    "dataset = VehicleTypeDataset(root_dir=dataset_path, transform=transform, max_images=1000)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the Encoder network\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels + num_classes, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.fc_mean = nn.Linear(512 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(512 * 4 * 4, latent_dim)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        y = F.one_hot(y, num_classes=num_classes).float()\n",
    "        y = y.unsqueeze(-1).unsqueeze(-1)\n",
    "        y = y.expand(-1, -1, x.size(2), x.size(3))\n",
    "        x_with_y = torch.cat([x, y], dim=1)\n",
    "        \n",
    "        h1 = F.relu(self.conv1(x_with_y))\n",
    "        if torch.isnan(h1).any() or torch.isinf(h1).any():\n",
    "            print(\"NaN or Inf in h1\")\n",
    "        h2 = F.relu(self.conv2(h1))\n",
    "        if torch.isnan(h2).any() or torch.isinf(h2).any():\n",
    "            print(\"NaN or Inf in h2\")\n",
    "        h3 = F.relu(self.conv3(h2))\n",
    "        if torch.isnan(h3).any() or torch.isinf(h3).any():\n",
    "            print(\"NaN or Inf in h3\")\n",
    "        h4 = F.relu(self.conv4(h3))\n",
    "        if torch.isnan(h4).any() or torch.isinf(h4).any():\n",
    "            print(\"NaN or Inf in h4\")\n",
    "        h5 = F.relu(self.conv5(h4))\n",
    "        if torch.isnan(h5).any() or torch.isinf(h5).any():\n",
    "            print(\"NaN or Inf in h5\")\n",
    "        h = h5.view(h5.size(0), -1)\n",
    "        z_mean = self.fc_mean(h)\n",
    "        z_logvar = self.fc_logvar(h)\n",
    "        return z_mean, z_logvar, (h1, h2, h3, h4, h5)\n",
    "\n",
    "# Define the Decoder network\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim + num_classes, 512 * 4 * 4)\n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(512, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(256, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(128, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv5 = nn.ConvTranspose2d(64, channels, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, z, y, skip_connections):\n",
    "        h1, h2, h3, h4, h5 = skip_connections\n",
    "        y = F.one_hot(y, num_classes=num_classes).float()\n",
    "        z_with_y = torch.cat([z, y], dim=-1)\n",
    "        h = F.relu(self.fc(z_with_y))\n",
    "        h = h.view(h.size(0), 512, 4, 4)\n",
    "        \n",
    "        h = F.relu(self.deconv1(h))\n",
    "        h = torch.cat([h, h4], dim=1)\n",
    "        h = F.relu(self.deconv2(h))\n",
    "        h = torch.cat([h, h3], dim=1)\n",
    "        h = F.relu(self.deconv3(h))\n",
    "        h = torch.cat([h, h2], dim=1)\n",
    "        h = F.relu(self.deconv4(h))\n",
    "        h = torch.cat([h, h1], dim=1)\n",
    "        x_reconstructed = torch.sigmoid(self.deconv5(h))\n",
    "        if torch.isnan(x_reconstructed).any() or torch.isinf(x_reconstructed).any():\n",
    "            print(\"NaN or Inf in x_reconstructed\")\n",
    "        return x_reconstructed\n",
    "\n",
    "# Convert grayscale to RGB by duplicating channels\n",
    "def grayscale_to_rgb(tensor):\n",
    "    return tensor.repeat(1, 3, 1, 1)\n",
    "\n",
    "# Conditional VAE model\n",
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def reparameterize(self, z_mean, z_logvar):\n",
    "        std = torch.exp(0.5 * z_logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return z_mean + eps * std\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        z_mean, z_logvar, skip_connections = self.encoder(x, y)\n",
    "        z = self.reparameterize(z_mean, z_logvar)\n",
    "        x_reconstructed = self.decoder(z, y, skip_connections)\n",
    "        return x_reconstructed, z_mean, z_logvar\n",
    "\n",
    "# Load pretrained VGG16 for perceptual loss\n",
    "vgg = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features.to(device).eval()\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "def perceptual_loss(x, x_reconstructed):\n",
    "    x_rgb = grayscale_to_rgb(x)\n",
    "    x_reconstructed_rgb = grayscale_to_rgb(x_reconstructed)\n",
    "    x_rgb_subset = x_rgb[:8]\n",
    "    x_reconstructed_rgb_subset = x_reconstructed_rgb[:8]\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(x.device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(x.device)\n",
    "    x_normalized = (x_rgb_subset - mean) / std\n",
    "    x_reconstructed_normalized = (x_reconstructed_rgb_subset - mean) / std\n",
    "    x_features = vgg(x_normalized)\n",
    "    x_recon_features = vgg(x_reconstructed_normalized)\n",
    "    return F.mse_loss(x_features, x_recon_features)\n",
    "\n",
    "# Instantiate Encoder, Decoder, and CVAE\n",
    "encoder = Encoder(latent_dim, num_classes).to(device)\n",
    "decoder = Decoder(latent_dim, num_classes).to(device)\n",
    "cvae = ConditionalVAE(encoder, decoder).to(device)\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "optimizer = optim.Adam(cvae.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.5)\n",
    "\n",
    "# Define loss function with beta annealing\n",
    "def cvae_loss(x, x_reconstructed, z_mean, z_logvar, beta=1.0, recon_weight=1.0, perceptual_weight=1.0):\n",
    "    if torch.isnan(x).any() or torch.isinf(x).any():\n",
    "        print(\"NaN or Inf detected in x\")\n",
    "    if torch.isnan(x_reconstructed).any() or torch.isinf(x_reconstructed).any():\n",
    "        print(\"NaN or Inf detected in x_reconstructed\")\n",
    "    if torch.isnan(z_mean).any() or torch.isinf(z_mean).any():\n",
    "        print(\"NaN or Inf detected in z_mean\")\n",
    "    if torch.isnan(z_logvar).any() or torch.isinf(z_logvar).any():\n",
    "        print(\"NaN or Inf detected in z_logvar\")\n",
    "\n",
    "    recon_loss = F.binary_cross_entropy(x_reconstructed, x, reduction='sum')\n",
    "    kl_loss = -0.5 * torch.sum(1 + torch.clamp(z_logvar, -5, 5) - z_mean.pow(2) - torch.clamp(z_logvar, -5, 5).exp())\n",
    "    percep_loss = perceptual_loss(x, x_reconstructed) * perceptual_weight\n",
    "    total_loss = recon_weight * recon_loss + beta * kl_loss + percep_loss\n",
    "    return total_loss, recon_loss, kl_loss, percep_loss\n",
    "\n",
    "# Training loop with checkpointing\n",
    "cvae.train()\n",
    "for epoch in range(epochs):\n",
    "    if epoch < annealing_epochs:\n",
    "        beta = beta_max * (epoch / annealing_epochs)\n",
    "    else:\n",
    "        beta = beta_max\n",
    "\n",
    "    train_loss = 0\n",
    "    train_recon_loss = 0\n",
    "    train_kl_loss = 0\n",
    "    train_percep_loss = 0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "       \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        x_reconstructed, z_mean, z_logvar = cvae(data, labels)\n",
    "        total_loss, recon_loss, kl_loss, percep_loss = cvae_loss(\n",
    "            data, x_reconstructed, z_mean, z_logvar, \n",
    "            beta=beta, recon_weight=recon_weight, perceptual_weight=perceptual_weight\n",
    "        )\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(cvae.parameters(), max_norm=0.5)\n",
    "        train_loss += total_loss.item()\n",
    "        train_recon_loss += recon_loss.item()\n",
    "        train_kl_loss += kl_loss.item()\n",
    "        train_percep_loss += percep_loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    avg_recon_loss = train_recon_loss / len(train_loader.dataset)\n",
    "    avg_kl_loss = train_kl_loss / len(train_loader.dataset)\n",
    "    avg_percep_loss = train_percep_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Beta: {beta:.2f}, Total Loss: {avg_loss:.4f}, '\n",
    "          f'Recon Loss: {avg_recon_loss:.4f}, KL Loss: {avg_kl_loss:.4f}, '\n",
    "          f'Percep Loss: {avg_percep_loss:.4f}')\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        checkpoint_path = os.path.join(output_dir, f\"cvae_vehicle_epoch_{epoch + 1}.pth\")\n",
    "        torch.save(cvae.state_dict(), checkpoint_path)\n",
    "        print(f\"Saved checkpoint at epoch {epoch + 1} to {checkpoint_path}\")\n",
    "\n",
    "        cvae.eval()\n",
    "        base_dir = os.path.join(output_dir, f\"generated_samples_epoch_{epoch + 1}\")\n",
    "        classes_to_generate = [3, 4]\n",
    "        num_samples = 100\n",
    "        with torch.no_grad():\n",
    "            for class_label in classes_to_generate:\n",
    "                label_tensor = torch.tensor([class_label]).repeat(num_samples).to(device)\n",
    "                z = torch.randn(num_samples, latent_dim).to(device)\n",
    "                dummy_skips = [\n",
    "                    torch.randn(num_samples, 32, 64, 64).to(device) * 0.1,\n",
    "                    torch.randn(num_samples, 64, 32, 32).to(device) * 0.1,\n",
    "                    torch.randn(num_samples, 128, 16, 16).to(device) * 0.1,\n",
    "                    torch.randn(num_samples, 256, 8, 8).to(device) * 0.1,\n",
    "                    torch.randn(num_samples, 512, 4, 4).to(device) * 0.1\n",
    "                ]\n",
    "                generated_samples = cvae.decoder(z, label_tensor, dummy_skips)\n",
    "                generated_samples_rgb = grayscale_to_rgb(generated_samples)\n",
    "                class_dir = os.path.join(base_dir, str(class_label))\n",
    "                os.makedirs(class_dir, exist_ok=True)\n",
    "                for idx, sample in enumerate(generated_samples_rgb):\n",
    "                    sample = adjust_sharpness(sample, sharpness_factor=2.0)\n",
    "                    save_image(sample, os.path.join(class_dir, f\"sample_{idx}.png\"))\n",
    "                print(f\"Generated {num_samples} samples for Class {class_label} ({dataset.class_names[class_label]}) at epoch {epoch + 1}.\")\n",
    "\n",
    "        fig, axs = plt.subplots(len(classes_to_generate), 10, figsize=(20, 4))\n",
    "        for row, class_label in enumerate(classes_to_generate):\n",
    "            class_dir = os.path.join(base_dir, str(class_label))\n",
    "            sample_files = os.listdir(class_dir)\n",
    "            random_samples = np.random.choice(sample_files, 10, replace=False)\n",
    "            for col, sample_file in enumerate(random_samples):\n",
    "                sample_path = os.path.join(class_dir, sample_file)\n",
    "                sample_image = Image.open(sample_path).convert(\"RGB\")\n",
    "                sample_image = sample_image.resize((128, 128), Image.LANCZOS)\n",
    "                sample_image = np.array(sample_image) / 255.0\n",
    "                ax = axs[row, col] if len(classes_to_generate) > 1 else axs[col]\n",
    "                ax.imshow(sample_image)\n",
    "                ax.axis('off')\n",
    "                if col == 0:\n",
    "                    ax.set_ylabel(dataset.class_names[class_label], rotation=90, labelpad=10)\n",
    "        plt.tight_layout()\n",
    "        plot_path = os.path.join(output_dir, f\"synthetic_samples_classes_3_4_epoch_{epoch + 1}.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        print(f\"Saved synthetic samples plot at epoch {epoch + 1} to {plot_path}\")\n",
    "        cvae.train()\n",
    "\n",
    "# Final model save\n",
    "final_model_path = os.path.join(output_dir, \"cvae_vehicle_final.pth\")\n",
    "torch.save(cvae.state_dict(), final_model_path)\n",
    "print(f\"Saved final CVAE model to {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2286eee7-fa34-4cdd-8424-87cf26f1c467",
   "metadata": {},
   "outputs": [],
   "source": [
    "###SAVES AT LAST EPOCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09c5756c-5d19-4272-8afe-7d58e4f3b1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Searching for images in C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Found 1000 images across 2 classes.\n",
      "Classes: ['Hatchback', 'Other']\n",
      "Epoch 1/1000, Beta: 0.00, Total Loss: 6630.5874, Recon Loss: 13260.9483, KL Loss: 2531.6387, Percep Loss: 0.1133\n",
      "Epoch 2/1000, Beta: 0.10, Total Loss: 4385.7789, Recon Loss: 8740.7821, KL Loss: 152.8180, Percep Loss: 0.1060\n",
      "Epoch 3/1000, Beta: 0.20, Total Loss: 4329.0559, Recon Loss: 8657.9318, KL Loss: 0.0045, Percep Loss: 0.0891\n",
      "Epoch 4/1000, Beta: 0.30, Total Loss: 4315.0779, Recon Loss: 8629.9949, KL Loss: 0.0011, Percep Loss: 0.0802\n",
      "Epoch 5/1000, Beta: 0.40, Total Loss: 4299.5715, Recon Loss: 8598.9928, KL Loss: 0.0006, Percep Loss: 0.0749\n",
      "Epoch 6/1000, Beta: 0.50, Total Loss: 4285.4196, Recon Loss: 8570.7064, KL Loss: 0.0004, Percep Loss: 0.0662\n",
      "Epoch 7/1000, Beta: 0.60, Total Loss: 4277.3297, Recon Loss: 8554.5404, KL Loss: 0.0005, Percep Loss: 0.0592\n",
      "Epoch 8/1000, Beta: 0.70, Total Loss: 4273.8019, Recon Loss: 8547.4964, KL Loss: 0.0001, Percep Loss: 0.0536\n",
      "Epoch 9/1000, Beta: 0.80, Total Loss: 4269.4506, Recon Loss: 8538.7865, KL Loss: 0.0001, Percep Loss: 0.0573\n",
      "Epoch 10/1000, Beta: 0.90, Total Loss: 4267.2960, Recon Loss: 8534.4945, KL Loss: 0.0001, Percep Loss: 0.0487\n",
      "Epoch 11/1000, Beta: 1.00, Total Loss: 4264.3071, Recon Loss: 8528.5228, KL Loss: 0.0000, Percep Loss: 0.0457\n",
      "Epoch 12/1000, Beta: 1.10, Total Loss: 4262.5921, Recon Loss: 8525.0964, KL Loss: 0.0001, Percep Loss: 0.0438\n",
      "Epoch 13/1000, Beta: 1.20, Total Loss: 4262.1745, Recon Loss: 8524.2651, KL Loss: 0.0000, Percep Loss: 0.0420\n",
      "Epoch 14/1000, Beta: 1.30, Total Loss: 4262.4416, Recon Loss: 8524.7806, KL Loss: 0.0001, Percep Loss: 0.0512\n",
      "Epoch 15/1000, Beta: 1.40, Total Loss: 4259.3700, Recon Loss: 8518.6639, KL Loss: 0.0000, Percep Loss: 0.0380\n",
      "Epoch 16/1000, Beta: 1.50, Total Loss: 4258.2298, Recon Loss: 8516.3826, KL Loss: 0.0000, Percep Loss: 0.0384\n",
      "Epoch 17/1000, Beta: 1.60, Total Loss: 4257.1736, Recon Loss: 8514.2686, KL Loss: 0.0001, Percep Loss: 0.0391\n",
      "Epoch 18/1000, Beta: 1.70, Total Loss: 4255.8284, Recon Loss: 8511.5817, KL Loss: 0.0001, Percep Loss: 0.0374\n",
      "Epoch 19/1000, Beta: 1.80, Total Loss: 4254.2532, Recon Loss: 8508.4385, KL Loss: 0.0000, Percep Loss: 0.0339\n",
      "Epoch 20/1000, Beta: 1.90, Total Loss: 4252.7263, Recon Loss: 8505.3885, KL Loss: 0.0001, Percep Loss: 0.0320\n",
      "Epoch 21/1000, Beta: 2.00, Total Loss: 4252.1009, Recon Loss: 8504.1399, KL Loss: 0.0000, Percep Loss: 0.0309\n",
      "Epoch 22/1000, Beta: 2.10, Total Loss: 4252.8615, Recon Loss: 8505.6638, KL Loss: 0.0001, Percep Loss: 0.0295\n",
      "Epoch 23/1000, Beta: 2.20, Total Loss: 4252.5319, Recon Loss: 8505.0025, KL Loss: 0.0000, Percep Loss: 0.0306\n",
      "Epoch 24/1000, Beta: 2.30, Total Loss: 4251.0954, Recon Loss: 8502.1226, KL Loss: 0.0000, Percep Loss: 0.0340\n",
      "Epoch 25/1000, Beta: 2.40, Total Loss: 4251.6421, Recon Loss: 8503.2292, KL Loss: 0.0000, Percep Loss: 0.0274\n",
      "Epoch 26/1000, Beta: 2.50, Total Loss: 4250.7552, Recon Loss: 8501.4586, KL Loss: 0.0001, Percep Loss: 0.0258\n",
      "Epoch 27/1000, Beta: 2.60, Total Loss: 4251.7483, Recon Loss: 8503.4473, KL Loss: 0.0000, Percep Loss: 0.0246\n",
      "Epoch 28/1000, Beta: 2.70, Total Loss: 4250.0791, Recon Loss: 8500.0934, KL Loss: 0.0001, Percep Loss: 0.0322\n",
      "Epoch 29/1000, Beta: 2.80, Total Loss: 4249.5764, Recon Loss: 8499.0979, KL Loss: 0.0000, Percep Loss: 0.0273\n",
      "Epoch 30/1000, Beta: 2.90, Total Loss: 4249.1922, Recon Loss: 8498.3253, KL Loss: 0.0000, Percep Loss: 0.0295\n",
      "Epoch 31/1000, Beta: 3.00, Total Loss: 4247.7519, Recon Loss: 8495.4561, KL Loss: 0.0000, Percep Loss: 0.0238\n",
      "Epoch 32/1000, Beta: 3.10, Total Loss: 4247.6885, Recon Loss: 8495.3333, KL Loss: 0.0000, Percep Loss: 0.0218\n",
      "Epoch 33/1000, Beta: 3.20, Total Loss: 4247.3504, Recon Loss: 8494.6600, KL Loss: 0.0000, Percep Loss: 0.0204\n",
      "Epoch 34/1000, Beta: 3.30, Total Loss: 4247.2565, Recon Loss: 8494.4733, KL Loss: 0.0000, Percep Loss: 0.0198\n",
      "Epoch 35/1000, Beta: 3.40, Total Loss: 4248.4229, Recon Loss: 8496.8061, KL Loss: 0.0000, Percep Loss: 0.0199\n",
      "Epoch 36/1000, Beta: 3.50, Total Loss: 4246.6281, Recon Loss: 8493.2164, KL Loss: 0.0000, Percep Loss: 0.0198\n",
      "Epoch 37/1000, Beta: 3.60, Total Loss: 4245.6132, Recon Loss: 8491.1886, KL Loss: 0.0000, Percep Loss: 0.0188\n",
      "Epoch 38/1000, Beta: 3.70, Total Loss: 4245.5340, Recon Loss: 8491.0302, KL Loss: 0.0000, Percep Loss: 0.0188\n",
      "Epoch 39/1000, Beta: 3.80, Total Loss: 4245.7790, Recon Loss: 8491.5218, KL Loss: 0.0001, Percep Loss: 0.0177\n",
      "Epoch 40/1000, Beta: 3.90, Total Loss: 4245.9609, Recon Loss: 8491.8882, KL Loss: 0.0000, Percep Loss: 0.0167\n",
      "Epoch 41/1000, Beta: 4.00, Total Loss: 4244.7722, Recon Loss: 8489.5096, KL Loss: 0.0000, Percep Loss: 0.0174\n",
      "Epoch 42/1000, Beta: 4.10, Total Loss: 4244.8324, Recon Loss: 8489.6265, KL Loss: 0.0000, Percep Loss: 0.0191\n",
      "Epoch 43/1000, Beta: 4.20, Total Loss: 4244.9674, Recon Loss: 8489.9018, KL Loss: 0.0001, Percep Loss: 0.0163\n",
      "Epoch 44/1000, Beta: 4.30, Total Loss: 4245.4505, Recon Loss: 8490.8677, KL Loss: 0.0000, Percep Loss: 0.0166\n",
      "Epoch 45/1000, Beta: 4.40, Total Loss: 4244.2203, Recon Loss: 8488.4048, KL Loss: 0.0000, Percep Loss: 0.0178\n",
      "Epoch 46/1000, Beta: 4.50, Total Loss: 4244.8806, Recon Loss: 8489.7258, KL Loss: 0.0000, Percep Loss: 0.0176\n",
      "Epoch 47/1000, Beta: 4.60, Total Loss: 4245.2557, Recon Loss: 8490.4817, KL Loss: 0.0000, Percep Loss: 0.0148\n",
      "Epoch 48/1000, Beta: 4.70, Total Loss: 4244.0598, Recon Loss: 8488.0890, KL Loss: 0.0003, Percep Loss: 0.0141\n",
      "Epoch 49/1000, Beta: 4.80, Total Loss: 4244.3669, Recon Loss: 8488.7020, KL Loss: 0.0000, Percep Loss: 0.0159\n",
      "Epoch 50/1000, Beta: 4.90, Total Loss: 4243.5017, Recon Loss: 8486.9720, KL Loss: 0.0000, Percep Loss: 0.0157\n",
      "Epoch 51/1000, Beta: 5.00, Total Loss: 4243.2904, Recon Loss: 8486.5532, KL Loss: 0.0000, Percep Loss: 0.0138\n",
      "Epoch 52/1000, Beta: 5.00, Total Loss: 4243.3778, Recon Loss: 8486.7214, KL Loss: 0.0000, Percep Loss: 0.0170\n",
      "Epoch 53/1000, Beta: 5.00, Total Loss: 4243.4027, Recon Loss: 8486.7793, KL Loss: 0.0000, Percep Loss: 0.0130\n",
      "Epoch 54/1000, Beta: 5.00, Total Loss: 4242.8000, Recon Loss: 8485.5714, KL Loss: 0.0000, Percep Loss: 0.0143\n",
      "Epoch 55/1000, Beta: 5.00, Total Loss: 4242.6886, Recon Loss: 8485.3444, KL Loss: 0.0000, Percep Loss: 0.0163\n",
      "Epoch 56/1000, Beta: 5.00, Total Loss: 4242.6438, Recon Loss: 8485.2607, KL Loss: 0.0000, Percep Loss: 0.0134\n",
      "Epoch 57/1000, Beta: 5.00, Total Loss: 4242.6284, Recon Loss: 8485.2310, KL Loss: 0.0000, Percep Loss: 0.0128\n",
      "Epoch 58/1000, Beta: 5.00, Total Loss: 4243.0856, Recon Loss: 8486.1486, KL Loss: 0.0000, Percep Loss: 0.0113\n",
      "Epoch 59/1000, Beta: 5.00, Total Loss: 4242.2647, Recon Loss: 8484.5047, KL Loss: 0.0000, Percep Loss: 0.0124\n",
      "Epoch 60/1000, Beta: 5.00, Total Loss: 4242.7305, Recon Loss: 8485.4391, KL Loss: 0.0000, Percep Loss: 0.0109\n",
      "Epoch 61/1000, Beta: 5.00, Total Loss: 4242.0437, Recon Loss: 8484.0656, KL Loss: 0.0000, Percep Loss: 0.0109\n",
      "Epoch 62/1000, Beta: 5.00, Total Loss: 4242.2563, Recon Loss: 8484.4890, KL Loss: 0.0001, Percep Loss: 0.0116\n",
      "Epoch 63/1000, Beta: 5.00, Total Loss: 4241.9837, Recon Loss: 8483.9463, KL Loss: 0.0000, Percep Loss: 0.0105\n",
      "Epoch 64/1000, Beta: 5.00, Total Loss: 4242.6105, Recon Loss: 8485.1987, KL Loss: 0.0000, Percep Loss: 0.0110\n",
      "Epoch 65/1000, Beta: 5.00, Total Loss: 4241.9610, Recon Loss: 8483.9006, KL Loss: 0.0000, Percep Loss: 0.0107\n",
      "Epoch 66/1000, Beta: 5.00, Total Loss: 4241.8437, Recon Loss: 8483.6636, KL Loss: 0.0000, Percep Loss: 0.0119\n",
      "Epoch 67/1000, Beta: 5.00, Total Loss: 4242.0061, Recon Loss: 8483.9849, KL Loss: 0.0000, Percep Loss: 0.0135\n",
      "Epoch 68/1000, Beta: 5.00, Total Loss: 4241.5696, Recon Loss: 8483.1191, KL Loss: 0.0000, Percep Loss: 0.0100\n",
      "Epoch 69/1000, Beta: 5.00, Total Loss: 4241.4095, Recon Loss: 8482.8015, KL Loss: 0.0000, Percep Loss: 0.0087\n",
      "Epoch 70/1000, Beta: 5.00, Total Loss: 4241.7607, Recon Loss: 8483.5002, KL Loss: 0.0000, Percep Loss: 0.0106\n",
      "Epoch 71/1000, Beta: 5.00, Total Loss: 4241.9424, Recon Loss: 8483.8670, KL Loss: 0.0000, Percep Loss: 0.0090\n",
      "Epoch 72/1000, Beta: 5.00, Total Loss: 4241.5864, Recon Loss: 8483.1515, KL Loss: 0.0000, Percep Loss: 0.0106\n",
      "Epoch 73/1000, Beta: 5.00, Total Loss: 4241.5197, Recon Loss: 8483.0174, KL Loss: 0.0001, Percep Loss: 0.0107\n",
      "Epoch 74/1000, Beta: 5.00, Total Loss: 4241.2709, Recon Loss: 8482.5204, KL Loss: 0.0000, Percep Loss: 0.0107\n",
      "Epoch 75/1000, Beta: 5.00, Total Loss: 4241.2369, Recon Loss: 8482.4535, KL Loss: 0.0000, Percep Loss: 0.0101\n",
      "Epoch 76/1000, Beta: 5.00, Total Loss: 4241.4191, Recon Loss: 8482.8178, KL Loss: 0.0000, Percep Loss: 0.0103\n",
      "Epoch 77/1000, Beta: 5.00, Total Loss: 4241.5860, Recon Loss: 8483.1560, KL Loss: 0.0000, Percep Loss: 0.0080\n",
      "Epoch 78/1000, Beta: 5.00, Total Loss: 4241.6232, Recon Loss: 8483.2304, KL Loss: 0.0000, Percep Loss: 0.0080\n",
      "Epoch 79/1000, Beta: 5.00, Total Loss: 4241.4041, Recon Loss: 8482.7922, KL Loss: 0.0000, Percep Loss: 0.0080\n",
      "Epoch 80/1000, Beta: 5.00, Total Loss: 4240.8965, Recon Loss: 8481.7767, KL Loss: 0.0000, Percep Loss: 0.0081\n",
      "Epoch 81/1000, Beta: 5.00, Total Loss: 4240.9429, Recon Loss: 8481.8677, KL Loss: 0.0000, Percep Loss: 0.0091\n",
      "Epoch 82/1000, Beta: 5.00, Total Loss: 4240.9897, Recon Loss: 8481.9598, KL Loss: 0.0000, Percep Loss: 0.0098\n",
      "Epoch 83/1000, Beta: 5.00, Total Loss: 4240.8061, Recon Loss: 8481.5955, KL Loss: 0.0000, Percep Loss: 0.0083\n",
      "Epoch 84/1000, Beta: 5.00, Total Loss: 4240.7332, Recon Loss: 8481.4479, KL Loss: 0.0000, Percep Loss: 0.0092\n",
      "Epoch 85/1000, Beta: 5.00, Total Loss: 4241.3767, Recon Loss: 8482.7320, KL Loss: 0.0000, Percep Loss: 0.0106\n",
      "Epoch 86/1000, Beta: 5.00, Total Loss: 4240.8039, Recon Loss: 8481.5914, KL Loss: 0.0000, Percep Loss: 0.0082\n",
      "Epoch 87/1000, Beta: 5.00, Total Loss: 4240.6447, Recon Loss: 8481.2750, KL Loss: 0.0000, Percep Loss: 0.0072\n",
      "Epoch 88/1000, Beta: 5.00, Total Loss: 4240.9972, Recon Loss: 8481.9739, KL Loss: 0.0000, Percep Loss: 0.0103\n",
      "Epoch 89/1000, Beta: 5.00, Total Loss: 4240.7958, Recon Loss: 8481.5708, KL Loss: 0.0000, Percep Loss: 0.0104\n",
      "Epoch 90/1000, Beta: 5.00, Total Loss: 4240.5566, Recon Loss: 8481.0982, KL Loss: 0.0000, Percep Loss: 0.0075\n",
      "Epoch 91/1000, Beta: 5.00, Total Loss: 4240.4053, Recon Loss: 8480.7975, KL Loss: 0.0000, Percep Loss: 0.0066\n",
      "Epoch 92/1000, Beta: 5.00, Total Loss: 4240.4446, Recon Loss: 8480.8757, KL Loss: 0.0000, Percep Loss: 0.0067\n",
      "Epoch 93/1000, Beta: 5.00, Total Loss: 4240.6750, Recon Loss: 8481.3295, KL Loss: 0.0005, Percep Loss: 0.0077\n",
      "Epoch 94/1000, Beta: 5.00, Total Loss: 4240.3031, Recon Loss: 8480.5922, KL Loss: 0.0000, Percep Loss: 0.0070\n",
      "Epoch 95/1000, Beta: 5.00, Total Loss: 4240.6318, Recon Loss: 8481.2461, KL Loss: 0.0000, Percep Loss: 0.0087\n",
      "Epoch 96/1000, Beta: 5.00, Total Loss: 4240.2128, Recon Loss: 8480.4135, KL Loss: 0.0000, Percep Loss: 0.0061\n",
      "Epoch 97/1000, Beta: 5.00, Total Loss: 4240.5573, Recon Loss: 8481.0940, KL Loss: 0.0000, Percep Loss: 0.0103\n",
      "Epoch 98/1000, Beta: 5.00, Total Loss: 4240.2168, Recon Loss: 8480.4201, KL Loss: 0.0000, Percep Loss: 0.0068\n",
      "Epoch 99/1000, Beta: 5.00, Total Loss: 4240.4120, Recon Loss: 8480.8102, KL Loss: 0.0000, Percep Loss: 0.0069\n",
      "Epoch 100/1000, Beta: 5.00, Total Loss: 4240.6317, Recon Loss: 8481.2503, KL Loss: 0.0000, Percep Loss: 0.0065\n",
      "Epoch 101/1000, Beta: 5.00, Total Loss: 4240.5799, Recon Loss: 8481.1414, KL Loss: 0.0000, Percep Loss: 0.0092\n",
      "Epoch 102/1000, Beta: 5.00, Total Loss: 4240.2237, Recon Loss: 8480.4332, KL Loss: 0.0000, Percep Loss: 0.0071\n",
      "Epoch 103/1000, Beta: 5.00, Total Loss: 4240.2573, Recon Loss: 8480.4988, KL Loss: 0.0000, Percep Loss: 0.0079\n",
      "Epoch 104/1000, Beta: 5.00, Total Loss: 4240.1565, Recon Loss: 8480.2995, KL Loss: 0.0000, Percep Loss: 0.0068\n",
      "Epoch 105/1000, Beta: 5.00, Total Loss: 4240.0254, Recon Loss: 8480.0387, KL Loss: 0.0000, Percep Loss: 0.0061\n",
      "Epoch 106/1000, Beta: 5.00, Total Loss: 4240.1277, Recon Loss: 8480.2414, KL Loss: 0.0000, Percep Loss: 0.0069\n",
      "Epoch 107/1000, Beta: 5.00, Total Loss: 4239.9230, Recon Loss: 8479.8337, KL Loss: 0.0000, Percep Loss: 0.0062\n",
      "Epoch 108/1000, Beta: 5.00, Total Loss: 4240.0010, Recon Loss: 8479.9888, KL Loss: 0.0000, Percep Loss: 0.0066\n",
      "Epoch 109/1000, Beta: 5.00, Total Loss: 4240.2008, Recon Loss: 8480.3897, KL Loss: 0.0000, Percep Loss: 0.0059\n",
      "Epoch 110/1000, Beta: 5.00, Total Loss: 4239.9616, Recon Loss: 8479.9122, KL Loss: 0.0000, Percep Loss: 0.0055\n",
      "Epoch 111/1000, Beta: 5.00, Total Loss: 4239.9437, Recon Loss: 8479.8756, KL Loss: 0.0000, Percep Loss: 0.0059\n",
      "Epoch 112/1000, Beta: 5.00, Total Loss: 4239.8229, Recon Loss: 8479.6349, KL Loss: 0.0000, Percep Loss: 0.0054\n",
      "Epoch 113/1000, Beta: 5.00, Total Loss: 4239.9291, Recon Loss: 8479.8458, KL Loss: 0.0000, Percep Loss: 0.0062\n",
      "Epoch 114/1000, Beta: 5.00, Total Loss: 4239.8278, Recon Loss: 8479.6446, KL Loss: 0.0000, Percep Loss: 0.0055\n",
      "Epoch 115/1000, Beta: 5.00, Total Loss: 4239.8615, Recon Loss: 8479.7100, KL Loss: 0.0000, Percep Loss: 0.0064\n",
      "Epoch 116/1000, Beta: 5.00, Total Loss: 4239.8738, Recon Loss: 8479.7345, KL Loss: 0.0000, Percep Loss: 0.0065\n",
      "Epoch 117/1000, Beta: 5.00, Total Loss: 4239.8311, Recon Loss: 8479.6506, KL Loss: 0.0000, Percep Loss: 0.0057\n",
      "Epoch 118/1000, Beta: 5.00, Total Loss: 4239.7315, Recon Loss: 8479.4532, KL Loss: 0.0000, Percep Loss: 0.0048\n",
      "Epoch 119/1000, Beta: 5.00, Total Loss: 4239.6945, Recon Loss: 8479.3789, KL Loss: 0.0000, Percep Loss: 0.0050\n",
      "Epoch 120/1000, Beta: 5.00, Total Loss: 4239.6211, Recon Loss: 8479.2332, KL Loss: 0.0000, Percep Loss: 0.0045\n",
      "Epoch 121/1000, Beta: 5.00, Total Loss: 4239.7232, Recon Loss: 8479.4333, KL Loss: 0.0000, Percep Loss: 0.0065\n",
      "Epoch 122/1000, Beta: 5.00, Total Loss: 4239.7506, Recon Loss: 8479.4873, KL Loss: 0.0000, Percep Loss: 0.0070\n",
      "Epoch 123/1000, Beta: 5.00, Total Loss: 4239.7155, Recon Loss: 8479.4173, KL Loss: 0.0000, Percep Loss: 0.0068\n",
      "Epoch 124/1000, Beta: 5.00, Total Loss: 4239.7170, Recon Loss: 8479.4232, KL Loss: 0.0000, Percep Loss: 0.0053\n",
      "Epoch 125/1000, Beta: 5.00, Total Loss: 4239.9117, Recon Loss: 8479.8073, KL Loss: 0.0000, Percep Loss: 0.0080\n",
      "Epoch 126/1000, Beta: 5.00, Total Loss: 4239.6513, Recon Loss: 8479.2885, KL Loss: 0.0000, Percep Loss: 0.0071\n",
      "Epoch 127/1000, Beta: 5.00, Total Loss: 4239.7094, Recon Loss: 8479.4061, KL Loss: 0.0000, Percep Loss: 0.0063\n",
      "Epoch 128/1000, Beta: 5.00, Total Loss: 4239.5428, Recon Loss: 8479.0770, KL Loss: 0.0000, Percep Loss: 0.0043\n",
      "Epoch 129/1000, Beta: 5.00, Total Loss: 4239.5044, Recon Loss: 8479.0000, KL Loss: 0.0000, Percep Loss: 0.0043\n",
      "Epoch 130/1000, Beta: 5.00, Total Loss: 4239.4863, Recon Loss: 8478.9629, KL Loss: 0.0000, Percep Loss: 0.0049\n",
      "Epoch 131/1000, Beta: 5.00, Total Loss: 4239.5585, Recon Loss: 8479.1073, KL Loss: 0.0000, Percep Loss: 0.0048\n",
      "Epoch 132/1000, Beta: 5.00, Total Loss: 4239.7047, Recon Loss: 8479.3946, KL Loss: 0.0000, Percep Loss: 0.0074\n",
      "Epoch 133/1000, Beta: 5.00, Total Loss: 4239.4900, Recon Loss: 8478.9717, KL Loss: 0.0000, Percep Loss: 0.0041\n",
      "Epoch 134/1000, Beta: 5.00, Total Loss: 4239.4019, Recon Loss: 8478.7960, KL Loss: 0.0000, Percep Loss: 0.0038\n",
      "Epoch 135/1000, Beta: 5.00, Total Loss: 4239.4711, Recon Loss: 8478.9332, KL Loss: 0.0000, Percep Loss: 0.0044\n",
      "Epoch 136/1000, Beta: 5.00, Total Loss: 4239.4811, Recon Loss: 8478.9525, KL Loss: 0.0000, Percep Loss: 0.0048\n",
      "Epoch 137/1000, Beta: 5.00, Total Loss: 4239.4617, Recon Loss: 8478.9135, KL Loss: 0.0000, Percep Loss: 0.0048\n",
      "Epoch 138/1000, Beta: 5.00, Total Loss: 4239.3694, Recon Loss: 8478.7306, KL Loss: 0.0000, Percep Loss: 0.0041\n",
      "Epoch 139/1000, Beta: 5.00, Total Loss: 4239.4929, Recon Loss: 8478.9751, KL Loss: 0.0000, Percep Loss: 0.0053\n",
      "Epoch 140/1000, Beta: 5.00, Total Loss: 4239.3262, Recon Loss: 8478.6449, KL Loss: 0.0000, Percep Loss: 0.0037\n",
      "Epoch 141/1000, Beta: 5.00, Total Loss: 4239.3488, Recon Loss: 8478.6898, KL Loss: 0.0000, Percep Loss: 0.0039\n",
      "Epoch 142/1000, Beta: 5.00, Total Loss: 4239.5519, Recon Loss: 8479.0929, KL Loss: 0.0000, Percep Loss: 0.0053\n",
      "Epoch 143/1000, Beta: 5.00, Total Loss: 4239.3997, Recon Loss: 8478.7903, KL Loss: 0.0000, Percep Loss: 0.0045\n",
      "Epoch 144/1000, Beta: 5.00, Total Loss: 4239.4963, Recon Loss: 8478.9825, KL Loss: 0.0000, Percep Loss: 0.0050\n",
      "Epoch 145/1000, Beta: 5.00, Total Loss: 4239.3602, Recon Loss: 8478.7110, KL Loss: 0.0000, Percep Loss: 0.0047\n",
      "Epoch 146/1000, Beta: 5.00, Total Loss: 4239.3315, Recon Loss: 8478.6541, KL Loss: 0.0000, Percep Loss: 0.0045\n",
      "Epoch 147/1000, Beta: 5.00, Total Loss: 4239.2854, Recon Loss: 8478.5618, KL Loss: 0.0000, Percep Loss: 0.0045\n",
      "Epoch 148/1000, Beta: 5.00, Total Loss: 4239.4364, Recon Loss: 8478.8595, KL Loss: 0.0000, Percep Loss: 0.0066\n",
      "Epoch 149/1000, Beta: 5.00, Total Loss: 4239.3367, Recon Loss: 8478.6636, KL Loss: 0.0000, Percep Loss: 0.0049\n",
      "Epoch 150/1000, Beta: 5.00, Total Loss: 4239.2496, Recon Loss: 8478.4903, KL Loss: 0.0000, Percep Loss: 0.0044\n",
      "Epoch 151/1000, Beta: 5.00, Total Loss: 4239.3748, Recon Loss: 8478.7390, KL Loss: 0.0000, Percep Loss: 0.0053\n",
      "Epoch 152/1000, Beta: 5.00, Total Loss: 4239.2253, Recon Loss: 8478.4438, KL Loss: 0.0000, Percep Loss: 0.0034\n",
      "Epoch 153/1000, Beta: 5.00, Total Loss: 4239.3028, Recon Loss: 8478.5956, KL Loss: 0.0000, Percep Loss: 0.0050\n",
      "Epoch 154/1000, Beta: 5.00, Total Loss: 4239.2148, Recon Loss: 8478.4218, KL Loss: 0.0000, Percep Loss: 0.0039\n",
      "Epoch 155/1000, Beta: 5.00, Total Loss: 4239.3826, Recon Loss: 8478.7556, KL Loss: 0.0000, Percep Loss: 0.0048\n",
      "Epoch 156/1000, Beta: 5.00, Total Loss: 4239.3979, Recon Loss: 8478.7844, KL Loss: 0.0000, Percep Loss: 0.0057\n",
      "Epoch 157/1000, Beta: 5.00, Total Loss: 4239.1827, Recon Loss: 8478.3579, KL Loss: 0.0000, Percep Loss: 0.0038\n",
      "Epoch 158/1000, Beta: 5.00, Total Loss: 4239.1674, Recon Loss: 8478.3266, KL Loss: 0.0000, Percep Loss: 0.0041\n",
      "Epoch 159/1000, Beta: 5.00, Total Loss: 4239.2558, Recon Loss: 8478.5011, KL Loss: 0.0000, Percep Loss: 0.0053\n",
      "Epoch 160/1000, Beta: 5.00, Total Loss: 4239.4279, Recon Loss: 8478.8456, KL Loss: 0.0000, Percep Loss: 0.0051\n",
      "Epoch 161/1000, Beta: 5.00, Total Loss: 4239.2697, Recon Loss: 8478.5325, KL Loss: 0.0000, Percep Loss: 0.0035\n",
      "Epoch 162/1000, Beta: 5.00, Total Loss: 4239.0903, Recon Loss: 8478.1741, KL Loss: 0.0000, Percep Loss: 0.0032\n",
      "Epoch 163/1000, Beta: 5.00, Total Loss: 4239.4229, Recon Loss: 8478.8372, KL Loss: 0.0000, Percep Loss: 0.0043\n",
      "Epoch 164/1000, Beta: 5.00, Total Loss: 4239.2171, Recon Loss: 8478.4270, KL Loss: 0.0000, Percep Loss: 0.0036\n",
      "Epoch 165/1000, Beta: 5.00, Total Loss: 4239.1465, Recon Loss: 8478.2847, KL Loss: 0.0000, Percep Loss: 0.0041\n",
      "Epoch 166/1000, Beta: 5.00, Total Loss: 4239.0797, Recon Loss: 8478.1528, KL Loss: 0.0000, Percep Loss: 0.0034\n",
      "Epoch 167/1000, Beta: 5.00, Total Loss: 4239.1546, Recon Loss: 8478.3003, KL Loss: 0.0000, Percep Loss: 0.0044\n",
      "Epoch 168/1000, Beta: 5.00, Total Loss: 4239.1526, Recon Loss: 8478.2971, KL Loss: 0.0000, Percep Loss: 0.0041\n",
      "Epoch 169/1000, Beta: 5.00, Total Loss: 4239.0016, Recon Loss: 8477.9974, KL Loss: 0.0000, Percep Loss: 0.0029\n",
      "Epoch 170/1000, Beta: 5.00, Total Loss: 4239.1360, Recon Loss: 8478.2621, KL Loss: 0.0000, Percep Loss: 0.0049\n",
      "Epoch 171/1000, Beta: 5.00, Total Loss: 4239.2105, Recon Loss: 8478.4133, KL Loss: 0.0000, Percep Loss: 0.0038\n",
      "Epoch 172/1000, Beta: 5.00, Total Loss: 4239.2482, Recon Loss: 8478.4907, KL Loss: 0.0000, Percep Loss: 0.0029\n",
      "Epoch 173/1000, Beta: 5.00, Total Loss: 4239.2192, Recon Loss: 8478.4323, KL Loss: 0.0000, Percep Loss: 0.0031\n",
      "Epoch 174/1000, Beta: 5.00, Total Loss: 4239.0366, Recon Loss: 8478.0642, KL Loss: 0.0000, Percep Loss: 0.0045\n",
      "Epoch 175/1000, Beta: 5.00, Total Loss: 4239.0481, Recon Loss: 8478.0886, KL Loss: 0.0000, Percep Loss: 0.0039\n",
      "Epoch 176/1000, Beta: 5.00, Total Loss: 4239.0109, Recon Loss: 8478.0155, KL Loss: 0.0000, Percep Loss: 0.0032\n",
      "Epoch 177/1000, Beta: 5.00, Total Loss: 4239.0925, Recon Loss: 8478.1761, KL Loss: 0.0000, Percep Loss: 0.0045\n",
      "Epoch 178/1000, Beta: 5.00, Total Loss: 4239.0688, Recon Loss: 8478.1315, KL Loss: 0.0000, Percep Loss: 0.0030\n",
      "Epoch 179/1000, Beta: 5.00, Total Loss: 4239.1170, Recon Loss: 8478.2263, KL Loss: 0.0000, Percep Loss: 0.0039\n",
      "Epoch 180/1000, Beta: 5.00, Total Loss: 4239.0174, Recon Loss: 8478.0282, KL Loss: 0.0000, Percep Loss: 0.0033\n",
      "Epoch 181/1000, Beta: 5.00, Total Loss: 4239.2091, Recon Loss: 8478.4081, KL Loss: 0.0000, Percep Loss: 0.0050\n",
      "Epoch 182/1000, Beta: 5.00, Total Loss: 4238.9801, Recon Loss: 8477.9528, KL Loss: 0.0000, Percep Loss: 0.0037\n",
      "Epoch 183/1000, Beta: 5.00, Total Loss: 4238.9787, Recon Loss: 8477.9505, KL Loss: 0.0000, Percep Loss: 0.0035\n",
      "Epoch 184/1000, Beta: 5.00, Total Loss: 4238.9872, Recon Loss: 8477.9683, KL Loss: 0.0000, Percep Loss: 0.0031\n",
      "Epoch 185/1000, Beta: 5.00, Total Loss: 4239.0279, Recon Loss: 8478.0485, KL Loss: 0.0000, Percep Loss: 0.0037\n",
      "Epoch 186/1000, Beta: 5.00, Total Loss: 4238.9449, Recon Loss: 8477.8830, KL Loss: 0.0000, Percep Loss: 0.0034\n",
      "Epoch 187/1000, Beta: 5.00, Total Loss: 4238.8794, Recon Loss: 8477.7534, KL Loss: 0.0000, Percep Loss: 0.0027\n",
      "Epoch 188/1000, Beta: 5.00, Total Loss: 4239.0364, Recon Loss: 8478.0661, KL Loss: 0.0000, Percep Loss: 0.0033\n",
      "Epoch 189/1000, Beta: 5.00, Total Loss: 4239.0440, Recon Loss: 8478.0817, KL Loss: 0.0000, Percep Loss: 0.0031\n",
      "Epoch 190/1000, Beta: 5.00, Total Loss: 4238.9927, Recon Loss: 8477.9786, KL Loss: 0.0000, Percep Loss: 0.0034\n",
      "Epoch 191/1000, Beta: 5.00, Total Loss: 4238.8949, Recon Loss: 8477.7839, KL Loss: 0.0000, Percep Loss: 0.0030\n",
      "Epoch 192/1000, Beta: 5.00, Total Loss: 4239.0181, Recon Loss: 8478.0291, KL Loss: 0.0000, Percep Loss: 0.0035\n",
      "Epoch 193/1000, Beta: 5.00, Total Loss: 4238.9335, Recon Loss: 8477.8601, KL Loss: 0.0000, Percep Loss: 0.0034\n",
      "Epoch 194/1000, Beta: 5.00, Total Loss: 4238.9017, Recon Loss: 8477.7967, KL Loss: 0.0000, Percep Loss: 0.0033\n",
      "Epoch 195/1000, Beta: 5.00, Total Loss: 4238.9104, Recon Loss: 8477.8138, KL Loss: 0.0000, Percep Loss: 0.0034\n",
      "Epoch 196/1000, Beta: 5.00, Total Loss: 4238.9348, Recon Loss: 8477.8620, KL Loss: 0.0000, Percep Loss: 0.0039\n",
      "Epoch 197/1000, Beta: 5.00, Total Loss: 4239.0453, Recon Loss: 8478.0846, KL Loss: 0.0000, Percep Loss: 0.0030\n",
      "Epoch 198/1000, Beta: 5.00, Total Loss: 4239.0062, Recon Loss: 8478.0077, KL Loss: 0.0000, Percep Loss: 0.0023\n",
      "Epoch 199/1000, Beta: 5.00, Total Loss: 4238.8089, Recon Loss: 8477.6110, KL Loss: 0.0000, Percep Loss: 0.0034\n",
      "Epoch 200/1000, Beta: 5.00, Total Loss: 4238.9145, Recon Loss: 8477.8201, KL Loss: 0.0000, Percep Loss: 0.0044\n",
      "Epoch 201/1000, Beta: 5.00, Total Loss: 4238.7397, Recon Loss: 8477.4740, KL Loss: 0.0000, Percep Loss: 0.0026\n",
      "Epoch 202/1000, Beta: 5.00, Total Loss: 4238.6471, Recon Loss: 8477.2898, KL Loss: 0.0000, Percep Loss: 0.0022\n",
      "Epoch 203/1000, Beta: 5.00, Total Loss: 4238.6719, Recon Loss: 8477.3385, KL Loss: 0.0000, Percep Loss: 0.0026\n",
      "Epoch 204/1000, Beta: 5.00, Total Loss: 4238.6843, Recon Loss: 8477.3639, KL Loss: 0.0000, Percep Loss: 0.0023\n",
      "Epoch 205/1000, Beta: 5.00, Total Loss: 4238.6434, Recon Loss: 8477.2822, KL Loss: 0.0000, Percep Loss: 0.0023\n",
      "Epoch 206/1000, Beta: 5.00, Total Loss: 4238.6431, Recon Loss: 8477.2817, KL Loss: 0.0000, Percep Loss: 0.0022\n",
      "Epoch 207/1000, Beta: 5.00, Total Loss: 4238.6326, Recon Loss: 8477.2607, KL Loss: 0.0000, Percep Loss: 0.0023\n",
      "Epoch 208/1000, Beta: 5.00, Total Loss: 4238.6106, Recon Loss: 8477.2169, KL Loss: 0.0000, Percep Loss: 0.0021\n",
      "Epoch 209/1000, Beta: 5.00, Total Loss: 4238.6310, Recon Loss: 8477.2576, KL Loss: 0.0000, Percep Loss: 0.0022\n",
      "Epoch 210/1000, Beta: 5.00, Total Loss: 4238.5966, Recon Loss: 8477.1891, KL Loss: 0.0000, Percep Loss: 0.0021\n",
      "Epoch 211/1000, Beta: 5.00, Total Loss: 4238.6149, Recon Loss: 8477.2255, KL Loss: 0.0000, Percep Loss: 0.0021\n",
      "Epoch 212/1000, Beta: 5.00, Total Loss: 4238.6006, Recon Loss: 8477.1970, KL Loss: 0.0000, Percep Loss: 0.0022\n",
      "Epoch 213/1000, Beta: 5.00, Total Loss: 4238.6166, Recon Loss: 8477.2288, KL Loss: 0.0000, Percep Loss: 0.0022\n",
      "Epoch 214/1000, Beta: 5.00, Total Loss: 4238.5819, Recon Loss: 8477.1596, KL Loss: 0.0000, Percep Loss: 0.0021\n",
      "Epoch 215/1000, Beta: 5.00, Total Loss: 4238.5906, Recon Loss: 8477.1768, KL Loss: 0.0000, Percep Loss: 0.0022\n",
      "Epoch 216/1000, Beta: 5.00, Total Loss: 4238.5888, Recon Loss: 8477.1734, KL Loss: 0.0000, Percep Loss: 0.0022\n",
      "Epoch 217/1000, Beta: 5.00, Total Loss: 4238.5830, Recon Loss: 8477.1620, KL Loss: 0.0000, Percep Loss: 0.0020\n",
      "Epoch 218/1000, Beta: 5.00, Total Loss: 4238.5926, Recon Loss: 8477.1809, KL Loss: 0.0000, Percep Loss: 0.0022\n",
      "Epoch 219/1000, Beta: 5.00, Total Loss: 4238.5887, Recon Loss: 8477.1731, KL Loss: 0.0000, Percep Loss: 0.0021\n",
      "Epoch 220/1000, Beta: 5.00, Total Loss: 4238.5594, Recon Loss: 8477.1149, KL Loss: 0.0000, Percep Loss: 0.0020\n",
      "Epoch 221/1000, Beta: 5.00, Total Loss: 4238.5535, Recon Loss: 8477.1029, KL Loss: 0.0000, Percep Loss: 0.0020\n",
      "Epoch 222/1000, Beta: 5.00, Total Loss: 4238.5495, Recon Loss: 8477.0952, KL Loss: 0.0000, Percep Loss: 0.0019\n",
      "Epoch 223/1000, Beta: 5.00, Total Loss: 4238.5541, Recon Loss: 8477.1040, KL Loss: 0.0000, Percep Loss: 0.0021\n",
      "Epoch 224/1000, Beta: 5.00, Total Loss: 4238.5511, Recon Loss: 8477.0982, KL Loss: 0.0000, Percep Loss: 0.0020\n",
      "Epoch 225/1000, Beta: 5.00, Total Loss: 4238.5806, Recon Loss: 8477.1570, KL Loss: 0.0000, Percep Loss: 0.0021\n",
      "Epoch 226/1000, Beta: 5.00, Total Loss: 4238.5456, Recon Loss: 8477.0870, KL Loss: 0.0000, Percep Loss: 0.0020\n",
      "Epoch 227/1000, Beta: 5.00, Total Loss: 4238.5494, Recon Loss: 8477.0948, KL Loss: 0.0000, Percep Loss: 0.0020\n",
      "Epoch 228/1000, Beta: 5.00, Total Loss: 4238.5433, Recon Loss: 8477.0827, KL Loss: 0.0000, Percep Loss: 0.0020\n",
      "Epoch 229/1000, Beta: 5.00, Total Loss: 4238.5647, Recon Loss: 8477.1249, KL Loss: 0.0000, Percep Loss: 0.0022\n",
      "Epoch 230/1000, Beta: 5.00, Total Loss: 4238.5427, Recon Loss: 8477.0814, KL Loss: 0.0000, Percep Loss: 0.0020\n",
      "Epoch 231/1000, Beta: 5.00, Total Loss: 4238.5434, Recon Loss: 8477.0828, KL Loss: 0.0000, Percep Loss: 0.0020\n",
      "Epoch 232/1000, Beta: 5.00, Total Loss: 4238.5145, Recon Loss: 8477.0254, KL Loss: 0.0000, Percep Loss: 0.0018\n",
      "Epoch 233/1000, Beta: 5.00, Total Loss: 4238.5399, Recon Loss: 8477.0760, KL Loss: 0.0000, Percep Loss: 0.0019\n",
      "Epoch 234/1000, Beta: 5.00, Total Loss: 4238.5882, Recon Loss: 8477.1716, KL Loss: 0.0000, Percep Loss: 0.0024\n",
      "Epoch 235/1000, Beta: 5.00, Total Loss: 4238.5101, Recon Loss: 8477.0161, KL Loss: 0.0000, Percep Loss: 0.0020\n",
      "Epoch 236/1000, Beta: 5.00, Total Loss: 4238.5198, Recon Loss: 8477.0354, KL Loss: 0.0000, Percep Loss: 0.0021\n",
      "Epoch 237/1000, Beta: 5.00, Total Loss: 4238.5157, Recon Loss: 8477.0274, KL Loss: 0.0000, Percep Loss: 0.0021\n",
      "Epoch 238/1000, Beta: 5.00, Total Loss: 4238.5434, Recon Loss: 8477.0824, KL Loss: 0.0000, Percep Loss: 0.0022\n",
      "Epoch 239/1000, Beta: 5.00, Total Loss: 4238.4966, Recon Loss: 8476.9894, KL Loss: 0.0000, Percep Loss: 0.0019\n",
      "Epoch 240/1000, Beta: 5.00, Total Loss: 4238.4919, Recon Loss: 8476.9798, KL Loss: 0.0000, Percep Loss: 0.0019\n",
      "Epoch 241/1000, Beta: 5.00, Total Loss: 4238.5215, Recon Loss: 8477.0388, KL Loss: 0.0000, Percep Loss: 0.0021\n",
      "Epoch 242/1000, Beta: 5.00, Total Loss: 4238.5211, Recon Loss: 8477.0384, KL Loss: 0.0000, Percep Loss: 0.0020\n",
      "Epoch 243/1000, Beta: 5.00, Total Loss: 4238.4940, Recon Loss: 8476.9844, KL Loss: 0.0000, Percep Loss: 0.0018\n",
      "Epoch 244/1000, Beta: 5.00, Total Loss: 4238.4871, Recon Loss: 8476.9705, KL Loss: 0.0000, Percep Loss: 0.0018\n",
      "Epoch 245/1000, Beta: 5.00, Total Loss: 4238.4893, Recon Loss: 8476.9746, KL Loss: 0.0000, Percep Loss: 0.0020\n",
      "Epoch 246/1000, Beta: 5.00, Total Loss: 4238.4957, Recon Loss: 8476.9875, KL Loss: 0.0000, Percep Loss: 0.0020\n",
      "Epoch 247/1000, Beta: 5.00, Total Loss: 4238.4726, Recon Loss: 8476.9415, KL Loss: 0.0000, Percep Loss: 0.0018\n",
      "Epoch 248/1000, Beta: 5.00, Total Loss: 4238.4892, Recon Loss: 8476.9746, KL Loss: 0.0000, Percep Loss: 0.0019\n",
      "Epoch 249/1000, Beta: 5.00, Total Loss: 4238.4733, Recon Loss: 8476.9431, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 250/1000, Beta: 5.00, Total Loss: 4238.4741, Recon Loss: 8476.9443, KL Loss: 0.0000, Percep Loss: 0.0019\n",
      "Epoch 251/1000, Beta: 5.00, Total Loss: 4238.4948, Recon Loss: 8476.9859, KL Loss: 0.0000, Percep Loss: 0.0018\n",
      "Epoch 252/1000, Beta: 5.00, Total Loss: 4238.4897, Recon Loss: 8476.9757, KL Loss: 0.0000, Percep Loss: 0.0019\n",
      "Epoch 253/1000, Beta: 5.00, Total Loss: 4238.4720, Recon Loss: 8476.9403, KL Loss: 0.0000, Percep Loss: 0.0019\n",
      "Epoch 254/1000, Beta: 5.00, Total Loss: 4238.4706, Recon Loss: 8476.9376, KL Loss: 0.0000, Percep Loss: 0.0018\n",
      "Epoch 255/1000, Beta: 5.00, Total Loss: 4238.4843, Recon Loss: 8476.9648, KL Loss: 0.0000, Percep Loss: 0.0019\n",
      "Epoch 256/1000, Beta: 5.00, Total Loss: 4238.4639, Recon Loss: 8476.9242, KL Loss: 0.0000, Percep Loss: 0.0018\n",
      "Epoch 257/1000, Beta: 5.00, Total Loss: 4238.4655, Recon Loss: 8476.9272, KL Loss: 0.0000, Percep Loss: 0.0019\n",
      "Epoch 258/1000, Beta: 5.00, Total Loss: 4238.4781, Recon Loss: 8476.9523, KL Loss: 0.0000, Percep Loss: 0.0020\n",
      "Epoch 259/1000, Beta: 5.00, Total Loss: 4238.4539, Recon Loss: 8476.9042, KL Loss: 0.0000, Percep Loss: 0.0018\n",
      "Epoch 260/1000, Beta: 5.00, Total Loss: 4238.4495, Recon Loss: 8476.8954, KL Loss: 0.0000, Percep Loss: 0.0018\n",
      "Epoch 261/1000, Beta: 5.00, Total Loss: 4238.4971, Recon Loss: 8476.9901, KL Loss: 0.0000, Percep Loss: 0.0021\n",
      "Epoch 262/1000, Beta: 5.00, Total Loss: 4238.4398, Recon Loss: 8476.8762, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 263/1000, Beta: 5.00, Total Loss: 4238.4763, Recon Loss: 8476.9486, KL Loss: 0.0000, Percep Loss: 0.0020\n",
      "Epoch 264/1000, Beta: 5.00, Total Loss: 4238.5065, Recon Loss: 8477.0092, KL Loss: 0.0000, Percep Loss: 0.0019\n",
      "Epoch 265/1000, Beta: 5.00, Total Loss: 4238.4322, Recon Loss: 8476.8609, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 266/1000, Beta: 5.00, Total Loss: 4238.4303, Recon Loss: 8476.8571, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 267/1000, Beta: 5.00, Total Loss: 4238.4566, Recon Loss: 8476.9091, KL Loss: 0.0000, Percep Loss: 0.0021\n",
      "Epoch 268/1000, Beta: 5.00, Total Loss: 4238.4847, Recon Loss: 8476.9653, KL Loss: 0.0000, Percep Loss: 0.0021\n",
      "Epoch 269/1000, Beta: 5.00, Total Loss: 4238.4254, Recon Loss: 8476.8475, KL Loss: 0.0000, Percep Loss: 0.0016\n",
      "Epoch 270/1000, Beta: 5.00, Total Loss: 4238.4185, Recon Loss: 8476.8337, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 271/1000, Beta: 5.00, Total Loss: 4238.4185, Recon Loss: 8476.8337, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 272/1000, Beta: 5.00, Total Loss: 4238.4430, Recon Loss: 8476.8828, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 273/1000, Beta: 5.00, Total Loss: 4238.4663, Recon Loss: 8476.9286, KL Loss: 0.0000, Percep Loss: 0.0020\n",
      "Epoch 274/1000, Beta: 5.00, Total Loss: 4238.4885, Recon Loss: 8476.9731, KL Loss: 0.0000, Percep Loss: 0.0020\n",
      "Epoch 275/1000, Beta: 5.00, Total Loss: 4238.4449, Recon Loss: 8476.8866, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 276/1000, Beta: 5.00, Total Loss: 4238.4112, Recon Loss: 8476.8190, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 277/1000, Beta: 5.00, Total Loss: 4238.4140, Recon Loss: 8476.8247, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 278/1000, Beta: 5.00, Total Loss: 4238.4222, Recon Loss: 8476.8407, KL Loss: 0.0000, Percep Loss: 0.0018\n",
      "Epoch 279/1000, Beta: 5.00, Total Loss: 4238.4057, Recon Loss: 8476.8083, KL Loss: 0.0000, Percep Loss: 0.0016\n",
      "Epoch 280/1000, Beta: 5.00, Total Loss: 4238.4399, Recon Loss: 8476.8759, KL Loss: 0.0000, Percep Loss: 0.0019\n",
      "Epoch 281/1000, Beta: 5.00, Total Loss: 4238.4238, Recon Loss: 8476.8440, KL Loss: 0.0000, Percep Loss: 0.0018\n",
      "Epoch 282/1000, Beta: 5.00, Total Loss: 4238.3931, Recon Loss: 8476.7830, KL Loss: 0.0000, Percep Loss: 0.0016\n",
      "Epoch 283/1000, Beta: 5.00, Total Loss: 4238.4382, Recon Loss: 8476.8723, KL Loss: 0.0000, Percep Loss: 0.0021\n",
      "Epoch 284/1000, Beta: 5.00, Total Loss: 4238.4462, Recon Loss: 8476.8888, KL Loss: 0.0000, Percep Loss: 0.0018\n",
      "Epoch 285/1000, Beta: 5.00, Total Loss: 4238.4335, Recon Loss: 8476.8634, KL Loss: 0.0000, Percep Loss: 0.0018\n",
      "Epoch 286/1000, Beta: 5.00, Total Loss: 4238.3987, Recon Loss: 8476.7940, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 287/1000, Beta: 5.00, Total Loss: 4238.4084, Recon Loss: 8476.8131, KL Loss: 0.0000, Percep Loss: 0.0018\n",
      "Epoch 288/1000, Beta: 5.00, Total Loss: 4238.4553, Recon Loss: 8476.9067, KL Loss: 0.0000, Percep Loss: 0.0019\n",
      "Epoch 289/1000, Beta: 5.00, Total Loss: 4238.4511, Recon Loss: 8476.8983, KL Loss: 0.0000, Percep Loss: 0.0020\n",
      "Epoch 290/1000, Beta: 5.00, Total Loss: 4238.3982, Recon Loss: 8476.7932, KL Loss: 0.0000, Percep Loss: 0.0016\n",
      "Epoch 291/1000, Beta: 5.00, Total Loss: 4238.3935, Recon Loss: 8476.7837, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 292/1000, Beta: 5.00, Total Loss: 4238.3892, Recon Loss: 8476.7753, KL Loss: 0.0000, Percep Loss: 0.0016\n",
      "Epoch 293/1000, Beta: 5.00, Total Loss: 4238.3834, Recon Loss: 8476.7635, KL Loss: 0.0000, Percep Loss: 0.0016\n",
      "Epoch 294/1000, Beta: 5.00, Total Loss: 4238.3776, Recon Loss: 8476.7523, KL Loss: 0.0000, Percep Loss: 0.0015\n",
      "Epoch 295/1000, Beta: 5.00, Total Loss: 4238.3813, Recon Loss: 8476.7592, KL Loss: 0.0000, Percep Loss: 0.0016\n",
      "Epoch 296/1000, Beta: 5.00, Total Loss: 4238.3806, Recon Loss: 8476.7579, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 297/1000, Beta: 5.00, Total Loss: 4238.4049, Recon Loss: 8476.8062, KL Loss: 0.0000, Percep Loss: 0.0018\n",
      "Epoch 298/1000, Beta: 5.00, Total Loss: 4238.4117, Recon Loss: 8476.8198, KL Loss: 0.0000, Percep Loss: 0.0018\n",
      "Epoch 299/1000, Beta: 5.00, Total Loss: 4238.3752, Recon Loss: 8476.7472, KL Loss: 0.0000, Percep Loss: 0.0016\n",
      "Epoch 300/1000, Beta: 5.00, Total Loss: 4238.3864, Recon Loss: 8476.7696, KL Loss: 0.0000, Percep Loss: 0.0016\n",
      "Epoch 301/1000, Beta: 5.00, Total Loss: 4238.3709, Recon Loss: 8476.7387, KL Loss: 0.0000, Percep Loss: 0.0016\n",
      "Epoch 302/1000, Beta: 5.00, Total Loss: 4238.3640, Recon Loss: 8476.7251, KL Loss: 0.0000, Percep Loss: 0.0015\n",
      "Epoch 303/1000, Beta: 5.00, Total Loss: 4238.3944, Recon Loss: 8476.7856, KL Loss: 0.0000, Percep Loss: 0.0016\n",
      "Epoch 304/1000, Beta: 5.00, Total Loss: 4238.3538, Recon Loss: 8476.7044, KL Loss: 0.0000, Percep Loss: 0.0015\n",
      "Epoch 305/1000, Beta: 5.00, Total Loss: 4238.3879, Recon Loss: 8476.7725, KL Loss: 0.0000, Percep Loss: 0.0016\n",
      "Epoch 306/1000, Beta: 5.00, Total Loss: 4238.3646, Recon Loss: 8476.7261, KL Loss: 0.0000, Percep Loss: 0.0016\n",
      "Epoch 307/1000, Beta: 5.00, Total Loss: 4238.3643, Recon Loss: 8476.7256, KL Loss: 0.0000, Percep Loss: 0.0015\n",
      "Epoch 308/1000, Beta: 5.00, Total Loss: 4238.3936, Recon Loss: 8476.7838, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 309/1000, Beta: 5.00, Total Loss: 4238.3952, Recon Loss: 8476.7870, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 310/1000, Beta: 5.00, Total Loss: 4238.3829, Recon Loss: 8476.7627, KL Loss: 0.0000, Percep Loss: 0.0016\n",
      "Epoch 311/1000, Beta: 5.00, Total Loss: 4238.4119, Recon Loss: 8476.8203, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 312/1000, Beta: 5.00, Total Loss: 4238.3607, Recon Loss: 8476.7181, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 313/1000, Beta: 5.00, Total Loss: 4238.3785, Recon Loss: 8476.7536, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 314/1000, Beta: 5.00, Total Loss: 4238.3740, Recon Loss: 8476.7447, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 315/1000, Beta: 5.00, Total Loss: 4238.3845, Recon Loss: 8476.7651, KL Loss: 0.0000, Percep Loss: 0.0019\n",
      "Epoch 316/1000, Beta: 5.00, Total Loss: 4238.3744, Recon Loss: 8476.7448, KL Loss: 0.0000, Percep Loss: 0.0020\n",
      "Epoch 317/1000, Beta: 5.00, Total Loss: 4238.3678, Recon Loss: 8476.7323, KL Loss: 0.0000, Percep Loss: 0.0016\n",
      "Epoch 318/1000, Beta: 5.00, Total Loss: 4238.3753, Recon Loss: 8476.7470, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 319/1000, Beta: 5.00, Total Loss: 4238.3674, Recon Loss: 8476.7316, KL Loss: 0.0000, Percep Loss: 0.0016\n",
      "Epoch 320/1000, Beta: 5.00, Total Loss: 4238.3676, Recon Loss: 8476.7322, KL Loss: 0.0000, Percep Loss: 0.0015\n",
      "Epoch 321/1000, Beta: 5.00, Total Loss: 4238.3470, Recon Loss: 8476.6912, KL Loss: 0.0000, Percep Loss: 0.0014\n",
      "Epoch 322/1000, Beta: 5.00, Total Loss: 4238.3768, Recon Loss: 8476.7507, KL Loss: 0.0000, Percep Loss: 0.0014\n",
      "Epoch 323/1000, Beta: 5.00, Total Loss: 4238.3533, Recon Loss: 8476.7036, KL Loss: 0.0000, Percep Loss: 0.0015\n",
      "Epoch 324/1000, Beta: 5.00, Total Loss: 4238.3824, Recon Loss: 8476.7615, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 325/1000, Beta: 5.00, Total Loss: 4238.3613, Recon Loss: 8476.7192, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 326/1000, Beta: 5.00, Total Loss: 4238.3536, Recon Loss: 8476.7041, KL Loss: 0.0000, Percep Loss: 0.0015\n",
      "Epoch 327/1000, Beta: 5.00, Total Loss: 4238.3855, Recon Loss: 8476.7675, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 328/1000, Beta: 5.00, Total Loss: 4238.3685, Recon Loss: 8476.7337, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 329/1000, Beta: 5.00, Total Loss: 4238.3331, Recon Loss: 8476.6634, KL Loss: 0.0000, Percep Loss: 0.0014\n",
      "Epoch 330/1000, Beta: 5.00, Total Loss: 4238.3281, Recon Loss: 8476.6534, KL Loss: 0.0000, Percep Loss: 0.0015\n",
      "Epoch 331/1000, Beta: 5.00, Total Loss: 4238.3343, Recon Loss: 8476.6653, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 332/1000, Beta: 5.00, Total Loss: 4238.3369, Recon Loss: 8476.6707, KL Loss: 0.0000, Percep Loss: 0.0015\n",
      "Epoch 333/1000, Beta: 5.00, Total Loss: 4238.3451, Recon Loss: 8476.6871, KL Loss: 0.0000, Percep Loss: 0.0015\n",
      "Epoch 334/1000, Beta: 5.00, Total Loss: 4238.3491, Recon Loss: 8476.6949, KL Loss: 0.0000, Percep Loss: 0.0016\n",
      "Epoch 335/1000, Beta: 5.00, Total Loss: 4238.3995, Recon Loss: 8476.7947, KL Loss: 0.0000, Percep Loss: 0.0021\n",
      "Epoch 336/1000, Beta: 5.00, Total Loss: 4238.3333, Recon Loss: 8476.6637, KL Loss: 0.0000, Percep Loss: 0.0014\n",
      "Epoch 337/1000, Beta: 5.00, Total Loss: 4238.3392, Recon Loss: 8476.6756, KL Loss: 0.0000, Percep Loss: 0.0015\n",
      "Epoch 338/1000, Beta: 5.00, Total Loss: 4238.3170, Recon Loss: 8476.6310, KL Loss: 0.0000, Percep Loss: 0.0014\n",
      "Epoch 339/1000, Beta: 5.00, Total Loss: 4238.3664, Recon Loss: 8476.7295, KL Loss: 0.0000, Percep Loss: 0.0016\n",
      "Epoch 340/1000, Beta: 5.00, Total Loss: 4238.3641, Recon Loss: 8476.7245, KL Loss: 0.0000, Percep Loss: 0.0019\n",
      "Epoch 341/1000, Beta: 5.00, Total Loss: 4238.3231, Recon Loss: 8476.6434, KL Loss: 0.0000, Percep Loss: 0.0014\n",
      "Epoch 342/1000, Beta: 5.00, Total Loss: 4238.3362, Recon Loss: 8476.6693, KL Loss: 0.0000, Percep Loss: 0.0015\n",
      "Epoch 343/1000, Beta: 5.00, Total Loss: 4238.3373, Recon Loss: 8476.6716, KL Loss: 0.0000, Percep Loss: 0.0015\n",
      "Epoch 344/1000, Beta: 5.00, Total Loss: 4238.3413, Recon Loss: 8476.6793, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 345/1000, Beta: 5.00, Total Loss: 4238.3285, Recon Loss: 8476.6541, KL Loss: 0.0000, Percep Loss: 0.0015\n",
      "Epoch 346/1000, Beta: 5.00, Total Loss: 4238.3333, Recon Loss: 8476.6635, KL Loss: 0.0000, Percep Loss: 0.0015\n",
      "Epoch 347/1000, Beta: 5.00, Total Loss: 4238.3420, Recon Loss: 8476.6810, KL Loss: 0.0000, Percep Loss: 0.0015\n",
      "Epoch 348/1000, Beta: 5.00, Total Loss: 4238.3280, Recon Loss: 8476.6530, KL Loss: 0.0000, Percep Loss: 0.0015\n",
      "Epoch 349/1000, Beta: 5.00, Total Loss: 4238.3346, Recon Loss: 8476.6661, KL Loss: 0.0000, Percep Loss: 0.0015\n",
      "Epoch 350/1000, Beta: 5.00, Total Loss: 4238.3287, Recon Loss: 8476.6544, KL Loss: 0.0000, Percep Loss: 0.0015\n",
      "Epoch 351/1000, Beta: 5.00, Total Loss: 4238.3114, Recon Loss: 8476.6200, KL Loss: 0.0000, Percep Loss: 0.0014\n",
      "Epoch 352/1000, Beta: 5.00, Total Loss: 4238.3097, Recon Loss: 8476.6165, KL Loss: 0.0000, Percep Loss: 0.0014\n",
      "Epoch 353/1000, Beta: 5.00, Total Loss: 4238.3112, Recon Loss: 8476.6194, KL Loss: 0.0000, Percep Loss: 0.0014\n",
      "Epoch 354/1000, Beta: 5.00, Total Loss: 4238.2972, Recon Loss: 8476.5915, KL Loss: 0.0000, Percep Loss: 0.0014\n",
      "Epoch 355/1000, Beta: 5.00, Total Loss: 4238.3003, Recon Loss: 8476.5977, KL Loss: 0.0000, Percep Loss: 0.0014\n",
      "Epoch 356/1000, Beta: 5.00, Total Loss: 4238.3103, Recon Loss: 8476.6174, KL Loss: 0.0000, Percep Loss: 0.0016\n",
      "Epoch 357/1000, Beta: 5.00, Total Loss: 4238.3237, Recon Loss: 8476.6441, KL Loss: 0.0000, Percep Loss: 0.0016\n",
      "Epoch 358/1000, Beta: 5.00, Total Loss: 4238.3111, Recon Loss: 8476.6193, KL Loss: 0.0000, Percep Loss: 0.0015\n",
      "Epoch 359/1000, Beta: 5.00, Total Loss: 4238.3439, Recon Loss: 8476.6848, KL Loss: 0.0000, Percep Loss: 0.0015\n",
      "Epoch 360/1000, Beta: 5.00, Total Loss: 4238.3226, Recon Loss: 8476.6422, KL Loss: 0.0000, Percep Loss: 0.0015\n",
      "Epoch 361/1000, Beta: 5.00, Total Loss: 4238.3307, Recon Loss: 8476.6584, KL Loss: 0.0000, Percep Loss: 0.0015\n",
      "Epoch 362/1000, Beta: 5.00, Total Loss: 4238.3355, Recon Loss: 8476.6677, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 363/1000, Beta: 5.00, Total Loss: 4238.2992, Recon Loss: 8476.5956, KL Loss: 0.0000, Percep Loss: 0.0015\n",
      "Epoch 364/1000, Beta: 5.00, Total Loss: 4238.2906, Recon Loss: 8476.5786, KL Loss: 0.0000, Percep Loss: 0.0013\n",
      "Epoch 365/1000, Beta: 5.00, Total Loss: 4238.3413, Recon Loss: 8476.6790, KL Loss: 0.0000, Percep Loss: 0.0018\n",
      "Epoch 366/1000, Beta: 5.00, Total Loss: 4238.3558, Recon Loss: 8476.7083, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 367/1000, Beta: 5.00, Total Loss: 4238.3087, Recon Loss: 8476.6146, KL Loss: 0.0000, Percep Loss: 0.0014\n",
      "Epoch 368/1000, Beta: 5.00, Total Loss: 4238.3358, Recon Loss: 8476.6685, KL Loss: 0.0000, Percep Loss: 0.0015\n",
      "Epoch 369/1000, Beta: 5.00, Total Loss: 4238.3319, Recon Loss: 8476.6607, KL Loss: 0.0000, Percep Loss: 0.0015\n",
      "Epoch 370/1000, Beta: 5.00, Total Loss: 4238.3013, Recon Loss: 8476.5993, KL Loss: 0.0000, Percep Loss: 0.0016\n",
      "Epoch 371/1000, Beta: 5.00, Total Loss: 4238.3010, Recon Loss: 8476.5990, KL Loss: 0.0000, Percep Loss: 0.0015\n",
      "Epoch 372/1000, Beta: 5.00, Total Loss: 4238.3013, Recon Loss: 8476.5998, KL Loss: 0.0000, Percep Loss: 0.0014\n",
      "Epoch 373/1000, Beta: 5.00, Total Loss: 4238.3036, Recon Loss: 8476.6038, KL Loss: 0.0000, Percep Loss: 0.0017\n",
      "Epoch 374/1000, Beta: 5.00, Total Loss: 4238.2851, Recon Loss: 8476.5675, KL Loss: 0.0000, Percep Loss: 0.0014\n",
      "Epoch 375/1000, Beta: 5.00, Total Loss: 4238.2780, Recon Loss: 8476.5533, KL Loss: 0.0000, Percep Loss: 0.0014\n",
      "Epoch 376/1000, Beta: 5.00, Total Loss: 4238.2765, Recon Loss: 8476.5503, KL Loss: 0.0000, Percep Loss: 0.0014\n",
      "Epoch 377/1000, Beta: 5.00, Total Loss: 4238.3236, Recon Loss: 8476.6444, KL Loss: 0.0000, Percep Loss: 0.0014\n",
      "Epoch 378/1000, Beta: 5.00, Total Loss: 4238.3257, Recon Loss: 8476.6480, KL Loss: 0.0000, Percep Loss: 0.0016\n",
      "Epoch 379/1000, Beta: 5.00, Total Loss: 4238.2889, Recon Loss: 8476.5750, KL Loss: 0.0000, Percep Loss: 0.0014\n",
      "Epoch 380/1000, Beta: 5.00, Total Loss: 4238.3255, Recon Loss: 8476.6479, KL Loss: 0.0000, Percep Loss: 0.0016\n",
      "Epoch 381/1000, Beta: 5.00, Total Loss: 4238.2949, Recon Loss: 8476.5870, KL Loss: 0.0000, Percep Loss: 0.0014\n",
      "Epoch 382/1000, Beta: 5.00, Total Loss: 4238.2914, Recon Loss: 8476.5800, KL Loss: 0.0000, Percep Loss: 0.0014\n",
      "Epoch 383/1000, Beta: 5.00, Total Loss: 4238.2875, Recon Loss: 8476.5722, KL Loss: 0.0000, Percep Loss: 0.0014\n",
      "Epoch 384/1000, Beta: 5.00, Total Loss: 4238.2757, Recon Loss: 8476.5488, KL Loss: 0.0000, Percep Loss: 0.0013\n",
      "Epoch 385/1000, Beta: 5.00, Total Loss: 4238.2970, Recon Loss: 8476.5913, KL Loss: 0.0000, Percep Loss: 0.0014\n",
      "Epoch 386/1000, Beta: 5.00, Total Loss: 4238.2825, Recon Loss: 8476.5621, KL Loss: 0.0000, Percep Loss: 0.0014\n",
      "Epoch 387/1000, Beta: 5.00, Total Loss: 4238.2830, Recon Loss: 8476.5635, KL Loss: 0.0000, Percep Loss: 0.0013\n",
      "Epoch 388/1000, Beta: 5.00, Total Loss: 4238.2980, Recon Loss: 8476.5931, KL Loss: 0.0000, Percep Loss: 0.0014\n",
      "Epoch 389/1000, Beta: 5.00, Total Loss: 4238.2922, Recon Loss: 8476.5816, KL Loss: 0.0000, Percep Loss: 0.0014\n",
      "Epoch 390/1000, Beta: 5.00, Total Loss: 4238.2866, Recon Loss: 8476.5703, KL Loss: 0.0000, Percep Loss: 0.0015\n",
      "Epoch 391/1000, Beta: 5.00, Total Loss: 4238.2632, Recon Loss: 8476.5238, KL Loss: 0.0000, Percep Loss: 0.0013\n",
      "Epoch 392/1000, Beta: 5.00, Total Loss: 4238.2816, Recon Loss: 8476.5606, KL Loss: 0.0000, Percep Loss: 0.0013\n",
      "Epoch 393/1000, Beta: 5.00, Total Loss: 4238.3015, Recon Loss: 8476.6004, KL Loss: 0.0000, Percep Loss: 0.0013\n",
      "Epoch 394/1000, Beta: 5.00, Total Loss: 4238.3027, Recon Loss: 8476.6024, KL Loss: 0.0000, Percep Loss: 0.0015\n",
      "Epoch 395/1000, Beta: 5.00, Total Loss: 4238.2822, Recon Loss: 8476.5614, KL Loss: 0.0000, Percep Loss: 0.0014\n",
      "Epoch 396/1000, Beta: 5.00, Total Loss: 4238.2941, Recon Loss: 8476.5854, KL Loss: 0.0000, Percep Loss: 0.0014\n",
      "Epoch 397/1000, Beta: 5.00, Total Loss: 4238.3216, Recon Loss: 8476.6401, KL Loss: 0.0000, Percep Loss: 0.0016\n",
      "Epoch 398/1000, Beta: 5.00, Total Loss: 4238.2677, Recon Loss: 8476.5330, KL Loss: 0.0000, Percep Loss: 0.0013\n",
      "Epoch 399/1000, Beta: 5.00, Total Loss: 4238.2921, Recon Loss: 8476.5813, KL Loss: 0.0000, Percep Loss: 0.0015\n",
      "Epoch 400/1000, Beta: 5.00, Total Loss: 4238.2642, Recon Loss: 8476.5257, KL Loss: 0.0000, Percep Loss: 0.0013\n",
      "Epoch 401/1000, Beta: 5.00, Total Loss: 4238.2364, Recon Loss: 8476.4703, KL Loss: 0.0000, Percep Loss: 0.0013\n",
      "Epoch 402/1000, Beta: 5.00, Total Loss: 4238.2331, Recon Loss: 8476.4638, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 403/1000, Beta: 5.00, Total Loss: 4238.2306, Recon Loss: 8476.4586, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 404/1000, Beta: 5.00, Total Loss: 4238.2277, Recon Loss: 8476.4530, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 405/1000, Beta: 5.00, Total Loss: 4238.2242, Recon Loss: 8476.4461, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 406/1000, Beta: 5.00, Total Loss: 4238.2247, Recon Loss: 8476.4470, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 407/1000, Beta: 5.00, Total Loss: 4238.2264, Recon Loss: 8476.4504, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 408/1000, Beta: 5.00, Total Loss: 4238.2214, Recon Loss: 8476.4405, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 409/1000, Beta: 5.00, Total Loss: 4238.2222, Recon Loss: 8476.4420, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 410/1000, Beta: 5.00, Total Loss: 4238.2269, Recon Loss: 8476.4513, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 411/1000, Beta: 5.00, Total Loss: 4238.2186, Recon Loss: 8476.4347, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 412/1000, Beta: 5.00, Total Loss: 4238.2223, Recon Loss: 8476.4423, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 413/1000, Beta: 5.00, Total Loss: 4238.2141, Recon Loss: 8476.4257, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 414/1000, Beta: 5.00, Total Loss: 4238.2104, Recon Loss: 8476.4184, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 415/1000, Beta: 5.00, Total Loss: 4238.2125, Recon Loss: 8476.4226, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 416/1000, Beta: 5.00, Total Loss: 4238.2132, Recon Loss: 8476.4240, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 417/1000, Beta: 5.00, Total Loss: 4238.2100, Recon Loss: 8476.4176, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 418/1000, Beta: 5.00, Total Loss: 4238.2104, Recon Loss: 8476.4185, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 419/1000, Beta: 5.00, Total Loss: 4238.2058, Recon Loss: 8476.4092, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 420/1000, Beta: 5.00, Total Loss: 4238.2136, Recon Loss: 8476.4249, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 421/1000, Beta: 5.00, Total Loss: 4238.2073, Recon Loss: 8476.4122, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 422/1000, Beta: 5.00, Total Loss: 4238.2073, Recon Loss: 8476.4124, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 423/1000, Beta: 5.00, Total Loss: 4238.2080, Recon Loss: 8476.4136, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 424/1000, Beta: 5.00, Total Loss: 4238.2107, Recon Loss: 8476.4192, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 425/1000, Beta: 5.00, Total Loss: 4238.2157, Recon Loss: 8476.4291, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 426/1000, Beta: 5.00, Total Loss: 4238.2102, Recon Loss: 8476.4182, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 427/1000, Beta: 5.00, Total Loss: 4238.2057, Recon Loss: 8476.4091, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 428/1000, Beta: 5.00, Total Loss: 4238.2089, Recon Loss: 8476.4155, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 429/1000, Beta: 5.00, Total Loss: 4238.2024, Recon Loss: 8476.4025, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 430/1000, Beta: 5.00, Total Loss: 4238.2069, Recon Loss: 8476.4114, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 431/1000, Beta: 5.00, Total Loss: 4238.2019, Recon Loss: 8476.4016, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 432/1000, Beta: 5.00, Total Loss: 4238.2082, Recon Loss: 8476.4140, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 433/1000, Beta: 5.00, Total Loss: 4238.2056, Recon Loss: 8476.4089, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 434/1000, Beta: 5.00, Total Loss: 4238.1980, Recon Loss: 8476.3937, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 435/1000, Beta: 5.00, Total Loss: 4238.1981, Recon Loss: 8476.3939, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 436/1000, Beta: 5.00, Total Loss: 4238.2025, Recon Loss: 8476.4026, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 437/1000, Beta: 5.00, Total Loss: 4238.1970, Recon Loss: 8476.3917, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 438/1000, Beta: 5.00, Total Loss: 4238.1934, Recon Loss: 8476.3844, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 439/1000, Beta: 5.00, Total Loss: 4238.1952, Recon Loss: 8476.3882, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 440/1000, Beta: 5.00, Total Loss: 4238.2001, Recon Loss: 8476.3977, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 441/1000, Beta: 5.00, Total Loss: 4238.1890, Recon Loss: 8476.3759, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 442/1000, Beta: 5.00, Total Loss: 4238.1940, Recon Loss: 8476.3857, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 443/1000, Beta: 5.00, Total Loss: 4238.1886, Recon Loss: 8476.3749, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 444/1000, Beta: 5.00, Total Loss: 4238.1886, Recon Loss: 8476.3750, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 445/1000, Beta: 5.00, Total Loss: 4238.1921, Recon Loss: 8476.3818, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 446/1000, Beta: 5.00, Total Loss: 4238.1931, Recon Loss: 8476.3839, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 447/1000, Beta: 5.00, Total Loss: 4238.1886, Recon Loss: 8476.3750, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 448/1000, Beta: 5.00, Total Loss: 4238.1872, Recon Loss: 8476.3722, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 449/1000, Beta: 5.00, Total Loss: 4238.1941, Recon Loss: 8476.3859, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 450/1000, Beta: 5.00, Total Loss: 4238.1905, Recon Loss: 8476.3788, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 451/1000, Beta: 5.00, Total Loss: 4238.1869, Recon Loss: 8476.3715, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 452/1000, Beta: 5.00, Total Loss: 4238.1933, Recon Loss: 8476.3842, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 453/1000, Beta: 5.00, Total Loss: 4238.1947, Recon Loss: 8476.3870, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 454/1000, Beta: 5.00, Total Loss: 4238.1891, Recon Loss: 8476.3759, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 455/1000, Beta: 5.00, Total Loss: 4238.1925, Recon Loss: 8476.3828, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 456/1000, Beta: 5.00, Total Loss: 4238.1864, Recon Loss: 8476.3706, KL Loss: 0.0000, Percep Loss: 0.0012\n",
      "Epoch 457/1000, Beta: 5.00, Total Loss: 4238.1818, Recon Loss: 8476.3615, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 458/1000, Beta: 5.00, Total Loss: 4238.1896, Recon Loss: 8476.3770, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 459/1000, Beta: 5.00, Total Loss: 4238.1818, Recon Loss: 8476.3613, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 460/1000, Beta: 5.00, Total Loss: 4238.1831, Recon Loss: 8476.3641, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 461/1000, Beta: 5.00, Total Loss: 4238.1790, Recon Loss: 8476.3557, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 462/1000, Beta: 5.00, Total Loss: 4238.1778, Recon Loss: 8476.3534, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 463/1000, Beta: 5.00, Total Loss: 4238.1868, Recon Loss: 8476.3715, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 464/1000, Beta: 5.00, Total Loss: 4238.1905, Recon Loss: 8476.3787, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 465/1000, Beta: 5.00, Total Loss: 4238.1780, Recon Loss: 8476.3540, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 466/1000, Beta: 5.00, Total Loss: 4238.1800, Recon Loss: 8476.3578, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 467/1000, Beta: 5.00, Total Loss: 4238.1772, Recon Loss: 8476.3522, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 468/1000, Beta: 5.00, Total Loss: 4238.1863, Recon Loss: 8476.3703, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 469/1000, Beta: 5.00, Total Loss: 4238.1795, Recon Loss: 8476.3569, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 470/1000, Beta: 5.00, Total Loss: 4238.1763, Recon Loss: 8476.3503, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 471/1000, Beta: 5.00, Total Loss: 4238.1691, Recon Loss: 8476.3361, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 472/1000, Beta: 5.00, Total Loss: 4238.1733, Recon Loss: 8476.3444, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 473/1000, Beta: 5.00, Total Loss: 4238.1751, Recon Loss: 8476.3480, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 474/1000, Beta: 5.00, Total Loss: 4238.1820, Recon Loss: 8476.3619, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 475/1000, Beta: 5.00, Total Loss: 4238.1735, Recon Loss: 8476.3449, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 476/1000, Beta: 5.00, Total Loss: 4238.1770, Recon Loss: 8476.3517, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 477/1000, Beta: 5.00, Total Loss: 4238.1734, Recon Loss: 8476.3448, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 478/1000, Beta: 5.00, Total Loss: 4238.1704, Recon Loss: 8476.3386, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 479/1000, Beta: 5.00, Total Loss: 4238.1683, Recon Loss: 8476.3344, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 480/1000, Beta: 5.00, Total Loss: 4238.1707, Recon Loss: 8476.3392, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 481/1000, Beta: 5.00, Total Loss: 4238.1718, Recon Loss: 8476.3414, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 482/1000, Beta: 5.00, Total Loss: 4238.1719, Recon Loss: 8476.3416, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 483/1000, Beta: 5.00, Total Loss: 4238.1683, Recon Loss: 8476.3344, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 484/1000, Beta: 5.00, Total Loss: 4238.1699, Recon Loss: 8476.3377, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 485/1000, Beta: 5.00, Total Loss: 4238.1763, Recon Loss: 8476.3504, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 486/1000, Beta: 5.00, Total Loss: 4238.1649, Recon Loss: 8476.3277, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 487/1000, Beta: 5.00, Total Loss: 4238.1724, Recon Loss: 8476.3426, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 488/1000, Beta: 5.00, Total Loss: 4238.1695, Recon Loss: 8476.3368, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 489/1000, Beta: 5.00, Total Loss: 4238.1626, Recon Loss: 8476.3232, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 490/1000, Beta: 5.00, Total Loss: 4238.1734, Recon Loss: 8476.3446, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 491/1000, Beta: 5.00, Total Loss: 4238.1661, Recon Loss: 8476.3301, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 492/1000, Beta: 5.00, Total Loss: 4238.1644, Recon Loss: 8476.3266, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 493/1000, Beta: 5.00, Total Loss: 4238.1659, Recon Loss: 8476.3297, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 494/1000, Beta: 5.00, Total Loss: 4238.1657, Recon Loss: 8476.3293, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 495/1000, Beta: 5.00, Total Loss: 4238.1617, Recon Loss: 8476.3213, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 496/1000, Beta: 5.00, Total Loss: 4238.1622, Recon Loss: 8476.3222, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 497/1000, Beta: 5.00, Total Loss: 4238.1624, Recon Loss: 8476.3228, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 498/1000, Beta: 5.00, Total Loss: 4238.1615, Recon Loss: 8476.3209, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 499/1000, Beta: 5.00, Total Loss: 4238.1592, Recon Loss: 8476.3163, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 500/1000, Beta: 5.00, Total Loss: 4238.1658, Recon Loss: 8476.3295, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 501/1000, Beta: 5.00, Total Loss: 4238.1607, Recon Loss: 8476.3193, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 502/1000, Beta: 5.00, Total Loss: 4238.1689, Recon Loss: 8476.3356, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 503/1000, Beta: 5.00, Total Loss: 4238.1631, Recon Loss: 8476.3241, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 504/1000, Beta: 5.00, Total Loss: 4238.1592, Recon Loss: 8476.3163, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 505/1000, Beta: 5.00, Total Loss: 4238.1584, Recon Loss: 8476.3147, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 506/1000, Beta: 5.00, Total Loss: 4238.1587, Recon Loss: 8476.3154, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 507/1000, Beta: 5.00, Total Loss: 4238.1604, Recon Loss: 8476.3186, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 508/1000, Beta: 5.00, Total Loss: 4238.1528, Recon Loss: 8476.3036, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 509/1000, Beta: 5.00, Total Loss: 4238.1553, Recon Loss: 8476.3084, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 510/1000, Beta: 5.00, Total Loss: 4238.1587, Recon Loss: 8476.3152, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 511/1000, Beta: 5.00, Total Loss: 4238.1546, Recon Loss: 8476.3070, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 512/1000, Beta: 5.00, Total Loss: 4238.1520, Recon Loss: 8476.3019, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 513/1000, Beta: 5.00, Total Loss: 4238.1530, Recon Loss: 8476.3040, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 514/1000, Beta: 5.00, Total Loss: 4238.1559, Recon Loss: 8476.3097, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 515/1000, Beta: 5.00, Total Loss: 4238.1499, Recon Loss: 8476.2978, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 516/1000, Beta: 5.00, Total Loss: 4238.1576, Recon Loss: 8476.3131, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 517/1000, Beta: 5.00, Total Loss: 4238.1503, Recon Loss: 8476.2986, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 518/1000, Beta: 5.00, Total Loss: 4238.1514, Recon Loss: 8476.3008, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 519/1000, Beta: 5.00, Total Loss: 4238.1520, Recon Loss: 8476.3018, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 520/1000, Beta: 5.00, Total Loss: 4238.1562, Recon Loss: 8476.3105, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 521/1000, Beta: 5.00, Total Loss: 4238.1487, Recon Loss: 8476.2954, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 522/1000, Beta: 5.00, Total Loss: 4238.1503, Recon Loss: 8476.2985, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 523/1000, Beta: 5.00, Total Loss: 4238.1491, Recon Loss: 8476.2961, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 524/1000, Beta: 5.00, Total Loss: 4238.1470, Recon Loss: 8476.2918, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 525/1000, Beta: 5.00, Total Loss: 4238.1465, Recon Loss: 8476.2909, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 526/1000, Beta: 5.00, Total Loss: 4238.1624, Recon Loss: 8476.3227, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 527/1000, Beta: 5.00, Total Loss: 4238.1518, Recon Loss: 8476.3015, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 528/1000, Beta: 5.00, Total Loss: 4238.1505, Recon Loss: 8476.2989, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 529/1000, Beta: 5.00, Total Loss: 4238.1547, Recon Loss: 8476.3073, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 530/1000, Beta: 5.00, Total Loss: 4238.1502, Recon Loss: 8476.2983, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 531/1000, Beta: 5.00, Total Loss: 4238.1466, Recon Loss: 8476.2911, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 532/1000, Beta: 5.00, Total Loss: 4238.1462, Recon Loss: 8476.2904, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 533/1000, Beta: 5.00, Total Loss: 4238.1400, Recon Loss: 8476.2780, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 534/1000, Beta: 5.00, Total Loss: 4238.1420, Recon Loss: 8476.2819, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 535/1000, Beta: 5.00, Total Loss: 4238.1495, Recon Loss: 8476.2970, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 536/1000, Beta: 5.00, Total Loss: 4238.1441, Recon Loss: 8476.2860, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 537/1000, Beta: 5.00, Total Loss: 4238.1471, Recon Loss: 8476.2922, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 538/1000, Beta: 5.00, Total Loss: 4238.1438, Recon Loss: 8476.2855, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 539/1000, Beta: 5.00, Total Loss: 4238.1401, Recon Loss: 8476.2781, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 540/1000, Beta: 5.00, Total Loss: 4238.1404, Recon Loss: 8476.2787, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 541/1000, Beta: 5.00, Total Loss: 4238.1349, Recon Loss: 8476.2679, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 542/1000, Beta: 5.00, Total Loss: 4238.1419, Recon Loss: 8476.2816, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 543/1000, Beta: 5.00, Total Loss: 4238.1368, Recon Loss: 8476.2717, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 544/1000, Beta: 5.00, Total Loss: 4238.1364, Recon Loss: 8476.2709, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 545/1000, Beta: 5.00, Total Loss: 4238.1390, Recon Loss: 8476.2760, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 546/1000, Beta: 5.00, Total Loss: 4238.1432, Recon Loss: 8476.2843, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 547/1000, Beta: 5.00, Total Loss: 4238.1447, Recon Loss: 8476.2873, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 548/1000, Beta: 5.00, Total Loss: 4238.1383, Recon Loss: 8476.2746, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 549/1000, Beta: 5.00, Total Loss: 4238.1408, Recon Loss: 8476.2795, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 550/1000, Beta: 5.00, Total Loss: 4238.1375, Recon Loss: 8476.2730, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 551/1000, Beta: 5.00, Total Loss: 4238.1455, Recon Loss: 8476.2891, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 552/1000, Beta: 5.00, Total Loss: 4238.1385, Recon Loss: 8476.2750, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 553/1000, Beta: 5.00, Total Loss: 4238.1351, Recon Loss: 8476.2681, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 554/1000, Beta: 5.00, Total Loss: 4238.1470, Recon Loss: 8476.2920, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 555/1000, Beta: 5.00, Total Loss: 4238.1362, Recon Loss: 8476.2705, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 556/1000, Beta: 5.00, Total Loss: 4238.1349, Recon Loss: 8476.2678, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 557/1000, Beta: 5.00, Total Loss: 4238.1317, Recon Loss: 8476.2613, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 558/1000, Beta: 5.00, Total Loss: 4238.1351, Recon Loss: 8476.2681, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 559/1000, Beta: 5.00, Total Loss: 4238.1422, Recon Loss: 8476.2825, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 560/1000, Beta: 5.00, Total Loss: 4238.1400, Recon Loss: 8476.2781, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 561/1000, Beta: 5.00, Total Loss: 4238.1392, Recon Loss: 8476.2763, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 562/1000, Beta: 5.00, Total Loss: 4238.1315, Recon Loss: 8476.2610, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 563/1000, Beta: 5.00, Total Loss: 4238.1327, Recon Loss: 8476.2634, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 564/1000, Beta: 5.00, Total Loss: 4238.1302, Recon Loss: 8476.2584, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 565/1000, Beta: 5.00, Total Loss: 4238.1275, Recon Loss: 8476.2531, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 566/1000, Beta: 5.00, Total Loss: 4238.1247, Recon Loss: 8476.2474, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 567/1000, Beta: 5.00, Total Loss: 4238.1303, Recon Loss: 8476.2585, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 568/1000, Beta: 5.00, Total Loss: 4238.1238, Recon Loss: 8476.2456, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 569/1000, Beta: 5.00, Total Loss: 4238.1348, Recon Loss: 8476.2675, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 570/1000, Beta: 5.00, Total Loss: 4238.1336, Recon Loss: 8476.2651, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 571/1000, Beta: 5.00, Total Loss: 4238.1328, Recon Loss: 8476.2635, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 572/1000, Beta: 5.00, Total Loss: 4238.1269, Recon Loss: 8476.2517, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 573/1000, Beta: 5.00, Total Loss: 4238.1270, Recon Loss: 8476.2520, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 574/1000, Beta: 5.00, Total Loss: 4238.1274, Recon Loss: 8476.2529, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 575/1000, Beta: 5.00, Total Loss: 4238.1237, Recon Loss: 8476.2455, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 576/1000, Beta: 5.00, Total Loss: 4238.1272, Recon Loss: 8476.2524, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 577/1000, Beta: 5.00, Total Loss: 4238.1325, Recon Loss: 8476.2629, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 578/1000, Beta: 5.00, Total Loss: 4238.1311, Recon Loss: 8476.2601, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 579/1000, Beta: 5.00, Total Loss: 4238.1316, Recon Loss: 8476.2613, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 580/1000, Beta: 5.00, Total Loss: 4238.1350, Recon Loss: 8476.2680, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 581/1000, Beta: 5.00, Total Loss: 4238.1268, Recon Loss: 8476.2516, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 582/1000, Beta: 5.00, Total Loss: 4238.1273, Recon Loss: 8476.2526, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 583/1000, Beta: 5.00, Total Loss: 4238.1454, Recon Loss: 8476.2886, KL Loss: 0.0000, Percep Loss: 0.0011\n",
      "Epoch 584/1000, Beta: 5.00, Total Loss: 4238.1436, Recon Loss: 8476.2853, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 585/1000, Beta: 5.00, Total Loss: 4238.1314, Recon Loss: 8476.2607, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 586/1000, Beta: 5.00, Total Loss: 4238.1305, Recon Loss: 8476.2589, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 587/1000, Beta: 5.00, Total Loss: 4238.1268, Recon Loss: 8476.2517, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 588/1000, Beta: 5.00, Total Loss: 4238.1217, Recon Loss: 8476.2415, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 589/1000, Beta: 5.00, Total Loss: 4238.1300, Recon Loss: 8476.2581, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 590/1000, Beta: 5.00, Total Loss: 4238.1230, Recon Loss: 8476.2440, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 591/1000, Beta: 5.00, Total Loss: 4238.1275, Recon Loss: 8476.2529, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 592/1000, Beta: 5.00, Total Loss: 4238.1285, Recon Loss: 8476.2550, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 593/1000, Beta: 5.00, Total Loss: 4238.1258, Recon Loss: 8476.2497, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 594/1000, Beta: 5.00, Total Loss: 4238.1198, Recon Loss: 8476.2376, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 595/1000, Beta: 5.00, Total Loss: 4238.1206, Recon Loss: 8476.2392, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 596/1000, Beta: 5.00, Total Loss: 4238.1303, Recon Loss: 8476.2586, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 597/1000, Beta: 5.00, Total Loss: 4238.1156, Recon Loss: 8476.2294, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 598/1000, Beta: 5.00, Total Loss: 4238.1179, Recon Loss: 8476.2339, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 599/1000, Beta: 5.00, Total Loss: 4238.1150, Recon Loss: 8476.2280, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 600/1000, Beta: 5.00, Total Loss: 4238.1211, Recon Loss: 8476.2403, KL Loss: 0.0000, Percep Loss: 0.0010\n",
      "Epoch 601/1000, Beta: 5.00, Total Loss: 4238.1082, Recon Loss: 8476.2145, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 602/1000, Beta: 5.00, Total Loss: 4238.1067, Recon Loss: 8476.2115, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 603/1000, Beta: 5.00, Total Loss: 4238.1048, Recon Loss: 8476.2077, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 604/1000, Beta: 5.00, Total Loss: 4238.1041, Recon Loss: 8476.2064, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 605/1000, Beta: 5.00, Total Loss: 4238.1030, Recon Loss: 8476.2041, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 606/1000, Beta: 5.00, Total Loss: 4238.1051, Recon Loss: 8476.2084, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 607/1000, Beta: 5.00, Total Loss: 4238.1028, Recon Loss: 8476.2037, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 608/1000, Beta: 5.00, Total Loss: 4238.1029, Recon Loss: 8476.2040, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 609/1000, Beta: 5.00, Total Loss: 4238.1018, Recon Loss: 8476.2018, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 610/1000, Beta: 5.00, Total Loss: 4238.1024, Recon Loss: 8476.2029, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 611/1000, Beta: 5.00, Total Loss: 4238.0999, Recon Loss: 8476.1979, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 612/1000, Beta: 5.00, Total Loss: 4238.1017, Recon Loss: 8476.2016, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 613/1000, Beta: 5.00, Total Loss: 4238.1016, Recon Loss: 8476.2013, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 614/1000, Beta: 5.00, Total Loss: 4238.1021, Recon Loss: 8476.2023, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 615/1000, Beta: 5.00, Total Loss: 4238.1028, Recon Loss: 8476.2038, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 616/1000, Beta: 5.00, Total Loss: 4238.1003, Recon Loss: 8476.1988, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 617/1000, Beta: 5.00, Total Loss: 4238.0993, Recon Loss: 8476.1967, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 618/1000, Beta: 5.00, Total Loss: 4238.0982, Recon Loss: 8476.1945, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 619/1000, Beta: 5.00, Total Loss: 4238.0986, Recon Loss: 8476.1954, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 620/1000, Beta: 5.00, Total Loss: 4238.0990, Recon Loss: 8476.1961, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 621/1000, Beta: 5.00, Total Loss: 4238.1022, Recon Loss: 8476.2026, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 622/1000, Beta: 5.00, Total Loss: 4238.0986, Recon Loss: 8476.1953, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 623/1000, Beta: 5.00, Total Loss: 4238.0985, Recon Loss: 8476.1952, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 624/1000, Beta: 5.00, Total Loss: 4238.0978, Recon Loss: 8476.1939, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 625/1000, Beta: 5.00, Total Loss: 4238.0950, Recon Loss: 8476.1882, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 626/1000, Beta: 5.00, Total Loss: 4238.0969, Recon Loss: 8476.1919, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 627/1000, Beta: 5.00, Total Loss: 4238.0954, Recon Loss: 8476.1889, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 628/1000, Beta: 5.00, Total Loss: 4238.0959, Recon Loss: 8476.1899, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 629/1000, Beta: 5.00, Total Loss: 4238.0958, Recon Loss: 8476.1897, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 630/1000, Beta: 5.00, Total Loss: 4238.0946, Recon Loss: 8476.1873, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 631/1000, Beta: 5.00, Total Loss: 4238.0954, Recon Loss: 8476.1890, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 632/1000, Beta: 5.00, Total Loss: 4238.0930, Recon Loss: 8476.1842, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 633/1000, Beta: 5.00, Total Loss: 4238.0935, Recon Loss: 8476.1852, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 634/1000, Beta: 5.00, Total Loss: 4238.0924, Recon Loss: 8476.1832, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 635/1000, Beta: 5.00, Total Loss: 4238.0938, Recon Loss: 8476.1857, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 636/1000, Beta: 5.00, Total Loss: 4238.0932, Recon Loss: 8476.1846, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 637/1000, Beta: 5.00, Total Loss: 4238.0926, Recon Loss: 8476.1833, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 638/1000, Beta: 5.00, Total Loss: 4238.0932, Recon Loss: 8476.1846, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 639/1000, Beta: 5.00, Total Loss: 4238.0914, Recon Loss: 8476.1811, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 640/1000, Beta: 5.00, Total Loss: 4238.0932, Recon Loss: 8476.1845, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 641/1000, Beta: 5.00, Total Loss: 4238.0917, Recon Loss: 8476.1817, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 642/1000, Beta: 5.00, Total Loss: 4238.0910, Recon Loss: 8476.1802, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 643/1000, Beta: 5.00, Total Loss: 4238.0904, Recon Loss: 8476.1792, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 644/1000, Beta: 5.00, Total Loss: 4238.0936, Recon Loss: 8476.1854, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 645/1000, Beta: 5.00, Total Loss: 4238.0945, Recon Loss: 8476.1871, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 646/1000, Beta: 5.00, Total Loss: 4238.0888, Recon Loss: 8476.1758, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 647/1000, Beta: 5.00, Total Loss: 4238.0900, Recon Loss: 8476.1781, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 648/1000, Beta: 5.00, Total Loss: 4238.0894, Recon Loss: 8476.1770, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 649/1000, Beta: 5.00, Total Loss: 4238.0915, Recon Loss: 8476.1812, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 650/1000, Beta: 5.00, Total Loss: 4238.0886, Recon Loss: 8476.1755, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 651/1000, Beta: 5.00, Total Loss: 4238.0876, Recon Loss: 8476.1734, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 652/1000, Beta: 5.00, Total Loss: 4238.0883, Recon Loss: 8476.1748, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 653/1000, Beta: 5.00, Total Loss: 4238.0865, Recon Loss: 8476.1711, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 654/1000, Beta: 5.00, Total Loss: 4238.0872, Recon Loss: 8476.1726, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 655/1000, Beta: 5.00, Total Loss: 4238.0879, Recon Loss: 8476.1741, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 656/1000, Beta: 5.00, Total Loss: 4238.0864, Recon Loss: 8476.1709, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 657/1000, Beta: 5.00, Total Loss: 4238.0874, Recon Loss: 8476.1730, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 658/1000, Beta: 5.00, Total Loss: 4238.0884, Recon Loss: 8476.1749, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 659/1000, Beta: 5.00, Total Loss: 4238.0903, Recon Loss: 8476.1788, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 660/1000, Beta: 5.00, Total Loss: 4238.0869, Recon Loss: 8476.1721, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 661/1000, Beta: 5.00, Total Loss: 4238.0845, Recon Loss: 8476.1672, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 662/1000, Beta: 5.00, Total Loss: 4238.0882, Recon Loss: 8476.1746, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 663/1000, Beta: 5.00, Total Loss: 4238.0853, Recon Loss: 8476.1688, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 664/1000, Beta: 5.00, Total Loss: 4238.0854, Recon Loss: 8476.1691, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 665/1000, Beta: 5.00, Total Loss: 4238.0852, Recon Loss: 8476.1687, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 666/1000, Beta: 5.00, Total Loss: 4238.0841, Recon Loss: 8476.1664, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 667/1000, Beta: 5.00, Total Loss: 4238.0850, Recon Loss: 8476.1682, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 668/1000, Beta: 5.00, Total Loss: 4238.0870, Recon Loss: 8476.1721, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 669/1000, Beta: 5.00, Total Loss: 4238.0840, Recon Loss: 8476.1662, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 670/1000, Beta: 5.00, Total Loss: 4238.0839, Recon Loss: 8476.1660, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 671/1000, Beta: 5.00, Total Loss: 4238.0849, Recon Loss: 8476.1679, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 672/1000, Beta: 5.00, Total Loss: 4238.0829, Recon Loss: 8476.1641, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 673/1000, Beta: 5.00, Total Loss: 4238.0820, Recon Loss: 8476.1621, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 674/1000, Beta: 5.00, Total Loss: 4238.0819, Recon Loss: 8476.1620, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 675/1000, Beta: 5.00, Total Loss: 4238.0823, Recon Loss: 8476.1629, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 676/1000, Beta: 5.00, Total Loss: 4238.0818, Recon Loss: 8476.1618, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 677/1000, Beta: 5.00, Total Loss: 4238.0809, Recon Loss: 8476.1602, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 678/1000, Beta: 5.00, Total Loss: 4238.0819, Recon Loss: 8476.1621, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 679/1000, Beta: 5.00, Total Loss: 4238.0813, Recon Loss: 8476.1609, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 680/1000, Beta: 5.00, Total Loss: 4238.0835, Recon Loss: 8476.1651, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 681/1000, Beta: 5.00, Total Loss: 4238.0816, Recon Loss: 8476.1614, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 682/1000, Beta: 5.00, Total Loss: 4238.0796, Recon Loss: 8476.1574, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 683/1000, Beta: 5.00, Total Loss: 4238.0784, Recon Loss: 8476.1551, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 684/1000, Beta: 5.00, Total Loss: 4238.0790, Recon Loss: 8476.1562, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 685/1000, Beta: 5.00, Total Loss: 4238.0794, Recon Loss: 8476.1571, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 686/1000, Beta: 5.00, Total Loss: 4238.0777, Recon Loss: 8476.1536, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 687/1000, Beta: 5.00, Total Loss: 4238.0788, Recon Loss: 8476.1559, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 688/1000, Beta: 5.00, Total Loss: 4238.0783, Recon Loss: 8476.1548, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 689/1000, Beta: 5.00, Total Loss: 4238.0775, Recon Loss: 8476.1532, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 690/1000, Beta: 5.00, Total Loss: 4238.0802, Recon Loss: 8476.1586, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 691/1000, Beta: 5.00, Total Loss: 4238.0774, Recon Loss: 8476.1531, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 692/1000, Beta: 5.00, Total Loss: 4238.0788, Recon Loss: 8476.1557, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 693/1000, Beta: 5.00, Total Loss: 4238.0761, Recon Loss: 8476.1506, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 694/1000, Beta: 5.00, Total Loss: 4238.0754, Recon Loss: 8476.1491, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 695/1000, Beta: 5.00, Total Loss: 4238.0771, Recon Loss: 8476.1526, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 696/1000, Beta: 5.00, Total Loss: 4238.0762, Recon Loss: 8476.1506, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 697/1000, Beta: 5.00, Total Loss: 4238.0758, Recon Loss: 8476.1500, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 698/1000, Beta: 5.00, Total Loss: 4238.0760, Recon Loss: 8476.1503, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 699/1000, Beta: 5.00, Total Loss: 4238.0759, Recon Loss: 8476.1501, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 700/1000, Beta: 5.00, Total Loss: 4238.0750, Recon Loss: 8476.1482, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 701/1000, Beta: 5.00, Total Loss: 4238.0770, Recon Loss: 8476.1522, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 702/1000, Beta: 5.00, Total Loss: 4238.0775, Recon Loss: 8476.1533, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 703/1000, Beta: 5.00, Total Loss: 4238.0746, Recon Loss: 8476.1475, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 704/1000, Beta: 5.00, Total Loss: 4238.0737, Recon Loss: 8476.1456, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 705/1000, Beta: 5.00, Total Loss: 4238.0749, Recon Loss: 8476.1481, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 706/1000, Beta: 5.00, Total Loss: 4238.0742, Recon Loss: 8476.1467, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 707/1000, Beta: 5.00, Total Loss: 4238.0737, Recon Loss: 8476.1457, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 708/1000, Beta: 5.00, Total Loss: 4238.0715, Recon Loss: 8476.1413, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 709/1000, Beta: 5.00, Total Loss: 4238.0720, Recon Loss: 8476.1422, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 710/1000, Beta: 5.00, Total Loss: 4238.0719, Recon Loss: 8476.1421, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 711/1000, Beta: 5.00, Total Loss: 4238.0710, Recon Loss: 8476.1402, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 712/1000, Beta: 5.00, Total Loss: 4238.0737, Recon Loss: 8476.1457, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 713/1000, Beta: 5.00, Total Loss: 4238.0745, Recon Loss: 8476.1472, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 714/1000, Beta: 5.00, Total Loss: 4238.0750, Recon Loss: 8476.1483, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 715/1000, Beta: 5.00, Total Loss: 4238.0710, Recon Loss: 8476.1403, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 716/1000, Beta: 5.00, Total Loss: 4238.0706, Recon Loss: 8476.1394, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 717/1000, Beta: 5.00, Total Loss: 4238.0719, Recon Loss: 8476.1423, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 718/1000, Beta: 5.00, Total Loss: 4238.0707, Recon Loss: 8476.1396, KL Loss: 0.0000, Percep Loss: 0.0009\n",
      "Epoch 719/1000, Beta: 5.00, Total Loss: 4238.0688, Recon Loss: 8476.1360, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 720/1000, Beta: 5.00, Total Loss: 4238.0693, Recon Loss: 8476.1369, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 721/1000, Beta: 5.00, Total Loss: 4238.0703, Recon Loss: 8476.1389, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 722/1000, Beta: 5.00, Total Loss: 4238.0711, Recon Loss: 8476.1406, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 723/1000, Beta: 5.00, Total Loss: 4238.0695, Recon Loss: 8476.1374, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 724/1000, Beta: 5.00, Total Loss: 4238.0687, Recon Loss: 8476.1357, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 725/1000, Beta: 5.00, Total Loss: 4238.0683, Recon Loss: 8476.1348, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 726/1000, Beta: 5.00, Total Loss: 4238.0683, Recon Loss: 8476.1350, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 727/1000, Beta: 5.00, Total Loss: 4238.0684, Recon Loss: 8476.1350, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 728/1000, Beta: 5.00, Total Loss: 4238.0674, Recon Loss: 8476.1331, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 729/1000, Beta: 5.00, Total Loss: 4238.0691, Recon Loss: 8476.1366, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 730/1000, Beta: 5.00, Total Loss: 4238.0663, Recon Loss: 8476.1310, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 731/1000, Beta: 5.00, Total Loss: 4238.0661, Recon Loss: 8476.1304, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 732/1000, Beta: 5.00, Total Loss: 4238.0701, Recon Loss: 8476.1385, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 733/1000, Beta: 5.00, Total Loss: 4238.0694, Recon Loss: 8476.1370, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 734/1000, Beta: 5.00, Total Loss: 4238.0662, Recon Loss: 8476.1306, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 735/1000, Beta: 5.00, Total Loss: 4238.0650, Recon Loss: 8476.1283, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 736/1000, Beta: 5.00, Total Loss: 4238.0698, Recon Loss: 8476.1378, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 737/1000, Beta: 5.00, Total Loss: 4238.0678, Recon Loss: 8476.1340, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 738/1000, Beta: 5.00, Total Loss: 4238.0652, Recon Loss: 8476.1288, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 739/1000, Beta: 5.00, Total Loss: 4238.0648, Recon Loss: 8476.1279, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 740/1000, Beta: 5.00, Total Loss: 4238.0641, Recon Loss: 8476.1265, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 741/1000, Beta: 5.00, Total Loss: 4238.0645, Recon Loss: 8476.1273, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 742/1000, Beta: 5.00, Total Loss: 4238.0645, Recon Loss: 8476.1272, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 743/1000, Beta: 5.00, Total Loss: 4238.0645, Recon Loss: 8476.1272, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 744/1000, Beta: 5.00, Total Loss: 4238.0631, Recon Loss: 8476.1246, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 745/1000, Beta: 5.00, Total Loss: 4238.0654, Recon Loss: 8476.1291, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 746/1000, Beta: 5.00, Total Loss: 4238.0656, Recon Loss: 8476.1297, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 747/1000, Beta: 5.00, Total Loss: 4238.0621, Recon Loss: 8476.1226, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 748/1000, Beta: 5.00, Total Loss: 4238.0624, Recon Loss: 8476.1231, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 749/1000, Beta: 5.00, Total Loss: 4238.0652, Recon Loss: 8476.1287, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 750/1000, Beta: 5.00, Total Loss: 4238.0615, Recon Loss: 8476.1214, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 751/1000, Beta: 5.00, Total Loss: 4238.0629, Recon Loss: 8476.1240, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 752/1000, Beta: 5.00, Total Loss: 4238.0628, Recon Loss: 8476.1240, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 753/1000, Beta: 5.00, Total Loss: 4238.0611, Recon Loss: 8476.1205, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 754/1000, Beta: 5.00, Total Loss: 4238.0620, Recon Loss: 8476.1224, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 755/1000, Beta: 5.00, Total Loss: 4238.0610, Recon Loss: 8476.1203, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 756/1000, Beta: 5.00, Total Loss: 4238.0623, Recon Loss: 8476.1229, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 757/1000, Beta: 5.00, Total Loss: 4238.0608, Recon Loss: 8476.1200, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 758/1000, Beta: 5.00, Total Loss: 4238.0607, Recon Loss: 8476.1197, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 759/1000, Beta: 5.00, Total Loss: 4238.0624, Recon Loss: 8476.1233, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 760/1000, Beta: 5.00, Total Loss: 4238.0604, Recon Loss: 8476.1192, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 761/1000, Beta: 5.00, Total Loss: 4238.0616, Recon Loss: 8476.1216, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 762/1000, Beta: 5.00, Total Loss: 4238.0611, Recon Loss: 8476.1205, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 763/1000, Beta: 5.00, Total Loss: 4238.0605, Recon Loss: 8476.1193, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 764/1000, Beta: 5.00, Total Loss: 4238.0590, Recon Loss: 8476.1165, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 765/1000, Beta: 5.00, Total Loss: 4238.0604, Recon Loss: 8476.1193, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 766/1000, Beta: 5.00, Total Loss: 4238.0576, Recon Loss: 8476.1134, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 767/1000, Beta: 5.00, Total Loss: 4238.0588, Recon Loss: 8476.1160, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 768/1000, Beta: 5.00, Total Loss: 4238.0572, Recon Loss: 8476.1129, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 769/1000, Beta: 5.00, Total Loss: 4238.0604, Recon Loss: 8476.1191, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 770/1000, Beta: 5.00, Total Loss: 4238.0588, Recon Loss: 8476.1160, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 771/1000, Beta: 5.00, Total Loss: 4238.0566, Recon Loss: 8476.1115, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 772/1000, Beta: 5.00, Total Loss: 4238.0577, Recon Loss: 8476.1136, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 773/1000, Beta: 5.00, Total Loss: 4238.0589, Recon Loss: 8476.1162, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 774/1000, Beta: 5.00, Total Loss: 4238.0589, Recon Loss: 8476.1162, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 775/1000, Beta: 5.00, Total Loss: 4238.0564, Recon Loss: 8476.1112, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 776/1000, Beta: 5.00, Total Loss: 4238.0560, Recon Loss: 8476.1103, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 777/1000, Beta: 5.00, Total Loss: 4238.0563, Recon Loss: 8476.1110, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 778/1000, Beta: 5.00, Total Loss: 4238.0550, Recon Loss: 8476.1084, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 779/1000, Beta: 5.00, Total Loss: 4238.0587, Recon Loss: 8476.1156, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 780/1000, Beta: 5.00, Total Loss: 4238.0549, Recon Loss: 8476.1081, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 781/1000, Beta: 5.00, Total Loss: 4238.0549, Recon Loss: 8476.1082, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 782/1000, Beta: 5.00, Total Loss: 4238.0593, Recon Loss: 8476.1168, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 783/1000, Beta: 5.00, Total Loss: 4238.0569, Recon Loss: 8476.1123, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 784/1000, Beta: 5.00, Total Loss: 4238.0548, Recon Loss: 8476.1080, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 785/1000, Beta: 5.00, Total Loss: 4238.0556, Recon Loss: 8476.1096, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 786/1000, Beta: 5.00, Total Loss: 4238.0548, Recon Loss: 8476.1079, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 787/1000, Beta: 5.00, Total Loss: 4238.0548, Recon Loss: 8476.1080, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 788/1000, Beta: 5.00, Total Loss: 4238.0534, Recon Loss: 8476.1052, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 789/1000, Beta: 5.00, Total Loss: 4238.0539, Recon Loss: 8476.1061, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 790/1000, Beta: 5.00, Total Loss: 4238.0530, Recon Loss: 8476.1045, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 791/1000, Beta: 5.00, Total Loss: 4238.0534, Recon Loss: 8476.1053, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 792/1000, Beta: 5.00, Total Loss: 4238.0526, Recon Loss: 8476.1036, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 793/1000, Beta: 5.00, Total Loss: 4238.0532, Recon Loss: 8476.1048, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 794/1000, Beta: 5.00, Total Loss: 4238.0527, Recon Loss: 8476.1037, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 795/1000, Beta: 5.00, Total Loss: 4238.0534, Recon Loss: 8476.1052, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 796/1000, Beta: 5.00, Total Loss: 4238.0521, Recon Loss: 8476.1026, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 797/1000, Beta: 5.00, Total Loss: 4238.0520, Recon Loss: 8476.1023, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 798/1000, Beta: 5.00, Total Loss: 4238.0512, Recon Loss: 8476.1009, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 799/1000, Beta: 5.00, Total Loss: 4238.0527, Recon Loss: 8476.1037, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 800/1000, Beta: 5.00, Total Loss: 4238.0513, Recon Loss: 8476.1011, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 801/1000, Beta: 5.00, Total Loss: 4238.0481, Recon Loss: 8476.0946, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 802/1000, Beta: 5.00, Total Loss: 4238.0470, Recon Loss: 8476.0926, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 803/1000, Beta: 5.00, Total Loss: 4238.0460, Recon Loss: 8476.0905, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 804/1000, Beta: 5.00, Total Loss: 4238.0464, Recon Loss: 8476.0912, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 805/1000, Beta: 5.00, Total Loss: 4238.0466, Recon Loss: 8476.0915, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 806/1000, Beta: 5.00, Total Loss: 4238.0454, Recon Loss: 8476.0892, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 807/1000, Beta: 5.00, Total Loss: 4238.0451, Recon Loss: 8476.0886, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 808/1000, Beta: 5.00, Total Loss: 4238.0449, Recon Loss: 8476.0883, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 809/1000, Beta: 5.00, Total Loss: 4238.0450, Recon Loss: 8476.0884, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 810/1000, Beta: 5.00, Total Loss: 4238.0445, Recon Loss: 8476.0873, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 811/1000, Beta: 5.00, Total Loss: 4238.0448, Recon Loss: 8476.0880, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 812/1000, Beta: 5.00, Total Loss: 4238.0441, Recon Loss: 8476.0868, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 813/1000, Beta: 5.00, Total Loss: 4238.0434, Recon Loss: 8476.0853, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 814/1000, Beta: 5.00, Total Loss: 4238.0439, Recon Loss: 8476.0862, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 815/1000, Beta: 5.00, Total Loss: 4238.0432, Recon Loss: 8476.0849, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 816/1000, Beta: 5.00, Total Loss: 4238.0430, Recon Loss: 8476.0844, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 817/1000, Beta: 5.00, Total Loss: 4238.0431, Recon Loss: 8476.0847, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 818/1000, Beta: 5.00, Total Loss: 4238.0429, Recon Loss: 8476.0843, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 819/1000, Beta: 5.00, Total Loss: 4238.0427, Recon Loss: 8476.0839, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 820/1000, Beta: 5.00, Total Loss: 4238.0423, Recon Loss: 8476.0830, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 821/1000, Beta: 5.00, Total Loss: 4238.0426, Recon Loss: 8476.0837, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 822/1000, Beta: 5.00, Total Loss: 4238.0423, Recon Loss: 8476.0831, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 823/1000, Beta: 5.00, Total Loss: 4238.0423, Recon Loss: 8476.0830, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 824/1000, Beta: 5.00, Total Loss: 4238.0416, Recon Loss: 8476.0817, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 825/1000, Beta: 5.00, Total Loss: 4238.0416, Recon Loss: 8476.0818, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 826/1000, Beta: 5.00, Total Loss: 4238.0414, Recon Loss: 8476.0813, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 827/1000, Beta: 5.00, Total Loss: 4238.0418, Recon Loss: 8476.0820, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 828/1000, Beta: 5.00, Total Loss: 4238.0414, Recon Loss: 8476.0814, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 829/1000, Beta: 5.00, Total Loss: 4238.0408, Recon Loss: 8476.0801, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 830/1000, Beta: 5.00, Total Loss: 4238.0411, Recon Loss: 8476.0806, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 831/1000, Beta: 5.00, Total Loss: 4238.0403, Recon Loss: 8476.0791, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 832/1000, Beta: 5.00, Total Loss: 4238.0404, Recon Loss: 8476.0794, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 833/1000, Beta: 5.00, Total Loss: 4238.0400, Recon Loss: 8476.0785, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 834/1000, Beta: 5.00, Total Loss: 4238.0400, Recon Loss: 8476.0785, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 835/1000, Beta: 5.00, Total Loss: 4238.0400, Recon Loss: 8476.0785, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 836/1000, Beta: 5.00, Total Loss: 4238.0399, Recon Loss: 8476.0782, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 837/1000, Beta: 5.00, Total Loss: 4238.0394, Recon Loss: 8476.0772, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 838/1000, Beta: 5.00, Total Loss: 4238.0391, Recon Loss: 8476.0768, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 839/1000, Beta: 5.00, Total Loss: 4238.0394, Recon Loss: 8476.0773, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 840/1000, Beta: 5.00, Total Loss: 4238.0387, Recon Loss: 8476.0760, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 841/1000, Beta: 5.00, Total Loss: 4238.0392, Recon Loss: 8476.0769, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 842/1000, Beta: 5.00, Total Loss: 4238.0390, Recon Loss: 8476.0765, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 843/1000, Beta: 5.00, Total Loss: 4238.0385, Recon Loss: 8476.0756, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 844/1000, Beta: 5.00, Total Loss: 4238.0382, Recon Loss: 8476.0748, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 845/1000, Beta: 5.00, Total Loss: 4238.0383, Recon Loss: 8476.0752, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 846/1000, Beta: 5.00, Total Loss: 4238.0381, Recon Loss: 8476.0746, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 847/1000, Beta: 5.00, Total Loss: 4238.0377, Recon Loss: 8476.0738, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 848/1000, Beta: 5.00, Total Loss: 4238.0380, Recon Loss: 8476.0744, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 849/1000, Beta: 5.00, Total Loss: 4238.0369, Recon Loss: 8476.0724, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 850/1000, Beta: 5.00, Total Loss: 4238.0370, Recon Loss: 8476.0725, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 851/1000, Beta: 5.00, Total Loss: 4238.0370, Recon Loss: 8476.0726, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 852/1000, Beta: 5.00, Total Loss: 4238.0368, Recon Loss: 8476.0722, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 853/1000, Beta: 5.00, Total Loss: 4238.0368, Recon Loss: 8476.0720, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 854/1000, Beta: 5.00, Total Loss: 4238.0364, Recon Loss: 8476.0713, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 855/1000, Beta: 5.00, Total Loss: 4238.0364, Recon Loss: 8476.0713, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 856/1000, Beta: 5.00, Total Loss: 4238.0367, Recon Loss: 8476.0719, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 857/1000, Beta: 5.00, Total Loss: 4238.0360, Recon Loss: 8476.0705, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 858/1000, Beta: 5.00, Total Loss: 4238.0357, Recon Loss: 8476.0699, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 859/1000, Beta: 5.00, Total Loss: 4238.0351, Recon Loss: 8476.0687, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 860/1000, Beta: 5.00, Total Loss: 4238.0354, Recon Loss: 8476.0693, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 861/1000, Beta: 5.00, Total Loss: 4238.0352, Recon Loss: 8476.0688, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 862/1000, Beta: 5.00, Total Loss: 4238.0351, Recon Loss: 8476.0688, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 863/1000, Beta: 5.00, Total Loss: 4238.0348, Recon Loss: 8476.0681, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 864/1000, Beta: 5.00, Total Loss: 4238.0350, Recon Loss: 8476.0686, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 865/1000, Beta: 5.00, Total Loss: 4238.0342, Recon Loss: 8476.0669, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 866/1000, Beta: 5.00, Total Loss: 4238.0342, Recon Loss: 8476.0671, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 867/1000, Beta: 5.00, Total Loss: 4238.0343, Recon Loss: 8476.0671, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 868/1000, Beta: 5.00, Total Loss: 4238.0339, Recon Loss: 8476.0663, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 869/1000, Beta: 5.00, Total Loss: 4238.0337, Recon Loss: 8476.0659, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 870/1000, Beta: 5.00, Total Loss: 4238.0338, Recon Loss: 8476.0661, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 871/1000, Beta: 5.00, Total Loss: 4238.0331, Recon Loss: 8476.0648, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 872/1000, Beta: 5.00, Total Loss: 4238.0333, Recon Loss: 8476.0652, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 873/1000, Beta: 5.00, Total Loss: 4238.0332, Recon Loss: 8476.0648, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 874/1000, Beta: 5.00, Total Loss: 4238.0329, Recon Loss: 8476.0643, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 875/1000, Beta: 5.00, Total Loss: 4238.0326, Recon Loss: 8476.0638, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 876/1000, Beta: 5.00, Total Loss: 4238.0332, Recon Loss: 8476.0650, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 877/1000, Beta: 5.00, Total Loss: 4238.0326, Recon Loss: 8476.0635, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 878/1000, Beta: 5.00, Total Loss: 4238.0323, Recon Loss: 8476.0631, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 879/1000, Beta: 5.00, Total Loss: 4238.0320, Recon Loss: 8476.0626, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 880/1000, Beta: 5.00, Total Loss: 4238.0323, Recon Loss: 8476.0632, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 881/1000, Beta: 5.00, Total Loss: 4238.0318, Recon Loss: 8476.0621, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 882/1000, Beta: 5.00, Total Loss: 4238.0316, Recon Loss: 8476.0618, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 883/1000, Beta: 5.00, Total Loss: 4238.0313, Recon Loss: 8476.0612, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 884/1000, Beta: 5.00, Total Loss: 4238.0318, Recon Loss: 8476.0621, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 885/1000, Beta: 5.00, Total Loss: 4238.0310, Recon Loss: 8476.0605, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 886/1000, Beta: 5.00, Total Loss: 4238.0311, Recon Loss: 8476.0608, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 887/1000, Beta: 5.00, Total Loss: 4238.0309, Recon Loss: 8476.0603, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 888/1000, Beta: 5.00, Total Loss: 4238.0309, Recon Loss: 8476.0604, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 889/1000, Beta: 5.00, Total Loss: 4238.0303, Recon Loss: 8476.0592, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 890/1000, Beta: 5.00, Total Loss: 4238.0304, Recon Loss: 8476.0593, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 891/1000, Beta: 5.00, Total Loss: 4238.0303, Recon Loss: 8476.0591, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 892/1000, Beta: 5.00, Total Loss: 4238.0298, Recon Loss: 8476.0582, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 893/1000, Beta: 5.00, Total Loss: 4238.0305, Recon Loss: 8476.0596, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 894/1000, Beta: 5.00, Total Loss: 4238.0297, Recon Loss: 8476.0578, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 895/1000, Beta: 5.00, Total Loss: 4238.0299, Recon Loss: 8476.0582, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 896/1000, Beta: 5.00, Total Loss: 4238.0297, Recon Loss: 8476.0580, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 897/1000, Beta: 5.00, Total Loss: 4238.0293, Recon Loss: 8476.0571, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 898/1000, Beta: 5.00, Total Loss: 4238.0295, Recon Loss: 8476.0577, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 899/1000, Beta: 5.00, Total Loss: 4238.0293, Recon Loss: 8476.0572, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 900/1000, Beta: 5.00, Total Loss: 4238.0290, Recon Loss: 8476.0566, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 901/1000, Beta: 5.00, Total Loss: 4238.0282, Recon Loss: 8476.0549, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 902/1000, Beta: 5.00, Total Loss: 4238.0281, Recon Loss: 8476.0547, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 903/1000, Beta: 5.00, Total Loss: 4238.0288, Recon Loss: 8476.0562, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 904/1000, Beta: 5.00, Total Loss: 4238.0282, Recon Loss: 8476.0551, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 905/1000, Beta: 5.00, Total Loss: 4238.0282, Recon Loss: 8476.0548, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 906/1000, Beta: 5.00, Total Loss: 4238.0279, Recon Loss: 8476.0543, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 907/1000, Beta: 5.00, Total Loss: 4238.0287, Recon Loss: 8476.0559, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 908/1000, Beta: 5.00, Total Loss: 4238.0278, Recon Loss: 8476.0543, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 909/1000, Beta: 5.00, Total Loss: 4238.0274, Recon Loss: 8476.0533, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 910/1000, Beta: 5.00, Total Loss: 4238.0270, Recon Loss: 8476.0527, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 911/1000, Beta: 5.00, Total Loss: 4238.0269, Recon Loss: 8476.0524, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 912/1000, Beta: 5.00, Total Loss: 4238.0268, Recon Loss: 8476.0521, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 913/1000, Beta: 5.00, Total Loss: 4238.0267, Recon Loss: 8476.0520, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 914/1000, Beta: 5.00, Total Loss: 4238.0263, Recon Loss: 8476.0513, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 915/1000, Beta: 5.00, Total Loss: 4238.0261, Recon Loss: 8476.0508, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 916/1000, Beta: 5.00, Total Loss: 4238.0263, Recon Loss: 8476.0511, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 917/1000, Beta: 5.00, Total Loss: 4238.0260, Recon Loss: 8476.0506, KL Loss: 0.0000, Percep Loss: 0.0008\n",
      "Epoch 918/1000, Beta: 5.00, Total Loss: 4238.0263, Recon Loss: 8476.0511, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 919/1000, Beta: 5.00, Total Loss: 4238.0257, Recon Loss: 8476.0500, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 920/1000, Beta: 5.00, Total Loss: 4238.0262, Recon Loss: 8476.0512, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 921/1000, Beta: 5.00, Total Loss: 4238.0255, Recon Loss: 8476.0495, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 922/1000, Beta: 5.00, Total Loss: 4238.0251, Recon Loss: 8476.0488, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 923/1000, Beta: 5.00, Total Loss: 4238.0246, Recon Loss: 8476.0479, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 924/1000, Beta: 5.00, Total Loss: 4238.0249, Recon Loss: 8476.0484, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 925/1000, Beta: 5.00, Total Loss: 4238.0246, Recon Loss: 8476.0479, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 926/1000, Beta: 5.00, Total Loss: 4238.0246, Recon Loss: 8476.0480, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 927/1000, Beta: 5.00, Total Loss: 4238.0248, Recon Loss: 8476.0483, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 928/1000, Beta: 5.00, Total Loss: 4238.0240, Recon Loss: 8476.0467, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 929/1000, Beta: 5.00, Total Loss: 4238.0244, Recon Loss: 8476.0474, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 930/1000, Beta: 5.00, Total Loss: 4238.0242, Recon Loss: 8476.0470, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 931/1000, Beta: 5.00, Total Loss: 4238.0239, Recon Loss: 8476.0465, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 932/1000, Beta: 5.00, Total Loss: 4238.0236, Recon Loss: 8476.0458, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 933/1000, Beta: 5.00, Total Loss: 4238.0235, Recon Loss: 8476.0457, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 934/1000, Beta: 5.00, Total Loss: 4238.0238, Recon Loss: 8476.0461, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 935/1000, Beta: 5.00, Total Loss: 4238.0229, Recon Loss: 8476.0445, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 936/1000, Beta: 5.00, Total Loss: 4238.0230, Recon Loss: 8476.0447, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 937/1000, Beta: 5.00, Total Loss: 4238.0230, Recon Loss: 8476.0447, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 938/1000, Beta: 5.00, Total Loss: 4238.0226, Recon Loss: 8476.0438, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 939/1000, Beta: 5.00, Total Loss: 4238.0224, Recon Loss: 8476.0435, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 940/1000, Beta: 5.00, Total Loss: 4238.0222, Recon Loss: 8476.0429, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 941/1000, Beta: 5.00, Total Loss: 4238.0225, Recon Loss: 8476.0436, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 942/1000, Beta: 5.00, Total Loss: 4238.0220, Recon Loss: 8476.0425, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 943/1000, Beta: 5.00, Total Loss: 4238.0222, Recon Loss: 8476.0430, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 944/1000, Beta: 5.00, Total Loss: 4238.0215, Recon Loss: 8476.0417, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 945/1000, Beta: 5.00, Total Loss: 4238.0222, Recon Loss: 8476.0429, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 946/1000, Beta: 5.00, Total Loss: 4238.0216, Recon Loss: 8476.0419, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 947/1000, Beta: 5.00, Total Loss: 4238.0214, Recon Loss: 8476.0414, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 948/1000, Beta: 5.00, Total Loss: 4238.0209, Recon Loss: 8476.0405, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 949/1000, Beta: 5.00, Total Loss: 4238.0220, Recon Loss: 8476.0426, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 950/1000, Beta: 5.00, Total Loss: 4238.0207, Recon Loss: 8476.0400, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 951/1000, Beta: 5.00, Total Loss: 4238.0211, Recon Loss: 8476.0408, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 952/1000, Beta: 5.00, Total Loss: 4238.0200, Recon Loss: 8476.0387, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 953/1000, Beta: 5.00, Total Loss: 4238.0203, Recon Loss: 8476.0393, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 954/1000, Beta: 5.00, Total Loss: 4238.0203, Recon Loss: 8476.0392, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 955/1000, Beta: 5.00, Total Loss: 4238.0203, Recon Loss: 8476.0392, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 956/1000, Beta: 5.00, Total Loss: 4238.0201, Recon Loss: 8476.0389, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 957/1000, Beta: 5.00, Total Loss: 4238.0207, Recon Loss: 8476.0401, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 958/1000, Beta: 5.00, Total Loss: 4238.0198, Recon Loss: 8476.0383, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 959/1000, Beta: 5.00, Total Loss: 4238.0194, Recon Loss: 8476.0375, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 960/1000, Beta: 5.00, Total Loss: 4238.0194, Recon Loss: 8476.0375, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 961/1000, Beta: 5.00, Total Loss: 4238.0194, Recon Loss: 8476.0375, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 962/1000, Beta: 5.00, Total Loss: 4238.0189, Recon Loss: 8476.0364, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 963/1000, Beta: 5.00, Total Loss: 4238.0187, Recon Loss: 8476.0361, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 964/1000, Beta: 5.00, Total Loss: 4238.0189, Recon Loss: 8476.0364, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 965/1000, Beta: 5.00, Total Loss: 4238.0184, Recon Loss: 8476.0354, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 966/1000, Beta: 5.00, Total Loss: 4238.0186, Recon Loss: 8476.0357, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 967/1000, Beta: 5.00, Total Loss: 4238.0182, Recon Loss: 8476.0350, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 968/1000, Beta: 5.00, Total Loss: 4238.0182, Recon Loss: 8476.0351, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 969/1000, Beta: 5.00, Total Loss: 4238.0179, Recon Loss: 8476.0344, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 970/1000, Beta: 5.00, Total Loss: 4238.0182, Recon Loss: 8476.0350, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 971/1000, Beta: 5.00, Total Loss: 4238.0177, Recon Loss: 8476.0341, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 972/1000, Beta: 5.00, Total Loss: 4238.0177, Recon Loss: 8476.0341, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 973/1000, Beta: 5.00, Total Loss: 4238.0179, Recon Loss: 8476.0344, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 974/1000, Beta: 5.00, Total Loss: 4238.0174, Recon Loss: 8476.0334, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 975/1000, Beta: 5.00, Total Loss: 4238.0169, Recon Loss: 8476.0325, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 976/1000, Beta: 5.00, Total Loss: 4238.0172, Recon Loss: 8476.0330, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 977/1000, Beta: 5.00, Total Loss: 4238.0168, Recon Loss: 8476.0322, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 978/1000, Beta: 5.00, Total Loss: 4238.0163, Recon Loss: 8476.0312, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 979/1000, Beta: 5.00, Total Loss: 4238.0159, Recon Loss: 8476.0305, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 980/1000, Beta: 5.00, Total Loss: 4238.0168, Recon Loss: 8476.0324, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 981/1000, Beta: 5.00, Total Loss: 4238.0163, Recon Loss: 8476.0313, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 982/1000, Beta: 5.00, Total Loss: 4238.0158, Recon Loss: 8476.0303, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 983/1000, Beta: 5.00, Total Loss: 4238.0160, Recon Loss: 8476.0306, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 984/1000, Beta: 5.00, Total Loss: 4238.0160, Recon Loss: 8476.0306, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 985/1000, Beta: 5.00, Total Loss: 4238.0153, Recon Loss: 8476.0293, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 986/1000, Beta: 5.00, Total Loss: 4238.0152, Recon Loss: 8476.0292, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 987/1000, Beta: 5.00, Total Loss: 4238.0148, Recon Loss: 8476.0282, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 988/1000, Beta: 5.00, Total Loss: 4238.0150, Recon Loss: 8476.0287, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 989/1000, Beta: 5.00, Total Loss: 4238.0145, Recon Loss: 8476.0278, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 990/1000, Beta: 5.00, Total Loss: 4238.0150, Recon Loss: 8476.0286, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 991/1000, Beta: 5.00, Total Loss: 4238.0155, Recon Loss: 8476.0296, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 992/1000, Beta: 5.00, Total Loss: 4238.0150, Recon Loss: 8476.0287, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 993/1000, Beta: 5.00, Total Loss: 4238.0140, Recon Loss: 8476.0267, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 994/1000, Beta: 5.00, Total Loss: 4238.0140, Recon Loss: 8476.0268, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 995/1000, Beta: 5.00, Total Loss: 4238.0141, Recon Loss: 8476.0268, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 996/1000, Beta: 5.00, Total Loss: 4238.0142, Recon Loss: 8476.0269, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 997/1000, Beta: 5.00, Total Loss: 4238.0138, Recon Loss: 8476.0262, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 998/1000, Beta: 5.00, Total Loss: 4238.0139, Recon Loss: 8476.0265, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 999/1000, Beta: 5.00, Total Loss: 4238.0141, Recon Loss: 8476.0268, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Epoch 1000/1000, Beta: 5.00, Total Loss: 4238.0136, Recon Loss: 8476.0258, KL Loss: 0.0000, Percep Loss: 0.0007\n",
      "Saved final checkpoint at epoch 1000 to FL_CVAE\\cvae_vehicle_final.pth\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 327\u001b[0m\n\u001b[0;32m    325\u001b[0m             sample \u001b[38;5;241m=\u001b[39m adjust_sharpness(sample, sharpness_factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m)\n\u001b[0;32m    326\u001b[0m             save_image(sample, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(class_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m--> 327\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples for Class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39mclass_names[class_label]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) at final epoch.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# Create and save plot\u001b[39;00m\n\u001b[0;32m    330\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;28mlen\u001b[39m(classes_to_generate), \u001b[38;5;241m10\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms.functional import adjust_sharpness\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import kagglehub\n",
    "from PIL import Image\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Set CUDA_LAUNCH_BLOCKING for debugging\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "latent_dim = 128\n",
    "num_classes = 5\n",
    "batch_size = 16\n",
    "epochs = 1000\n",
    "learning_rate = 5e-4\n",
    "image_size = 128\n",
    "channels = 1\n",
    "output_dir = \"FL_CVAE\"\n",
    "beta_max = 5.0\n",
    "annealing_epochs = 50\n",
    "perceptual_weight = 1.0\n",
    "recon_weight = 0.5\n",
    "\n",
    "# Download Vehicle Type Image Dataset from Kaggle\n",
    "try:\n",
    "    path = kagglehub.dataset_download(\"sujaykapadnis/vehicle-type-image-dataset\")\n",
    "    print(\"Path to dataset files:\", path)\n",
    "    dataset_path = path\n",
    "except Exception as e:\n",
    "    print(f\"Failed to download dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Define the VehicleTypeDataset class with a limit\n",
    "class VehicleTypeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None, max_images=1000):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_names = []\n",
    "        self.class_to_idx = {}\n",
    "\n",
    "        print(f\"Searching for images in {root_dir}\")\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            image_files = [f for f in files if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "            if image_files:\n",
    "                class_name = os.path.basename(root)\n",
    "                if class_name not in self.class_to_idx:\n",
    "                    self.class_names.append(class_name)\n",
    "                    self.class_to_idx[class_name] = len(self.class_names) - 1\n",
    "                for img_file in image_files:\n",
    "                    img_path = os.path.join(root, img_file)\n",
    "                    self.images.append(img_path)\n",
    "                    self.labels.append(self.class_to_idx[class_name])\n",
    "                    if len(self.images) >= max_images:\n",
    "                        break\n",
    "                if len(self.images) >= max_images:\n",
    "                    break\n",
    "\n",
    "        if not self.images:\n",
    "            raise ValueError(\n",
    "                f\"No images found in {root_dir}. \"\n",
    "                \"Expected class folders containing .jpg, .png, or .jpeg images.\"\n",
    "            )\n",
    "\n",
    "        print(f\"Found {len(self.images)} images across {len(self.class_names)} classes.\")\n",
    "        print(f\"Classes: {self.class_names}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"L\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the dataset with a limit\n",
    "dataset = VehicleTypeDataset(root_dir=dataset_path, transform=transform, max_images=1000)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define the Encoder network\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels + num_classes, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1)\n",
    "        self.fc_mean = nn.Linear(512 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(512 * 4 * 4, latent_dim)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        y = F.one_hot(y, num_classes=num_classes).float()\n",
    "        y = y.unsqueeze(-1).unsqueeze(-1)\n",
    "        y = y.expand(-1, -1, x.size(2), x.size(3))\n",
    "        x_with_y = torch.cat([x, y], dim=1)\n",
    "        \n",
    "        h1 = F.relu(self.conv1(x_with_y))\n",
    "        if torch.isnan(h1).any() or torch.isinf(h1).any():\n",
    "            print(\"NaN or Inf in h1\")\n",
    "        h2 = F.relu(self.conv2(h1))\n",
    "        if torch.isnan(h2).any() or torch.isinf(h2).any():\n",
    "            print(\"NaN or Inf in h2\")\n",
    "        h3 = F.relu(self.conv3(h2))\n",
    "        if torch.isnan(h3).any() or torch.isinf(h3).any():\n",
    "            print(\"NaN or Inf in h3\")\n",
    "        h4 = F.relu(self.conv4(h3))\n",
    "        if torch.isnan(h4).any() or torch.isinf(h4).any():\n",
    "            print(\"NaN or Inf in h4\")\n",
    "        h5 = F.relu(self.conv5(h4))\n",
    "        if torch.isnan(h5).any() or torch.isinf(h5).any():\n",
    "            print(\"NaN or Inf in h5\")\n",
    "        h = h5.view(h5.size(0), -1)\n",
    "        z_mean = self.fc_mean(h)\n",
    "        z_logvar = self.fc_logvar(h)\n",
    "        return z_mean, z_logvar, (h1, h2, h3, h4, h5)\n",
    "\n",
    "# Define the Decoder network\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim + num_classes, 512 * 4 * 4)\n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(512, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(256, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(128, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv5 = nn.ConvTranspose2d(64, channels, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, z, y, skip_connections):\n",
    "        h1, h2, h3, h4, h5 = skip_connections\n",
    "        y = F.one_hot(y, num_classes=num_classes).float()\n",
    "        z_with_y = torch.cat([z, y], dim=-1)\n",
    "        h = F.relu(self.fc(z_with_y))\n",
    "        h = h.view(h.size(0), 512, 4, 4)\n",
    "        \n",
    "        h = F.relu(self.deconv1(h))\n",
    "        h = torch.cat([h, h4], dim=1)\n",
    "        h = F.relu(self.deconv2(h))\n",
    "        h = torch.cat([h, h3], dim=1)\n",
    "        h = F.relu(self.deconv3(h))\n",
    "        h = torch.cat([h, h2], dim=1)\n",
    "        h = F.relu(self.deconv4(h))\n",
    "        h = torch.cat([h, h1], dim=1)\n",
    "        x_reconstructed = torch.sigmoid(self.deconv5(h))\n",
    "        if torch.isnan(x_reconstructed).any() or torch.isinf(x_reconstructed).any():\n",
    "            print(\"NaN or Inf in x_reconstructed\")\n",
    "        return x_reconstructed\n",
    "\n",
    "# Convert grayscale to RGB by duplicating channels\n",
    "def grayscale_to_rgb(tensor):\n",
    "    return tensor.repeat(1, 3, 1, 1)\n",
    "\n",
    "# Conditional VAE model\n",
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def reparameterize(self, z_mean, z_logvar):\n",
    "        std = torch.exp(0.5 * z_logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return z_mean + eps * std\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        z_mean, z_logvar, skip_connections = self.encoder(x, y)\n",
    "        z = self.reparameterize(z_mean, z_logvar)\n",
    "        x_reconstructed = self.decoder(z, y, skip_connections)\n",
    "        return x_reconstructed, z_mean, z_logvar\n",
    "\n",
    "# Load pretrained VGG16 for perceptual loss\n",
    "vgg = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features.to(device).eval()\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "def perceptual_loss(x, x_reconstructed):\n",
    "    x_rgb = grayscale_to_rgb(x)\n",
    "    x_reconstructed_rgb = grayscale_to_rgb(x_reconstructed)\n",
    "    x_rgb_subset = x_rgb[:8]\n",
    "    x_reconstructed_rgb_subset = x_reconstructed_rgb[:8]\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(x.device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(x.device)\n",
    "    x_normalized = (x_rgb_subset - mean) / std\n",
    "    x_reconstructed_normalized = (x_reconstructed_rgb_subset - mean) / std\n",
    "    x_features = vgg(x_normalized)\n",
    "    x_recon_features = vgg(x_reconstructed_normalized)\n",
    "    return F.mse_loss(x_features, x_recon_features)\n",
    "\n",
    "# Instantiate Encoder, Decoder, and CVAE\n",
    "encoder = Encoder(latent_dim, num_classes).to(device)\n",
    "decoder = Decoder(latent_dim, num_classes).to(device)\n",
    "cvae = ConditionalVAE(encoder, decoder).to(device)\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "optimizer = optim.Adam(cvae.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.5)\n",
    "\n",
    "# Define loss function with beta annealing\n",
    "def cvae_loss(x, x_reconstructed, z_mean, z_logvar, beta=1.0, recon_weight=1.0, perceptual_weight=1.0):\n",
    "    if torch.isnan(x).any() or torch.isinf(x).any():\n",
    "        print(\"NaN or Inf detected in x\")\n",
    "    if torch.isnan(x_reconstructed).any() or torch.isinf(x_reconstructed).any():\n",
    "        print(\"NaN or Inf detected in x_reconstructed\")\n",
    "    if torch.isnan(z_mean).any() or torch.isinf(z_mean).any():\n",
    "        print(\"NaN or Inf detected in z_mean\")\n",
    "    if torch.isnan(z_logvar).any() or torch.isinf(z_logvar).any():\n",
    "        print(\"NaN or Inf detected in z_logvar\")\n",
    "\n",
    "    recon_loss = F.binary_cross_entropy(x_reconstructed, x, reduction='sum')\n",
    "    kl_loss = -0.5 * torch.sum(1 + torch.clamp(z_logvar, -5, 5) - z_mean.pow(2) - torch.clamp(z_logvar, -5, 5).exp())\n",
    "    percep_loss = perceptual_loss(x, x_reconstructed) * perceptual_weight\n",
    "    total_loss = recon_weight * recon_loss + beta * kl_loss + percep_loss\n",
    "    return total_loss, recon_loss, kl_loss, percep_loss\n",
    "\n",
    "# Training loop with checkpointing\n",
    "cvae.train()\n",
    "for epoch in range(epochs):\n",
    "    if epoch < annealing_epochs:\n",
    "        beta = beta_max * (epoch / annealing_epochs)\n",
    "    else:\n",
    "        beta = beta_max\n",
    "\n",
    "    train_loss = 0\n",
    "    train_recon_loss = 0\n",
    "    train_kl_loss = 0\n",
    "    train_percep_loss = 0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        x_reconstructed, z_mean, z_logvar = cvae(data, labels)\n",
    "        total_loss, recon_loss, kl_loss, percep_loss = cvae_loss(\n",
    "            data, x_reconstructed, z_mean, z_logvar, \n",
    "            beta=beta, recon_weight=recon_weight, perceptual_weight=perceptual_weight\n",
    "        )\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(cvae.parameters(), max_norm=0.5)\n",
    "        train_loss += total_loss.item()\n",
    "        train_recon_loss += recon_loss.item()\n",
    "        train_kl_loss += kl_loss.item()\n",
    "        train_percep_loss += percep_loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    avg_recon_loss = train_recon_loss / len(train_loader.dataset)\n",
    "    avg_kl_loss = train_kl_loss / len(train_loader.dataset)\n",
    "    avg_percep_loss = train_percep_loss / len(train_loader.dataset)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Beta: {beta:.2f}, Total Loss: {avg_loss:.4f}, '\n",
    "          f'Recon Loss: {avg_recon_loss:.4f}, KL Loss: {avg_kl_loss:.4f}, '\n",
    "          f'Percep Loss: {avg_percep_loss:.4f}')\n",
    "\n",
    "    # Save checkpoint, generate samples, and create plot only in the last epoch\n",
    "    if epoch + 1 == epochs:\n",
    "        # Save checkpoint\n",
    "        checkpoint_path = os.path.join(output_dir, f\"cvae_vehicle_final.pth\")\n",
    "        torch.save(cvae.state_dict(), checkpoint_path)\n",
    "        print(f\"Saved final checkpoint at epoch {epoch + 1} to {checkpoint_path}\")\n",
    "\n",
    "        # Generate and save samples\n",
    "        cvae.eval()\n",
    "        base_dir = os.path.join(output_dir, f\"generated_samples_final\")\n",
    "        classes_to_generate = [3, 4]\n",
    "        num_samples = 100\n",
    "        with torch.no_grad():\n",
    "            for class_label in classes_to_generate:\n",
    "                label_tensor = torch.tensor([class_label]).repeat(num_samples).to(device)\n",
    "                z = torch.randn(num_samples, latent_dim).to(device)\n",
    "                dummy_skips = [\n",
    "                    torch.randn(num_samples, 32, 64, 64).to(device) * 0.1,\n",
    "                    torch.randn(num_samples, 64, 32, 32).to(device) * 0.1,\n",
    "                    torch.randn(num_samples, 128, 16, 16).to(device) * 0.1,\n",
    "                    torch.randn(num_samples, 256, 8, 8).to(device) * 0.1,\n",
    "                    torch.randn(num_samples, 512, 4, 4).to(device) * 0.1\n",
    "                ]\n",
    "                generated_samples = cvae.decoder(z, label_tensor, dummy_skips)\n",
    "                generated_samples_rgb = grayscale_to_rgb(generated_samples)\n",
    "                class_dir = os.path.join(base_dir, str(class_label))\n",
    "                os.makedirs(class_dir, exist_ok=True)\n",
    "                for idx, sample in enumerate(generated_samples_rgb):\n",
    "                    sample = adjust_sharpness(sample, sharpness_factor=2.0)\n",
    "                    save_image(sample, os.path.join(class_dir, f\"sample_{idx}.png\"))\n",
    "                print(f\"Generated {num_samples} samples for Class {class_label} ({dataset.class_names[class_label]}) at final epoch.\")\n",
    "\n",
    "        # Create and save plot\n",
    "        fig, axs = plt.subplots(len(classes_to_generate), 10, figsize=(20, 4))\n",
    "        for row, class_label in enumerate(classes_to_generate):\n",
    "            class_dir = os.path.join(base_dir, str(class_label))\n",
    "            sample_files = os.listdir(class_dir)\n",
    "            random_samples = np.random.choice(sample_files, 10, replace=False)\n",
    "            for col, sample_file in enumerate(random_samples):\n",
    "                sample_path = os.path.join(class_dir, sample_file)\n",
    "                sample_image = Image.open(sample_path).convert(\"RGB\")\n",
    "                sample_image = sample_image.resize((128, 128), Image.LANCZOS)\n",
    "                sample_image = np.array(sample_image) / 255.0\n",
    "                ax = axs[row, col] if len(classes_to_generate) > 1 else axs[col]\n",
    "                ax.imshow(sample_image)\n",
    "                ax.axis('off')\n",
    "                if col == 0:\n",
    "                    ax.set_ylabel(dataset.class_names[class_label], rotation=90, labelpad=10)\n",
    "        plt.tight_layout()\n",
    "        plot_path = os.path.join(output_dir, f\"synthetic_samples_classes_3_4_final.png\")\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        print(f\"Saved synthetic samples plot at final epoch to {plot_path}\")\n",
    "        cvae.train()\n",
    "\n",
    "# Final model save (redundant since saved above, but kept for consistency)\n",
    "final_model_path = os.path.join(output_dir, \"cvae_vehicle_final.pth\")\n",
    "torch.save(cvae.state_dict(), final_model_path)\n",
    "print(f\"Saved final CVAE model to {final_model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
