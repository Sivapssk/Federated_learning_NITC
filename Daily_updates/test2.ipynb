{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3856349b-a8f5-412f-8305-08284b41e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Subset, Dataset, ConcatDataset\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import kagglehub\n",
    "import time\n",
    "\n",
    "# Start total script timer\n",
    "total_start_time = time.time()\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = \"FL_VEHICLE_CVAE_latent_test\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "RESIZE = 128\n",
    "original_dim = RESIZE * RESIZE * 3\n",
    "intermediate_dim = 512\n",
    "latent_dim = 256\n",
    "num_classes = 5\n",
    "batch_size = 8\n",
    "epochs = 2  # Changed to 2 for testing\n",
    "learning_rate = 1e-4\n",
    "beta_start = 1\n",
    "beta_end = 10\n",
    "device = torch.device('cuda:1' if torch.cuda.device_count() > 1 else 'cuda')\n",
    "\n",
    "# Print GPU information\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "\n",
    "# Download Vehicle Type Image Dataset from Kaggle\n",
    "try:\n",
    "    path = kagglehub.dataset_download(\"sujaykapadnis/vehicle-type-image-dataset\")\n",
    "    print(\"Path to dataset files:\", path)\n",
    "    dataset_path = path\n",
    "except Exception as e:\n",
    "    print(f\"Failed to download dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Transform for CVAE (normalize to [-1, 1])\n",
    "cvae_input_transform = transforms.Compose([\n",
    "    transforms.Resize((RESIZE, RESIZE)),\n",
    "    transforms.ToTensor(),  # [0, 1]\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # [-1, 1]\n",
    "])\n",
    "\n",
    "# Debug dataset directory structure\n",
    "print(\"Inspecting dataset path:\", dataset_path)\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    print(f\"Root: {root}\")\n",
    "    print(f\"Dirs: {dirs}\")\n",
    "    print(f\"Files (first 5): {files[:5]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Custom Dataset for the Vehicle Type Dataset\n",
    "class VehicleTypeDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_names = []\n",
    "        self.class_to_idx = {}\n",
    "\n",
    "        print(f\"Searching for images in {root_dir}\")\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            image_files = [f for f in files if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "            if image_files:\n",
    "                class_name = os.path.basename(root)\n",
    "                if class_name not in self.class_to_idx:\n",
    "                    self.class_names.append(class_name)\n",
    "                    self.class_to_idx[class_name] = len(self.class_names) - 1\n",
    "                for img_file in image_files:\n",
    "                    img_path = os.path.join(root, img_file)\n",
    "                    try:\n",
    "                        Image.open(img_path).verify()\n",
    "                        self.images.append(img_path)\n",
    "                        self.labels.append(self.class_to_idx[class_name])\n",
    "                    except:\n",
    "                        print(f\"Skipping corrupted image: {img_path}\")\n",
    "\n",
    "        if len(self.class_names) != num_classes:\n",
    "            raise ValueError(f\"Expected {num_classes} classes, found {len(self.class_names)}\")\n",
    "        print(f\"Found {len(self.images)} images across {len(self.class_names)} classes.\")\n",
    "        print(f\"Classes: {self.class_names}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Load the dataset\n",
    "dataset = VehicleTypeDataset(root_dir=dataset_path, transform=cvae_input_transform)\n",
    "\n",
    "# Update label_dim\n",
    "label_dim = len(dataset.class_names)\n",
    "print(f\"Number of classes (label_dim): {label_dim}\")\n",
    "\n",
    "# Step 1: Split dataset into train, validation, and test sets per class\n",
    "validation_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "train_ratio = 0.8\n",
    "\n",
    "class_datasets = [[] for _ in range(label_dim)]\n",
    "for idx in range(len(dataset)):\n",
    "    label = dataset.labels[idx]\n",
    "    class_datasets[label].append(idx)\n",
    "\n",
    "train_indices_per_class = []\n",
    "val_indices_per_class = []\n",
    "test_indices_per_class = []\n",
    "\n",
    "for class_idx in range(label_dim):\n",
    "    indices = class_datasets[class_idx]\n",
    "    total_samples = len(indices)\n",
    "    num_train = int(total_samples * train_ratio)\n",
    "    num_val = int(total_samples * validation_ratio)\n",
    "    num_test = total_samples - num_train - num_val\n",
    "\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_indices = indices[:num_train]\n",
    "    val_indices = indices[num_train:num_train + num_val]\n",
    "    test_indices = indices[num_train + num_val:]\n",
    "\n",
    "    train_indices_per_class.append(train_indices)\n",
    "    val_indices_per_class.append(val_indices)\n",
    "    test_indices_per_class.append(test_indices)\n",
    "\n",
    "    print(f\"Class {class_idx}: Train={len(train_indices)}, Val={len(val_indices)}, Test={len(test_indices)}\")\n",
    "\n",
    "# Verify no overlap\n",
    "for class_idx in range(label_dim):\n",
    "    train_set = set(train_indices_per_class[class_idx])\n",
    "    val_set = set(val_indices_per_class[class_idx])\n",
    "    test_set = set(test_indices_per_class[class_idx])\n",
    "    assert len(train_set.intersection(val_set)) == 0, f\"Overlap between train and val for class {class_idx}\"\n",
    "    assert len(train_set.intersection(test_set)) == 0, f\"Overlap between train and test for class {class_idx}\"\n",
    "    assert len(val_set.intersection(test_set)) == 0, f\"Overlap between val and test for class {class_idx}\"\n",
    "\n",
    "# Create training dataset\n",
    "train_dataset = Subset(dataset, [idx for class_indices in train_indices_per_class for idx in class_indices])\n",
    "val_dataset = Subset(dataset, [idx for class_indices in val_indices_per_class for idx in class_indices])\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Subsample to 500 samples per class\n",
    "target_samples_per_class = 500\n",
    "train_class_datasets = []\n",
    "\n",
    "for class_idx in range(label_dim):\n",
    "    class_indices = train_indices_per_class[class_idx]\n",
    "    if len(class_indices) > target_samples_per_class:\n",
    "        class_indices = np.random.choice(class_indices, target_samples_per_class, replace=False).tolist()\n",
    "    class_dataset = Subset(dataset, class_indices)\n",
    "    train_class_datasets.append(class_dataset)\n",
    "    print(f\"Class {class_idx} subsampled dataset length: {len(class_dataset)}\")\n",
    "\n",
    "# Weight initialization\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# Encoder (Convolutional)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, latent_dim, num_classes):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=5, stride=2, padding=2),    # Output: 64 x 64 x 64\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),  # Output: 128 x 32 x 32\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2), # Output: 256 x 16 x 16\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=5, stride=2, padding=2), # Output: 512 x 8 x 8\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=5, stride=2, padding=2), # Output: 512 x 4 x 4\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 1024, kernel_size=5, stride=2, padding=2),# Output: 1024 x 2 x 2\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=5, stride=1, padding=2),# Output: 1024 x 2 x 2\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_hidden = nn.Linear(1024 * 2 * 2 + num_classes, hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.encoder_cnn(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x_with_y = torch.cat([x, y], dim=-1)\n",
    "        h = F.relu(self.fc_hidden(x_with_y))\n",
    "        h = self.dropout(h)\n",
    "        z_mean = self.fc_mean(h)\n",
    "        z_logvar = self.fc_logvar(h)\n",
    "        z_logvar = torch.clamp(z_logvar, min=-10, max=10)\n",
    "        return z_mean, z_logvar\n",
    "\n",
    "# Decoder (Convolutional)\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, num_classes):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim + num_classes, hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc_to_cnn = nn.Linear(hidden_dim, 1024 * 2 * 2)\n",
    "        self.decoder_cnn = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 1024, kernel_size=5, stride=1, padding=2), # Output: 1024 x 2 x 2\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=5, stride=2, padding=2, output_padding=1), # Output: 512 x 4 x 4\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 512, kernel_size=5, stride=2, padding=2, output_padding=1),  # Output: 512 x 8 x 8\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=5, stride=2, padding=2, output_padding=1),  # Output: 256 x 16 x 16\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=5, stride=2, padding=2, output_padding=1),  # Output: 128 x 32 x 32\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2, padding=2, output_padding=1),   # Output: 64 x 64 x 64\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=5, stride=2, padding=2, output_padding=1),     # Output: 3 x 128 x 128\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        z_with_y = torch.cat([z, y], dim=-1)\n",
    "        h = F.relu(self.fc(z_with_y))\n",
    "        h = self.dropout(h)\n",
    "        h = F.relu(self.fc_to_cnn(h))\n",
    "        h = h.view(-1, 1024, 2, 2)\n",
    "        x_reconstructed = self.decoder_cnn(h)\n",
    "        x_reconstructed = torch.clamp(x_reconstructed, min=-1, max=1)\n",
    "        return x_reconstructed\n",
    "\n",
    "# Conditional VAE\n",
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def reparameterize(self, z_mean, z_logvar):\n",
    "        std = torch.exp(0.5 * z_logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return z_mean + eps * std\n",
    "\n",
    "    def forward(self, data, y):\n",
    "        z_mean, z_logvar = self.encoder(data, y)\n",
    "        z = self.reparameterize(z_mean, z_logvar)\n",
    "        x_reconstructed = self.decoder(z, y)\n",
    "        return x_reconstructed, z_mean, z_logvar\n",
    "\n",
    "# Loss Function\n",
    "def cvae_loss(data, x_reconstructed, z_mean, z_logvar, beta=1.0):\n",
    "    batch_size = data.size(0)\n",
    "    mse_loss = F.mse_loss(x_reconstructed, data, reduction='sum') / batch_size\n",
    "    kl_loss = -0.5 * torch.sum(1 + z_logvar - z_mean.pow(2) - z_logvar.exp()) / batch_size\n",
    "    return mse_loss + beta * kl_loss\n",
    "\n",
    "# Function to format time\n",
    "def format_time(seconds):\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = seconds % 60\n",
    "    if hours > 0:\n",
    "        return f\"{hours}h {minutes}m {secs:.2f}s\"\n",
    "    elif minutes > 0:\n",
    "        return f\"{minutes}m {secs:.2f}s\"\n",
    "    else:\n",
    "        return f\"{secs:.2f}s\"\n",
    "\n",
    "# Step 2: Define user classes and train CVAEs\n",
    "Num_users = 5\n",
    "user_classes = {\n",
    "    0: [0, 1, 3],  # User 1\n",
    "    1: [1, 2, 4],  # User 2\n",
    "    2: [0, 2, 3],  # User 3\n",
    "    3: [1, 3, 4],  # User 4\n",
    "    4: [0, 2, 4]   # User 5\n",
    "}\n",
    "\n",
    "cvae_users = {}\n",
    "train_losses_users = {}\n",
    "val_losses_users = {}\n",
    "\n",
    "for user_idx in range(Num_users):\n",
    "    # Start timer for this user's CVAE training\n",
    "    user_start_time = time.time()\n",
    "\n",
    "    # Create user dataset\n",
    "    user_dataset = ConcatDataset([train_class_datasets[i] for i in user_classes[user_idx]])\n",
    "    print(f\"User {user_idx + 1} dataset length: {len(user_dataset)}\")\n",
    "\n",
    "    # Validate indices\n",
    "    try:\n",
    "        for i in range(min(5, len(user_dataset))):\n",
    "            sample, label = user_dataset[i]\n",
    "            print(f\"User {user_idx + 1}, Sample {i}: Label={label}, Data shape={sample.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing samples for User {user_idx + 1}: {e}\")\n",
    "        raise\n",
    "\n",
    "    user_loader = DataLoader(user_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True, drop_last=True)\n",
    "\n",
    "    # Instantiate CVAE\n",
    "    encoder = Encoder(intermediate_dim, latent_dim, num_classes).to(device)\n",
    "    decoder = Decoder(latent_dim, intermediate_dim, num_classes).to(device)\n",
    "    cvae = ConditionalVAE(encoder, decoder).to(device)\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = optim.Adam(cvae.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.5)\n",
    "\n",
    "    # Training loop\n",
    "    checkpoint_dir = os.path.join(output_dir, f'checkpoints_cvae_user_{user_idx + 1}')\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    cvae.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        beta = beta_start + (beta_end - beta_start) * ((epoch - 1) / (epochs - 1)) if epochs > 1 else beta_end\n",
    "\n",
    "        train_loss = 0\n",
    "        batches_processed = 0\n",
    "\n",
    "        for batch_idx, (data, labels) in enumerate(user_loader):\n",
    "            try:\n",
    "                data = data.to(device)\n",
    "                y = F.one_hot(labels, num_classes=num_classes).float().to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                x_recon, z_mean, z_logvar = cvae(data, y)\n",
    "                loss = cvae_loss(data, x_recon, z_mean, z_logvar, beta)\n",
    "\n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    print(f\"NaN/Inf loss detected at epoch {epoch}, batch {batch_idx + 1} for User {user_idx + 1}\")\n",
    "                    continue\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(cvae.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                batches_processed += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error at epoch {epoch}, batch {batch_idx + 1} for User {user_idx + 1}: {e}\")\n",
    "                if \"out of memory\" in str(e).lower():\n",
    "                    print(\"Out of memory error detected. Clearing cache...\")\n",
    "                    if device.type == 'cuda':\n",
    "                        torch.cuda.empty_cache()\n",
    "                continue\n",
    "\n",
    "        avg_train_loss = train_loss / batches_processed if batches_processed > 0 else float('inf')\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        cvae.eval()\n",
    "        val_loss = 0\n",
    "        val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in val_loader:\n",
    "                data = data.to(device)\n",
    "                y = F.one_hot(labels, num_classes=num_classes).float().to(device)\n",
    "                x_recon, z_mean, z_logvar = cvae(data, y)\n",
    "                loss = cvae_loss(data, x_recon, z_mean, z_logvar, beta=1.0)\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "\n",
    "        avg_val_loss = val_loss / val_batches if val_batches > 0 else 0\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"User {user_idx + 1}, Epoch {epoch}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Time: {format_time(epoch_time)}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Clear GPU memory\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Save checkpoints, latent vectors, and decoder parameters every 500 epochs or at the end\n",
    "        if epoch % 500 == 0 or epoch == epochs:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'cvae_epoch_{epoch}.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': cvae.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Checkpoint saved for User {user_idx + 1} at epoch {epoch} to {checkpoint_path}\")\n",
    "\n",
    "            # Save decoder parameters\n",
    "            decoder_dir = os.path.join(checkpoint_dir, 'decoder')\n",
    "            os.makedirs(decoder_dir, exist_ok=True)\n",
    "            decoder_path = os.path.join(decoder_dir, f'decoder_epoch_{epoch}.pth')\n",
    "            torch.save(cvae.decoder.state_dict(), decoder_path)\n",
    "            print(f\"Decoder saved for User {user_idx + 1} at epoch {epoch} to {decoder_path}\")\n",
    "\n",
    "            # Save latent vectors with labels\n",
    "            latent_dir = os.path.join(checkpoint_dir, f'latent_vectors_epoch_{epoch}')\n",
    "            os.makedirs(latent_dir, exist_ok=True)\n",
    "\n",
    "            cvae.eval()\n",
    "            with torch.no_grad():\n",
    "                latent_vectors = {cls: {'z_mean': [], 'z_logvar': [], 'labels': []} for cls in user_classes[user_idx]}\n",
    "                for data, labels in user_loader:\n",
    "                    data = data.to(device)\n",
    "                    y = F.one_hot(labels, num_classes=num_classes).float().to(device)\n",
    "                    z_mean, z_logvar = cvae.encoder(data, y)\n",
    "                    for i, label in enumerate(labels):\n",
    "                        latent_vectors[label.item()]['z_mean'].append(z_mean[i].cpu())\n",
    "                        latent_vectors[label.item()]['z_logvar'].append(z_logvar[i].cpu())\n",
    "                        latent_vectors[label.item()]['labels'].append(label.item())\n",
    "\n",
    "                for cls in user_classes[user_idx]:\n",
    "                    if latent_vectors[cls]['z_mean']:\n",
    "                        z_mean = torch.stack(latent_vectors[cls]['z_mean'])\n",
    "                        z_logvar = torch.stack(latent_vectors[cls]['z_logvar'])\n",
    "                        labels = torch.tensor(latent_vectors[cls]['labels'])\n",
    "                        save_path = os.path.join(latent_dir, f'class_{cls}.pth')\n",
    "                        torch.save({\n",
    "                            'z_mean': z_mean,\n",
    "                            'z_logvar': z_logvar,\n",
    "                            'labels': labels\n",
    "                        }, save_path)\n",
    "                        print(f\"Saved latent vectors for User {user_idx + 1}, Class {cls} at epoch {epoch} to {save_path}\")\n",
    "\n",
    "    # Store losses for plotting\n",
    "    train_losses_users[user_idx] = train_losses\n",
    "    val_losses_users[user_idx] = val_losses\n",
    "\n",
    "    # Plot losses\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'User {user_idx + 1} CVAE Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    loss_plot_path = os.path.join(checkpoint_dir, 'loss_plot.png')\n",
    "    plt.savefig(loss_plot_path)\n",
    "    plt.close()\n",
    "    print(f\"Loss plot saved for User {user_idx + 1} to {loss_plot_path}\")\n",
    "\n",
    "    cvae_users[user_idx] = cvae\n",
    "\n",
    "    user_time = time.time() - user_start_time\n",
    "    print(f\"Total time for User {user_idx + 1} CVAE training: {format_time(user_time)}\\n\")\n",
    "\n",
    "# Step 3: Share latent vectors and decoder parameters to generate synthetic data\n",
    "class_to_users = {cls: [] for cls in range(label_dim)}\n",
    "for user_idx, classes in user_classes.items():\n",
    "    for cls in classes:\n",
    "        class_to_users[cls].append(user_idx)\n",
    "\n",
    "# Define sharing scheme\n",
    "sharing_scheme = {}\n",
    "for cls in range(label_dim):\n",
    "    target_users = [user_idx for user_idx in range(Num_users) if cls not in user_classes[user_idx]]\n",
    "    if target_users and class_to_users[cls]:\n",
    "        source_user = class_to_users[cls][0]  # First user with this class\n",
    "        sharing_scheme[f'class_{cls}'] = {\n",
    "            'source_user': source_user,\n",
    "            'target_users': target_users,\n",
    "            'share_decoder': True\n",
    "        }\n",
    "\n",
    "synthetic_datasets = [[] for _ in range(Num_users)]\n",
    "num_synthetic_per_class_generate = 1000\n",
    "num_synthetic_per_class_select = 500\n",
    "\n",
    "# Use the latest latent vectors and decoder (from epoch 2)\n",
    "for class_key, scheme in sharing_scheme.items():\n",
    "    class_id = int(class_key.split('_')[1])\n",
    "    source_user = scheme['source_user']\n",
    "    target_users = scheme['target_users']\n",
    "\n",
    "    # Load the latest latent vectors (epoch 2)\n",
    "    latent_dir = os.path.join(output_dir, f'checkpoints_cvae_user_{source_user+1}', 'latent_vectors_epoch_2')\n",
    "    latent_path = os.path.join(latent_dir, f'class_{class_id}.pth')\n",
    "    latent_data = torch.load(latent_path, weights_only=False)\n",
    "    print(f\"Loaded latent data for User {source_user+1}, Class {class_id}: z_mean shape={latent_data['z_mean'].shape}\")\n",
    "    z_mean_all = latent_data['z_mean'].to(device)\n",
    "    z_logvar_all = latent_data['z_logvar'].to(device)\n",
    "\n",
    "    # Load the latest decoder (epoch 2)\n",
    "    decoder_dir = os.path.join(output_dir, f'checkpoints_cvae_user_{source_user+1}', 'decoder')\n",
    "    decoder_path = os.path.join(decoder_dir, 'decoder_epoch_2.pth')\n",
    "    print(f\"Loading decoder for User {source_user + 1}, Class {class_id}\")\n",
    "\n",
    "    # Create shared CVAE instance\n",
    "    shared_cvae = ConditionalVAE(Encoder(intermediate_dim, latent_dim, num_classes), Decoder(latent_dim, intermediate_dim, num_classes)).to(device)\n",
    "\n",
    "    if scheme['share_decoder']:\n",
    "        decoder_params = torch.load(decoder_path, weights_only=False)\n",
    "        shared_cvae.decoder.load_state_dict(decoder_params)\n",
    "        print(f\"Loaded decoder parameters: {decoder_path}\")\n",
    "    else:\n",
    "        print(f\"Warning: No decoder shared for user {source_user + 1}, Class {class_id}. Using random decoder.\")\n",
    "\n",
    "    # Generate synthetic data for all target users\n",
    "    for user_idx in target_users:\n",
    "        synthetic_dir = os.path.join(output_dir, f'synthetic_user_{user_idx + 1}', f'class_{class_id}')\n",
    "        os.makedirs(synthetic_dir, exist_ok=True)\n",
    "\n",
    "        print(f\"Generating {num_synthetic_per_class_generate} synthetic images for User {user_idx + 1}, Class {class_id}\")\n",
    "        synthetic_images = []\n",
    "        mean_intensities = []\n",
    "\n",
    "        shared_cvae.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(num_synthetic_per_class_generate):\n",
    "                z = shared_cvae.reparameterize(z_mean_all[i % len(z_mean_all)].unsqueeze(0), \n",
    "                                               z_logvar_all[i % len(z_mean_all)].unsqueeze(0))\n",
    "                y = F.one_hot(torch.tensor([class_id]), num_classes=label_dim).float().to(device)\n",
    "                synthetic_img = shared_cvae.decoder(z, y).cpu()\n",
    "                mean_intensity = synthetic_img.mean().item()\n",
    "                synthetic_images.append(synthetic_img)\n",
    "                mean_intensities.append(mean_intensity)\n",
    "\n",
    "                if (i + 1) % 200 == 0:\n",
    "                    print(f\"Generated {i + 1} images for User {user_idx + 1}, Class {class_id}\")\n",
    "\n",
    "        # Select top 500 images based on mean pixel intensity\n",
    "        print(f\"Selecting top {num_synthetic_per_class_select} images for User {user_idx + 1}, Class {class_id}\")\n",
    "        sorted_indices = np.argsort(mean_intensities)[::-1]\n",
    "        selected_indices = sorted_indices[:num_synthetic_per_class_select]\n",
    "\n",
    "        # Save selected images\n",
    "        for idx, img_idx in enumerate(selected_indices):\n",
    "            img_path = os.path.join(synthetic_dir, f'image_{idx + 1}.png')\n",
    "            try:\n",
    "                img = synthetic_images[img_idx].view(3, RESIZE, RESIZE)\n",
    "                img = img * 0.5 + 0.5  # Denormalize to [0, 1]\n",
    "                img = img.clamp(0, 1)\n",
    "                img = transforms.ToPILImage()(img)\n",
    "                img.save(img_path)\n",
    "                if (idx + 1) % 100 == 0 or idx == 0:\n",
    "                    print(f\"Saved {idx + 1} selected images for User {user_idx + 1}, Class {class_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving image {img_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"Completed generating and selecting {num_synthetic_per_class_select} images for User {user_idx + 1}, Class {class_id}\")\n",
    "\n",
    "        class SyntheticDataset(Dataset):\n",
    "            def __init__(self, class_label, root_dir, transform=None):\n",
    "                self.class_label = class_label\n",
    "                self.root_dir = root_dir\n",
    "                self.transform = transform\n",
    "                self.image_files = sorted([f for f in os.listdir(root_dir) if f.endswith('.png')])\n",
    "                if len(self.image_files) == 0:\n",
    "                    raise ValueError(f\"No images found in {root_dir}\")\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.image_files)\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "                image = Image.open(img_path).convert(\"RGB\")\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                return image, self.class_label\n",
    "\n",
    "        synthetic_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        synthetic_dataset = SyntheticDataset(class_id, synthetic_dir, transform=synthetic_transform)\n",
    "        synthetic_datasets[user_idx].append(synthetic_dataset)\n",
    "\n",
    "# Step 4: Verify the converted IID distribution\n",
    "user_data = []\n",
    "for user_idx in range(Num_users):\n",
    "    real_data = ConcatDataset([train_class_datasets[i] for i in user_classes[user_idx]])\n",
    "    if synthetic_datasets[user_idx]:\n",
    "        user_data.append(ConcatDataset([real_data] + synthetic_datasets[user_idx]))\n",
    "    else:\n",
    "        user_data.append(real_data)\n",
    "\n",
    "print(\"\\n=== Verifying Converted IID Data Distribution Across Users ===\")\n",
    "class_counts_per_user = []\n",
    "for user_idx in range(Num_users):\n",
    "    user_dataset = user_data[user_idx]\n",
    "    class_counts = [0] * label_dim\n",
    "    for idx in range(len(user_dataset)):\n",
    "        _, label = user_dataset[idx]\n",
    "        class_counts[label] += 1\n",
    "    class_counts_per_user.append(class_counts)\n",
    "    print(f\"User {user_idx + 1} (CVAE IID) Class Distribution: {class_counts}\")\n",
    "    total_samples = len(user_dataset)\n",
    "    class_percentages = [count / total_samples * 100 if total_samples > 0 else 0 for count in class_counts]\n",
    "    print(f\"User {user_idx + 1} (CVAE IID) Class Percentages: {[f'{p:.2f}%' for p in class_percentages]}\")\n",
    "\n",
    "# Calculate and print total script time\n",
    "total_time = time.time() - total_start_time\n",
    "print(f\"\\nTotal time for the entire script: {format_time(total_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c4d06e4-ae0d-487d-af82-c5ebcf000b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "##3000epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80624d36-17ec-48d0-9b45-ba0c27b26c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Subset, Dataset, ConcatDataset\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import kagglehub\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Start total script timer\n",
    "total_start_time = time.time()\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = \"FL_VEHICLE_CVAE_latent\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "RESIZE = 128\n",
    "original_dim = RESIZE * RESIZE * 3\n",
    "intermediate_dim = 512\n",
    "latent_dim = 256\n",
    "num_classes = 5\n",
    "batch_size = 8\n",
    "epochs = 3000\n",
    "learning_rate = 1e-4\n",
    "beta_start = 1\n",
    "beta_end = 10\n",
    "device = torch.device('cuda:1' if torch.cuda.device_count() > 1 else 'cuda')\n",
    "\n",
    "# Print GPU information\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "\n",
    "# Download Vehicle Type Image Dataset from Kaggle\n",
    "try:\n",
    "    path = kagglehub.dataset_download(\"sujaykapadnis/vehicle-type-image-dataset\")\n",
    "    print(\"Path to dataset files:\", path)\n",
    "    dataset_path = path\n",
    "except Exception as e:\n",
    "    print(f\"Failed to download dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Transform for CVAE (normalize to [-1, 1])\n",
    "cvae_input_transform = transforms.Compose([\n",
    "    transforms.Resize((RESIZE, RESIZE)),\n",
    "    transforms.ToTensor(),  # [0, 1]\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # [-1, 1]\n",
    "])\n",
    "\n",
    "# Debug dataset directory structure\n",
    "print(\"Inspecting dataset path:\", dataset_path)\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    print(f\"Root: {root}\")\n",
    "    print(f\"Dirs: {dirs}\")\n",
    "    print(f\"Files (first 5): {files[:5]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Custom Dataset for the Vehicle Type Dataset\n",
    "class VehicleTypeDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_names = []\n",
    "        self.class_to_idx = {}\n",
    "\n",
    "        print(f\"Searching for images in {root_dir}\")\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            image_files = [f for f in files if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "            if image_files:\n",
    "                class_name = os.path.basename(root)\n",
    "                if class_name not in self.class_to_idx:\n",
    "                    self.class_names.append(class_name)\n",
    "                    self.class_to_idx[class_name] = len(self.class_names) - 1\n",
    "                for img_file in image_files:\n",
    "                    img_path = os.path.join(root, img_file)\n",
    "                    try:\n",
    "                        Image.open(img_path).verify()\n",
    "                        self.images.append(img_path)\n",
    "                        self.labels.append(self.class_to_idx[class_name])\n",
    "                    except:\n",
    "                        print(f\"Skipping corrupted image: {img_path}\")\n",
    "\n",
    "        if len(self.class_names) != num_classes:\n",
    "            raise ValueError(f\"Expected {num_classes} classes, found {len(self.class_names)}\")\n",
    "        print(f\"Found {len(self.images)} images across {len(self.class_names)} classes.\")\n",
    "        print(f\"Classes: {self.class_names}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Load the dataset\n",
    "dataset = VehicleTypeDataset(root_dir=dataset_path, transform=cvae_input_transform)\n",
    "\n",
    "# Update label_dim\n",
    "label_dim = len(dataset.class_names)\n",
    "print(f\"Number of classes (label_dim): {label_dim}\")\n",
    "\n",
    "# Step 1: Split dataset into train, validation, and test sets per class\n",
    "validation_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "train_ratio = 0.8\n",
    "\n",
    "class_datasets = [[] for _ in range(label_dim)]\n",
    "for idx in range(len(dataset)):\n",
    "    label = dataset.labels[idx]\n",
    "    class_datasets[label].append(idx)\n",
    "\n",
    "train_indices_per_class = []\n",
    "val_indices_per_class = []\n",
    "test_indices_per_class = []\n",
    "\n",
    "for class_idx in range(label_dim):\n",
    "    indices = class_datasets[class_idx]\n",
    "    total_samples = len(indices)\n",
    "    num_train = int(total_samples * train_ratio)\n",
    "    num_val = int(total_samples * validation_ratio)\n",
    "    num_test = total_samples - num_train - num_val\n",
    "\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_indices = indices[:num_train]\n",
    "    val_indices = indices[num_train:num_train + num_val]\n",
    "    test_indices = indices[num_train + num_val:]\n",
    "\n",
    "    train_indices_per_class.append(train_indices)\n",
    "    val_indices_per_class.append(val_indices)\n",
    "    test_indices_per_class.append(test_indices)\n",
    "\n",
    "    print(f\"Class {class_idx}: Train={len(train_indices)}, Val={len(val_indices)}, Test={len(test_indices)}\")\n",
    "\n",
    "# Verify no overlap\n",
    "for class_idx in range(label_dim):\n",
    "    train_set = set(train_indices_per_class[class_idx])\n",
    "    val_set = set(val_indices_per_class[class_idx])\n",
    "    test_set = set(test_indices_per_class[class_idx])\n",
    "    assert len(train_set.intersection(val_set)) == 0, f\"Overlap between train and val for class {class_idx}\"\n",
    "    assert len(train_set.intersection(test_set)) == 0, f\"Overlap between train and test for class {class_idx}\"\n",
    "    assert len(val_set.intersection(test_set)) == 0, f\"Overlap between val and test for class {class_idx}\"\n",
    "\n",
    "# Create training and validation datasets\n",
    "train_dataset = Subset(dataset, [idx for class_indices in train_indices_per_class for idx in class_indices])\n",
    "val_dataset = Subset(dataset, [idx for class_indices in val_indices_per_class for idx in class_indices])\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Step 1.5: Ensure 500 samples per class by generating synthetic samples for underrepresented classes\n",
    "target_samples_per_class = 500\n",
    "train_class_datasets = []\n",
    "cvae_per_class = {}\n",
    "\n",
    "# Weight initialization\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# Encoder (Convolutional)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, latent_dim, num_classes):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 1024, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_hidden = nn.Linear(1024 * 2 * 2 + num_classes, hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.encoder_cnn(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x_with_y = torch.cat([x, y], dim=-1)\n",
    "        h = F.relu(self.fc_hidden(x_with_y))\n",
    "        h = self.dropout(h)\n",
    "        z_mean = self.fc_mean(h)\n",
    "        z_logvar = self.fc_logvar(h)\n",
    "        z_logvar = torch.clamp(z_logvar, min=-10, max=10)\n",
    "        return z_mean, z_logvar\n",
    "\n",
    "# Decoder (Convolutional)\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, num_classes):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim + num_classes, hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc_to_cnn = nn.Linear(hidden_dim, 1024 * 2 * 2)\n",
    "        self.decoder_cnn = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 1024, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 512, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        z_with_y = torch.cat([z, y], dim=-1)\n",
    "        h = F.relu(self.fc(z_with_y))\n",
    "        h = self.dropout(h)\n",
    "        h = F.relu(self.fc_to_cnn(h))\n",
    "        h = h.view(-1, 1024, 2, 2)\n",
    "        x_reconstructed = self.decoder_cnn(h)\n",
    "        x_reconstructed = torch.clamp(x_reconstructed, min=-1, max=1)\n",
    "        return x_reconstructed\n",
    "\n",
    "# Conditional VAE\n",
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def reparameterize(self, z_mean, z_logvar):\n",
    "        std = torch.exp(0.5 * z_logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return z_mean + eps * std\n",
    "\n",
    "    def forward(self, data, y):\n",
    "        z_mean, z_logvar = self.encoder(data, y)\n",
    "        z = self.reparameterize(z_mean, z_logvar)\n",
    "        x_reconstructed = self.decoder(z, y)\n",
    "        return x_reconstructed, z_mean, z_logvar\n",
    "\n",
    "# Loss Function\n",
    "def cvae_loss(data, x_reconstructed, z_mean, z_logvar, beta=1.0):\n",
    "    batch_size = data.size(0)\n",
    "    mse_loss = F.mse_loss(x_reconstructed, data, reduction='sum') / batch_size\n",
    "    kl_loss = -0.5 * torch.sum(1 + z_logvar - z_mean.pow(2) - z_logvar.exp()) / batch_size\n",
    "    return mse_loss + beta * kl_loss\n",
    "\n",
    "# Function to format time\n",
    "def format_time(seconds):\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = seconds % 60\n",
    "    if hours > 0:\n",
    "        return f\"{hours}h {minutes}m {secs:.2f}s\"\n",
    "    elif minutes > 0:\n",
    "        return f\"{minutes}m {secs:.2f}s\"\n",
    "    else:\n",
    "        return f\"{secs:.2f}s\"\n",
    "\n",
    "# Generate synthetic samples for classes with fewer than 500 samples\n",
    "for class_idx in range(label_dim):\n",
    "    class_indices = train_indices_per_class[class_idx]\n",
    "    num_real_samples = len(class_indices)\n",
    "    print(f\"Class {class_idx} has {num_real_samples} real samples before augmentation.\")\n",
    "\n",
    "    if num_real_samples >= target_samples_per_class:\n",
    "        class_indices = np.random.choice(class_indices, target_samples_per_class, replace=False).tolist()\n",
    "        class_dataset = Subset(dataset, class_indices)\n",
    "        train_class_datasets.append(class_dataset)\n",
    "        print(f\"Class {class_idx} subsampled to {len(class_dataset)} samples.\")\n",
    "        continue\n",
    "\n",
    "    num_synthetic_needed = target_samples_per_class - num_real_samples\n",
    "    print(f\"Generating {num_synthetic_needed} synthetic samples for Class {class_idx} to reach {target_samples_per_class} samples.\")\n",
    "\n",
    "    class_dataset = Subset(dataset, class_indices)\n",
    "    class_loader = DataLoader(class_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True, drop_last=True)\n",
    "\n",
    "    encoder = Encoder(intermediate_dim, latent_dim, num_classes).to(device)\n",
    "    decoder = Decoder(latent_dim, intermediate_dim, num_classes).to(device)\n",
    "    cvae = ConditionalVAE(encoder, decoder).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(cvae.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.5)\n",
    "\n",
    "    class_checkpoint_dir = os.path.join(output_dir, f'checkpoints_cvae_class_{class_idx}')\n",
    "    os.makedirs(class_checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    cvae.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        beta = beta_start + (beta_end - beta_start) * ((epoch - 1) / (epochs - 1)) if epochs > 1 else beta_end\n",
    "\n",
    "        train_loss = 0\n",
    "        batches_processed = 0\n",
    "\n",
    "        for batch_idx, (data, labels) in enumerate(class_loader):\n",
    "            try:\n",
    "                data = data.to(device)\n",
    "                y = F.one_hot(labels, num_classes=num_classes).float().to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                x_recon, z_mean, z_logvar = cvae(data, y)\n",
    "                loss = cvae_loss(data, x_recon, z_mean, z_logvar, beta)\n",
    "\n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    print(f\"NaN/Inf loss detected at epoch {epoch}, batch {batch_idx + 1} for Class {class_idx}\")\n",
    "                    continue\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(cvae.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                batches_processed += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error at epoch {epoch}, batch {batch_idx + 1} for Class {class_idx}: {e}\")\n",
    "                if \"out of memory\" in str(e).lower():\n",
    "                    print(\"Out of memory error detected. Clearing cache...\")\n",
    "                    if device.type == 'cuda':\n",
    "                        torch.cuda.empty_cache()\n",
    "                continue\n",
    "\n",
    "        avg_train_loss = train_loss / batches_processed if batches_processed > 0 else float('inf')\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        cvae.eval()\n",
    "        val_loss = 0\n",
    "        val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in val_loader:\n",
    "                data = data.to(device)\n",
    "                y = F.one_hot(labels, num_classes=num_classes).float().to(device)\n",
    "                x_recon, z_mean, z_logvar = cvae(data, y)\n",
    "                loss = cvae_loss(data, x_recon, z_mean, z_logvar, beta=1.0)\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "\n",
    "        avg_val_loss = val_loss / val_batches if val_batches > 0 else 0\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"Class {class_idx}, Epoch {epoch}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Time: {format_time(epoch_time)}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        if epoch % 500 == 0 or epoch == epochs:\n",
    "            checkpoint_path = os.path.join(class_checkpoint_dir, f'cvae_epoch_{epoch}.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': cvae.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Checkpoint saved for Class {class_idx} at epoch {epoch} to {checkpoint_path}\")\n",
    "\n",
    "            latent_dir = os.path.join(class_checkpoint_dir, f'latent_vectors_epoch_{epoch}')\n",
    "            os.makedirs(latent_dir, exist_ok=True)\n",
    "\n",
    "            cvae.eval()\n",
    "            with torch.no_grad():\n",
    "                latent_vectors = {'z_mean': [], 'z_logvar': [], 'labels': []}\n",
    "                for data, labels in class_loader:\n",
    "                    data = data.to(device)\n",
    "                    y = F.one_hot(labels, num_classes=num_classes).float().to(device)\n",
    "                    z_mean, z_logvar = cvae.encoder(data, y)\n",
    "                    for i in range(len(labels)):\n",
    "                        latent_vectors['z_mean'].append(z_mean[i].cpu())\n",
    "                        latent_vectors['z_logvar'].append(z_logvar[i].cpu())\n",
    "                        latent_vectors['labels'].append(labels[i].item())\n",
    "\n",
    "                if latent_vectors['z_mean']:\n",
    "                    z_mean = torch.stack(latent_vectors['z_mean'])\n",
    "                    z_logvar = torch.stack(latent_vectors['z_logvar'])\n",
    "                    labels = torch.tensor(latent_vectors['labels'])\n",
    "                    save_path = os.path.join(latent_dir, f'class_{class_idx}.pth')\n",
    "                    torch.save({\n",
    "                        'z_mean': z_mean,\n",
    "                        'z_logvar': z_logvar,\n",
    "                        'labels': labels\n",
    "                    }, save_path)\n",
    "                    print(f\"Saved latent vectors for Class {class_idx} at epoch {epoch} to {save_path}\")\n",
    "\n",
    "    cvae_per_class[class_idx] = cvae\n",
    "\n",
    "    synthetic_dir = os.path.join(output_dir, f'synthetic_class_{class_idx}')\n",
    "    os.makedirs(synthetic_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Generating {num_synthetic_needed} synthetic images for Class {class_idx}\")\n",
    "    synthetic_images = []\n",
    "    latent_path = os.path.join(class_checkpoint_dir, 'latent_vectors_epoch_3000', f'class_{class_idx}.pth')\n",
    "    latent_data = torch.load(latent_path, weights_only=False)\n",
    "    z_mean_all = latent_data['z_mean'].to(device)\n",
    "    z_logvar_all = latent_data['z_logvar'].to(device)\n",
    "\n",
    "    cvae.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_synthetic_needed):\n",
    "            z = cvae.reparameterize(z_mean_all[i % len(z_mean_all)].unsqueeze(0), \n",
    "                                    z_logvar_all[i % len(z_mean_all)].unsqueeze(0))\n",
    "            y = F.one_hot(torch.tensor([class_idx]), num_classes=label_dim).float().to(device)\n",
    "            synthetic_img = cvae.decoder(z, y).cpu()\n",
    "            synthetic_images.append(synthetic_img)\n",
    "\n",
    "    for idx, img in enumerate(synthetic_images):\n",
    "        img_path = os.path.join(synthetic_dir, f'image_{idx + 1}.png')\n",
    "        try:\n",
    "            img = img.view(3, RESIZE, RESIZE)\n",
    "            img = img * 0.5 + 0.5\n",
    "            img = img.clamp(0, 1)\n",
    "            img = transforms.ToPILImage()(img)\n",
    "            img.save(img_path)\n",
    "            if (idx + 1) % 100 == 0 or idx == 0:\n",
    "                print(f\"Saved {idx + 1} synthetic images for Class {class_idx}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving image {img_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    class SyntheticDataset(Dataset):\n",
    "        def __init__(self, class_label, root_dir, transform=None):\n",
    "            self.class_label = class_label\n",
    "            self.root_dir = root_dir\n",
    "            self.transform = transform\n",
    "            self.image_files = sorted([f for f in os.listdir(root_dir) if f.endswith('.png')])\n",
    "            if len(self.image_files) == 0:\n",
    "                raise ValueError(f\"No images found in {root_dir}\")\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.image_files)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, self.class_label\n",
    "\n",
    "    synthetic_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    synthetic_dataset = SyntheticDataset(class_idx, synthetic_dir, transform=synthetic_transform)\n",
    "\n",
    "    combined_dataset = ConcatDataset([class_dataset, synthetic_dataset])\n",
    "    combined_indices = list(range(len(combined_dataset)))\n",
    "    if len(combined_indices) > target_samples_per_class:\n",
    "        combined_indices = np.random.choice(combined_indices, target_samples_per_class, replace=False).tolist()\n",
    "    final_class_dataset = Subset(combined_dataset, combined_indices)\n",
    "    train_class_datasets.append(final_class_dataset)\n",
    "    print(f\"Class {class_idx} final dataset length: {len(final_class_dataset)}\")\n",
    "\n",
    "# Step 1.6: Split train_class_datasets like the first code\n",
    "# Function to split a dataset into two parts\n",
    "def split_dataset(dataset, split_ratio):\n",
    "    train_size = int(np.round(split_ratio * len(dataset)))\n",
    "    remaining_size = len(dataset) - train_size\n",
    "    train_dataset, remaining_dataset = torch.utils.data.random_split(dataset, [train_size, remaining_size])\n",
    "    return train_dataset, remaining_dataset\n",
    "\n",
    "# Split each class dataset into two halves (50/50)\n",
    "split_ratio = 0.5\n",
    "split_datasets = []\n",
    "train_class_datasets1 = []\n",
    "train_class_datasets2 = []\n",
    "\n",
    "for class_dataset in train_class_datasets:\n",
    "    train_class_dataset1, train_class_dataset2 = split_dataset(class_dataset, split_ratio)\n",
    "    split_datasets.append((train_class_dataset1, train_class_dataset2))\n",
    "    train_class_datasets1.append(train_class_dataset1)\n",
    "    train_class_datasets2.append(train_class_dataset2)\n",
    "\n",
    "for i, (train_class_dataset1, train_class_dataset2) in enumerate(split_datasets):\n",
    "    print(f\"Class {i}:\")\n",
    "    print(f\"  Number of samples in train_class_datasets1: {len(train_class_dataset1)}\")\n",
    "    print(f\"  Number of samples in train_class_datasets2: {len(train_class_dataset2)}\")\n",
    "\n",
    "# Further split train_class_datasets2 into 70% and 30% parts\n",
    "split_ratio = 0.7\n",
    "split_datasets2 = []\n",
    "train_class_datasets2_part1 = []\n",
    "train_class_datasets2_part2 = []\n",
    "\n",
    "for class_dataset in train_class_datasets2:\n",
    "    part1_dataset, part2_dataset = split_dataset(class_dataset, split_ratio)\n",
    "    split_datasets2.append((part1_dataset, part2_dataset))\n",
    "    train_class_datasets2_part1.append(part1_dataset)\n",
    "    train_class_datasets2_part2.append(part2_dataset)\n",
    "\n",
    "for i, (part1_dataset, part2_dataset) in enumerate(split_datasets2):\n",
    "    print(f\"Class {i}:\")\n",
    "    print(f\"  Number of samples in train_class_datasets2_part1 (70%): {len(part1_dataset)}\")\n",
    "    print(f\"  Number of samples in train_class_datasets2_part2 (30%): {len(part2_dataset)}\")\n",
    "\n",
    "# Step 2: Define user datasets as per the first code\n",
    "Num_users = 5\n",
    "user_data = []\n",
    "user_classes = {}  # For synthetic data generation\n",
    "\n",
    "# Assign datasets to users\n",
    "user_data.append(ConcatDataset([train_class_datasets1[0], train_class_datasets2[1], train_class_datasets2_part2[2], train_class_datasets2_part2[3], train_class_datasets2_part2[4]]))\n",
    "user_classes[0] = [0, 1, 2, 3, 4]\n",
    "\n",
    "user_data.append(ConcatDataset([train_class_datasets1[1], train_class_datasets2_part1[2]]))\n",
    "user_classes[1] = [1, 2]\n",
    "\n",
    "user_data.append(ConcatDataset([train_class_datasets1[2], train_class_datasets2_part1[3]]))\n",
    "user_classes[2] = [2, 3]\n",
    "\n",
    "user_data.append(ConcatDataset([train_class_datasets1[3], train_class_datasets2_part1[4]]))\n",
    "user_classes[3] = [3, 4]\n",
    "\n",
    "user_data.append(ConcatDataset([train_class_datasets1[4]]))\n",
    "user_classes[4] = [4]\n",
    "\n",
    "for i, user_dataset in enumerate(user_data):\n",
    "    print(f\"User {i + 1}:\")\n",
    "    print(f\"Number of samples in the user dataset: {len(user_dataset)}\")\n",
    "\n",
    "cvae_users = {}\n",
    "train_losses_users = {}\n",
    "val_losses_users = {}\n",
    "\n",
    "for user_idx in range(Num_users):\n",
    "    user_start_time = time.time()\n",
    "\n",
    "    user_dataset = user_data[user_idx]\n",
    "    print(f\"User {user_idx + 1} dataset length: {len(user_dataset)}\")\n",
    "\n",
    "    try:\n",
    "        for i in range(min(5, len(user_dataset))):\n",
    "            sample, label = user_dataset[i]\n",
    "            print(f\"User {user_idx + 1}, Sample {i}: Label={label}, Data shape={sample.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing samples for User {user_idx + 1}: {e}\")\n",
    "        raise\n",
    "\n",
    "    user_loader = DataLoader(user_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True, drop_last=True)\n",
    "\n",
    "    encoder = Encoder(intermediate_dim, latent_dim, num_classes).to(device)\n",
    "    decoder = Decoder(latent_dim, intermediate_dim, num_classes).to(device)\n",
    "    cvae = ConditionalVAE(encoder, decoder).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(cvae.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.5)\n",
    "\n",
    "    checkpoint_dir = os.path.join(output_dir, f'checkpoints_cvae_user_{user_idx + 1}')\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    cvae.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        beta = beta_start + (beta_end - beta_start) * ((epoch - 1) / (epochs - 1)) if epochs > 1 else beta_end\n",
    "\n",
    "        train_loss = 0\n",
    "        batches_processed = 0\n",
    "\n",
    "        for batch_idx, (data, labels) in enumerate(user_loader):\n",
    "            try:\n",
    "                data = data.to(device)\n",
    "                y = F.one_hot(labels, num_classes=num_classes).float().to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                x_recon, z_mean, z_logvar = cvae(data, y)\n",
    "                loss = cvae_loss(data, x_recon, z_mean, z_logvar, beta)\n",
    "\n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    print(f\"NaN/Inf loss detected at epoch {epoch}, batch {batch_idx + 1} for User {user_idx + 1}\")\n",
    "                    continue\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(cvae.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                batches_processed += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error at epoch {epoch}, batch {batch_idx + 1} for User {user_idx + 1}: {e}\")\n",
    "                if \"out of memory\" in str(e).lower():\n",
    "                    print(\"Out of memory error detected. Clearing cache...\")\n",
    "                    if device.type == 'cuda':\n",
    "                        torch.cuda.empty_cache()\n",
    "                continue\n",
    "\n",
    "        avg_train_loss = train_loss / batches_processed if batches_processed > 0 else float('inf')\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        cvae.eval()\n",
    "        val_loss = 0\n",
    "        val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in val_loader:\n",
    "                data = data.to(device)\n",
    "                y = F.one_hot(labels, num_classes=num_classes).float().to(device)\n",
    "                x_recon, z_mean, z_logvar = cvae(data, y)\n",
    "                loss = cvae_loss(data, x_recon, z_mean, z_logvar, beta=1.0)\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "\n",
    "        avg_val_loss = val_loss / val_batches if val_batches > 0 else 0\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"User {user_idx + 1}, Epoch {epoch}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Time: {format_time(epoch_time)}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        if epoch % 500 == 0 or epoch == epochs:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'cvae_epoch_{epoch}.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': cvae.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Checkpoint saved for User {user_idx + 1} at epoch {epoch} to {checkpoint_path}\")\n",
    "\n",
    "            decoder_dir = os.path.join(checkpoint_dir, 'decoder')\n",
    "            os.makedirs(decoder_dir, exist_ok=True)\n",
    "            decoder_path = os.path.join(decoder_dir, f'decoder_epoch_{epoch}.pth')\n",
    "            torch.save(cvae.decoder.state_dict(), decoder_path)\n",
    "            print(f\"Decoder saved for User {user_idx + 1} at epoch {epoch} to {decoder_path}\")\n",
    "\n",
    "            latent_dir = os.path.join(checkpoint_dir, f'latent_vectors_epoch_{epoch}')\n",
    "            os.makedirs(latent_dir, exist_ok=True)\n",
    "\n",
    "            cvae.eval()\n",
    "            with torch.no_grad():\n",
    "                latent_vectors = {cls: {'z_mean': [], 'z_logvar': [], 'labels': []} for cls in user_classes[user_idx]}\n",
    "                for data, labels in user_loader:\n",
    "                    data = data.to(device)\n",
    "                    y = F.one_hot(labels, num_classes=num_classes).float().to(device)\n",
    "                    z_mean, z_logvar = cvae.encoder(data, y)\n",
    "                    for i, label in enumerate(labels):\n",
    "                        latent_vectors[label.item()]['z_mean'].append(z_mean[i].cpu())\n",
    "                        latent_vectors[label.item()]['z_logvar'].append(z_logvar[i].cpu())\n",
    "                        latent_vectors[label.item()]['labels'].append(label.item())\n",
    "\n",
    "                for cls in user_classes[user_idx]:\n",
    "                    if latent_vectors[cls]['z_mean']:\n",
    "                        z_mean = torch.stack(latent_vectors[cls]['z_mean'])\n",
    "                        z_logvar = torch.stack(latent_vectors[cls]['z_logvar'])\n",
    "                        labels = torch.tensor(latent_vectors[cls]['labels'])\n",
    "                        save_path = os.path.join(latent_dir, f'class_{cls}.pth')\n",
    "                        torch.save({\n",
    "                            'z_mean': z_mean,\n",
    "                            'z_logvar': z_logvar,\n",
    "                            'labels': labels\n",
    "                        }, save_path)\n",
    "                        print(f\"Saved latent vectors for User {user_idx + 1}, Class {cls} at epoch {epoch} to {save_path}\")\n",
    "\n",
    "    train_losses_users[user_idx] = train_losses\n",
    "    val_losses_users[user_idx] = val_losses\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'User {user_idx + 1} CVAE Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    loss_plot_path = os.path.join(checkpoint_dir, 'loss_plot.png')\n",
    "    plt.savefig(loss_plot_path)\n",
    "    plt.close()\n",
    "    print(f\"Loss plot saved for User {user_idx + 1} to {loss_plot_path}\")\n",
    "\n",
    "    cvae_users[user_idx] = cvae\n",
    "\n",
    "    user_time = time.time() - user_start_time\n",
    "    print(f\"Total time for User {user_idx + 1} CVAE training: {format_time(user_time)}\\n\")\n",
    "\n",
    "# Step 3: Share latent vectors and decoder parameters to generate synthetic data\n",
    "class_to_users = {cls: [] for cls in range(label_dim)}\n",
    "for user_idx, classes in user_classes.items():\n",
    "    for cls in classes:\n",
    "        class_to_users[cls].append(user_idx)\n",
    "\n",
    "sharing_scheme = {}\n",
    "for cls in range(label_dim):\n",
    "    target_users = [user_idx for user_idx in range(Num_users) if cls not in user_classes[user_idx]]\n",
    "    if target_users and class_to_users[cls]:\n",
    "        source_user = class_to_users[cls][0]\n",
    "        sharing_scheme[f'class_{cls}'] = {\n",
    "            'source_user': source_user,\n",
    "            'target_users': target_users,\n",
    "            'share_decoder': True\n",
    "        }\n",
    "\n",
    "synthetic_datasets = [[] for _ in range(Num_users)]\n",
    "num_synthetic_per_class_generate = 1000\n",
    "num_synthetic_per_class_select = 500\n",
    "\n",
    "for class_key, scheme in sharing_scheme.items():\n",
    "    class_id = int(class_key.split('_')[1])\n",
    "    source_user = scheme['source_user']\n",
    "    target_users = scheme['target_users']\n",
    "\n",
    "    latent_dir = os.path.join(output_dir, f'checkpoints_cvae_user_{source_user+1}', 'latent_vectors_epoch_3000')\n",
    "    latent_path = os.path.join(latent_dir, f'class_{class_id}.pth')\n",
    "    latent_data = torch.load(latent_path, weights_only=False)\n",
    "    print(f\"Loaded latent data for User {source_user+1}, Class {class_id}: z_mean shape={latent_data['z_mean'].shape}\")\n",
    "    z_mean_all = latent_data['z_mean'].to(device)\n",
    "    z_logvar_all = latent_data['z_logvar'].to(device)\n",
    "\n",
    "    decoder_dir = os.path.join(output_dir, f'checkpoints_cvae_user_{source_user+1}', 'decoder')\n",
    "    decoder_path = os.path.join(decoder_dir, 'decoder_epoch_3000.pth')\n",
    "    print(f\"Loading decoder for User {source_user + 1}, Class {class_id}\")\n",
    "\n",
    "    shared_cvae = ConditionalVAE(Encoder(intermediate_dim, latent_dim, num_classes), Decoder(latent_dim, intermediate_dim, num_classes)).to(device)\n",
    "\n",
    "    if scheme['share_decoder']:\n",
    "        decoder_params = torch.load(decoder_path, weights_only=False)\n",
    "        shared_cvae.decoder.load_state_dict(decoder_params)\n",
    "        print(f\"Loaded decoder parameters: {decoder_path}\")\n",
    "    else:\n",
    "        print(f\"Warning: No decoder shared for user {source_user + 1}, Class {class_id}. Using random decoder.\")\n",
    "\n",
    "    for user_idx in target_users:\n",
    "        synthetic_dir = os.path.join(output_dir, f'synthetic_user_{user_idx + 1}', f'class_{class_id}')\n",
    "        os.makedirs(synthetic_dir, exist_ok=True)\n",
    "\n",
    "        print(f\"Generating {num_synthetic_per_class_generate} synthetic images for User {user_idx + 1}, Class {class_id}\")\n",
    "        synthetic_images = []\n",
    "        mean_intensities = []\n",
    "\n",
    "        shared_cvae.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(num_synthetic_per_class_generate):\n",
    "                z = shared_cvae.reparameterize(z_mean_all[i % len(z_mean_all)].unsqueeze(0), \n",
    "                                               z_logvar_all[i % len(z_mean_all)].unsqueeze(0))\n",
    "                y = F.one_hot(torch.tensor([class_id]), num_classes=label_dim).float().to(device)\n",
    "                synthetic_img = shared_cvae.decoder(z, y).cpu()\n",
    "                mean_intensity = synthetic_img.mean().item()\n",
    "                synthetic_images.append(synthetic_img)\n",
    "                mean_intensities.append(mean_intensity)\n",
    "\n",
    "                if (i + 1) % 200 == 0:\n",
    "                    print(f\"Generated {i + 1} images for User {user_idx + 1}, Class {class_id}\")\n",
    "\n",
    "        print(f\"Selecting top {num_synthetic_per_class_select} images for User {user_idx + 1}, Class {class_id}\")\n",
    "        sorted_indices = np.argsort(mean_intensities)[::-1]\n",
    "        selected_indices = sorted_indices[:num_synthetic_per_class_select]\n",
    "\n",
    "        for idx, img_idx in enumerate(selected_indices):\n",
    "            img_path = os.path.join(synthetic_dir, f'image_{idx + 1}.png')\n",
    "            try:\n",
    "                img = synthetic_images[img_idx].view(3, RESIZE, RESIZE)\n",
    "                img = img * 0.5 + 0.5\n",
    "                img = img.clamp(0, 1)\n",
    "                img = transforms.ToPILImage()(img)\n",
    "                img.save(img_path)\n",
    "                if (idx + 1) % 100 == 0 or idx == 0:\n",
    "                    print(f\"Saved {idx + 1} selected images for User {user_idx + 1}, Class {class_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving image {img_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"Completed generating and selecting {num_synthetic_per_class_select} images for User {user_idx + 1}, Class {class_id}\")\n",
    "\n",
    "        class SyntheticDataset(Dataset):\n",
    "            def __init__(self, class_label, root_dir, transform=None):\n",
    "                self.class_label = class_label\n",
    "                self.root_dir = root_dir\n",
    "                self.transform = transform\n",
    "                self.image_files = sorted([f for f in os.listdir(root_dir) if f.endswith('.png')])\n",
    "                if len(self.image_files) == 0:\n",
    "                    raise ValueError(f\"No images found in {root_dir}\")\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.image_files)\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "                image = Image.open(img_path).convert(\"RGB\")\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                return image, self.class_label\n",
    "\n",
    "        synthetic_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        synthetic_dataset = SyntheticDataset(class_id, synthetic_dir, transform=synthetic_transform)\n",
    "        synthetic_datasets[user_idx].append(synthetic_dataset)\n",
    "\n",
    "# Step 4: Verify the final non-IID distribution\n",
    "final_user_data = []\n",
    "for user_idx in range(Num_users):\n",
    "    real_data = user_data[user_idx]\n",
    "    if synthetic_datasets[user_idx]:\n",
    "        final_user_data.append(ConcatDataset([real_data] + synthetic_datasets[user_idx]))\n",
    "    else:\n",
    "        final_user_data.append(real_data)\n",
    "\n",
    "print(\"\\n=== Verifying Final Non-IID Data Distribution Across Users ===\")\n",
    "class_counts_per_user = []\n",
    "for user_idx in range(Num_users):\n",
    "    user_dataset = final_user_data[user_idx]\n",
    "    class_counts = [0] * label_dim\n",
    "    for idx in range(len(user_dataset)):\n",
    "        _, label = user_dataset[idx]\n",
    "        class_counts[label] += 1\n",
    "    class_counts_per_user.append(class_counts)\n",
    "    print(f\"User {user_idx + 1} (Non-IID) Class Distribution: {class_counts}\")\n",
    "    total_samples = len(user_dataset)\n",
    "    class_percentages = [count / total_samples * 100 if total_samples > 0 else 0 for count in class_counts]\n",
    "    print(f\"User {user_idx + 1} (Non-IID) Class Percentages: {[f'{p:.2f}%' for p in class_percentages]}\")\n",
    "\n",
    "total_time = time.time() - total_start_time\n",
    "print(f\"\\nTotal time for the entire script: {format_time(total_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fd45139-7dc2-431f-8c90-6bfc3434a086",
   "metadata": {},
   "outputs": [],
   "source": [
    "##testing 2epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef03dbe-42e9-4b8e-a1f6-fd492891f80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "GPU Name: NVIDIA RTX A5000\n",
      "GPU Memory Allocated: 0.00 MB\n",
      "Path to dataset files: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Inspecting dataset path: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Root: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Dirs: ['Vehicle Type Image Dataset (Version 2) VTID2']\n",
      "Files (first 5): []\n",
      "--------------------------------------------------\n",
      "Root: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\\Vehicle Type Image Dataset (Version 2) VTID2\n",
      "Dirs: ['Hatchback', 'Other', 'Pickup', 'Seden', 'SUV']\n",
      "Files (first 5): []\n",
      "--------------------------------------------------\n",
      "Root: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\\Vehicle Type Image Dataset (Version 2) VTID2\\Hatchback\n",
      "Dirs: []\n",
      "Files (first 5): ['PHOTO_0.jpg', 'PHOTO_1.jpg', 'PHOTO_10.jpg', 'PHOTO_100.jpg', 'PHOTO_101.jpg']\n",
      "--------------------------------------------------\n",
      "Root: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\\Vehicle Type Image Dataset (Version 2) VTID2\\Other\n",
      "Dirs: []\n",
      "Files (first 5): ['PHOTO_0.jpg', 'PHOTO_1.jpg', 'PHOTO_10.jpg', 'PHOTO_100.jpg', 'PHOTO_101.jpg']\n",
      "--------------------------------------------------\n",
      "Root: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\\Vehicle Type Image Dataset (Version 2) VTID2\\Pickup\n",
      "Dirs: []\n",
      "Files (first 5): ['PHOTO_0.jpg', 'PHOTO_1.jpg', 'PHOTO_10.jpg', 'PHOTO_100(1).jpg', 'PHOTO_100.jpg']\n",
      "--------------------------------------------------\n",
      "Root: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\\Vehicle Type Image Dataset (Version 2) VTID2\\Seden\n",
      "Dirs: []\n",
      "Files (first 5): ['PHOTO_0.jpg', 'PHOTO_1.jpg', 'PHOTO_10.jpg', 'PHOTO_100.jpg', 'PHOTO_1000.jpg']\n",
      "--------------------------------------------------\n",
      "Root: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\\Vehicle Type Image Dataset (Version 2) VTID2\\SUV\n",
      "Dirs: []\n",
      "Files (first 5): ['PHOTO_0.jpg', 'PHOTO_1.jpg', 'PHOTO_10.jpg', 'PHOTO_100.jpg', 'PHOTO_101.jpg']\n",
      "--------------------------------------------------\n",
      "Searching for images in C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Found 4793 images across 5 classes.\n",
      "Classes: ['Hatchback', 'Other', 'Pickup', 'Seden', 'SUV']\n",
      "Number of classes (label_dim): 5\n",
      "Class 0: Train=481, Val=60, Test=61\n",
      "Class 1: Train=480, Val=60, Test=60\n",
      "Class 2: Train=1351, Val=168, Test=170\n",
      "Class 3: Train=977, Val=122, Test=123\n",
      "Class 4: Train=544, Val=68, Test=68\n",
      "Training samples: 3833\n",
      "Validation samples: 478\n",
      "Class 0 has 481 real samples before augmentation.\n",
      "Generating 19 synthetic samples for Class 0 to reach 500 samples.\n",
      "Class 0, Epoch 1/2, Train Loss: 12421.2004, Val Loss: 10191.1503, Time: 8.32s\n",
      "Checkpoint saved for Class 0 at epoch 1 to FL_VEHICLE_CVAE_latent_test2_non_iid_2\\checkpoints_cvae_class_0\\cvae_epoch_1.pth\n",
      "Saved latent vectors for Class 0 at epoch 1 to FL_VEHICLE_CVAE_latent_test2_non_iid_2\\checkpoints_cvae_class_0\\latent_vectors_epoch_1\\class_0.pth\n",
      "Class 0, Epoch 2/2, Train Loss: 8972.6893, Val Loss: 10356.7712, Time: 6.67s\n",
      "Checkpoint saved for Class 0 at epoch 2 to FL_VEHICLE_CVAE_latent_test2_non_iid_2\\checkpoints_cvae_class_0\\cvae_epoch_2.pth\n",
      "Saved latent vectors for Class 0 at epoch 2 to FL_VEHICLE_CVAE_latent_test2_non_iid_2\\checkpoints_cvae_class_0\\latent_vectors_epoch_2\\class_0.pth\n",
      "Generating 19 synthetic images for Class 0\n",
      "Saved 1 synthetic images for Class 0\n",
      "Class 0 final dataset length: 500\n",
      "Class 1 has 480 real samples before augmentation.\n",
      "Generating 20 synthetic samples for Class 1 to reach 500 samples.\n",
      "Class 1, Epoch 1/2, Train Loss: 9356.0268, Val Loss: 11716.7412, Time: 7.45s\n",
      "Checkpoint saved for Class 1 at epoch 1 to FL_VEHICLE_CVAE_latent_test2_non_iid_2\\checkpoints_cvae_class_1\\cvae_epoch_1.pth\n",
      "Saved latent vectors for Class 1 at epoch 1 to FL_VEHICLE_CVAE_latent_test2_non_iid_2\\checkpoints_cvae_class_1\\latent_vectors_epoch_1\\class_1.pth\n",
      "Class 1, Epoch 2/2, Train Loss: 6605.0422, Val Loss: 11270.8644, Time: 7.73s\n",
      "Checkpoint saved for Class 1 at epoch 2 to FL_VEHICLE_CVAE_latent_test2_non_iid_2\\checkpoints_cvae_class_1\\cvae_epoch_2.pth\n",
      "Saved latent vectors for Class 1 at epoch 2 to FL_VEHICLE_CVAE_latent_test2_non_iid_2\\checkpoints_cvae_class_1\\latent_vectors_epoch_2\\class_1.pth\n",
      "Generating 20 synthetic images for Class 1\n",
      "Saved 1 synthetic images for Class 1\n",
      "Class 1 final dataset length: 500\n",
      "Class 2 has 1351 real samples before augmentation.\n",
      "Class 2 subsampled to 500 samples.\n",
      "Class 3 has 977 real samples before augmentation.\n",
      "Class 3 subsampled to 500 samples.\n",
      "Class 4 has 544 real samples before augmentation.\n",
      "Class 4 subsampled to 500 samples.\n",
      "Class 0:\n",
      "  Number of samples in train_class_datasets1: 250\n",
      "  Number of samples in train_class_datasets2: 250\n",
      "Class 1:\n",
      "  Number of samples in train_class_datasets1: 250\n",
      "  Number of samples in train_class_datasets2: 250\n",
      "Class 2:\n",
      "  Number of samples in train_class_datasets1: 250\n",
      "  Number of samples in train_class_datasets2: 250\n",
      "Class 3:\n",
      "  Number of samples in train_class_datasets1: 250\n",
      "  Number of samples in train_class_datasets2: 250\n",
      "Class 4:\n",
      "  Number of samples in train_class_datasets1: 250\n",
      "  Number of samples in train_class_datasets2: 250\n",
      "Class 0:\n",
      "  Number of samples in train_class_datasets2_part1 (70%): 175\n",
      "  Number of samples in train_class_datasets2_part2 (30%): 75\n",
      "Class 1:\n",
      "  Number of samples in train_class_datasets2_part1 (70%): 175\n",
      "  Number of samples in train_class_datasets2_part2 (30%): 75\n",
      "Class 2:\n",
      "  Number of samples in train_class_datasets2_part1 (70%): 175\n",
      "  Number of samples in train_class_datasets2_part2 (30%): 75\n",
      "Class 3:\n",
      "  Number of samples in train_class_datasets2_part1 (70%): 175\n",
      "  Number of samples in train_class_datasets2_part2 (30%): 75\n",
      "Class 4:\n",
      "  Number of samples in train_class_datasets2_part1 (70%): 175\n",
      "  Number of samples in train_class_datasets2_part2 (30%): 75\n",
      "User 1:\n",
      "Number of samples in the user dataset: 725\n",
      "User 2:\n",
      "Number of samples in the user dataset: 425\n",
      "User 3:\n",
      "Number of samples in the user dataset: 425\n",
      "User 4:\n",
      "Number of samples in the user dataset: 425\n",
      "User 5:\n",
      "Number of samples in the user dataset: 250\n",
      "User 1 dataset length: 725\n",
      "User 1, Sample 0: Label=0, Data shape=torch.Size([3, 128, 128])\n",
      "User 1, Sample 1: Label=0, Data shape=torch.Size([3, 128, 128])\n",
      "User 1, Sample 2: Label=0, Data shape=torch.Size([3, 128, 128])\n",
      "User 1, Sample 3: Label=0, Data shape=torch.Size([3, 128, 128])\n",
      "User 1, Sample 4: Label=0, Data shape=torch.Size([3, 128, 128])\n",
      "User 1, Epoch 1/2, Train Loss: 10903.9323, Val Loss: 9520.1090, Time: 9.99s\n",
      "Checkpoint saved for User 1 at epoch 1 to FL_VEHICLE_CVAE_latent_test2_non_iid_2\\checkpoints_cvae_user_1\\cvae_epoch_1.pth\n",
      "Decoder saved for User 1 at epoch 1 to FL_VEHICLE_CVAE_latent_test2_non_iid_2\\checkpoints_cvae_user_1\\decoder\\decoder_epoch_1.pth\n",
      "Saved latent vectors for User 1, Class 0 at epoch 1 to FL_VEHICLE_CVAE_latent_test2_non_iid_2\\checkpoints_cvae_user_1\\latent_vectors_epoch_1\\class_0.pth\n",
      "Saved latent vectors for User 1, Class 1 at epoch 1 to FL_VEHICLE_CVAE_latent_test2_non_iid_2\\checkpoints_cvae_user_1\\latent_vectors_epoch_1\\class_1.pth\n",
      "Saved latent vectors for User 1, Class 2 at epoch 1 to FL_VEHICLE_CVAE_latent_test2_non_iid_2\\checkpoints_cvae_user_1\\latent_vectors_epoch_1\\class_2.pth\n",
      "Saved latent vectors for User 1, Class 3 at epoch 1 to FL_VEHICLE_CVAE_latent_test2_non_iid_2\\checkpoints_cvae_user_1\\latent_vectors_epoch_1\\class_3.pth\n",
      "Saved latent vectors for User 1, Class 4 at epoch 1 to FL_VEHICLE_CVAE_latent_test2_non_iid_2\\checkpoints_cvae_user_1\\latent_vectors_epoch_1\\class_4.pth\n",
      "User 1, Epoch 2/2, Train Loss: 8862.8126, Val Loss: 8777.7405, Time: 9.57s\n",
      "Checkpoint saved for User 1 at epoch 2 to FL_VEHICLE_CVAE_latent_test2_non_iid_2\\checkpoints_cvae_user_1\\cvae_epoch_2.pth\n",
      "Decoder saved for User 1 at epoch 2 to FL_VEHICLE_CVAE_latent_test2_non_iid_2\\checkpoints_cvae_user_1\\decoder\\decoder_epoch_2.pth\n",
      "Saved latent vectors for User 1, Class 0 at epoch 2 to FL_VEHICLE_CVAE_latent_test2_non_iid_2\\checkpoints_cvae_user_1\\latent_vectors_epoch_2\\class_0.pth\n",
      "Saved latent vectors for User 1, Class 1 at epoch 2 to FL_VEHICLE_CVAE_latent_test2_non_iid_2\\checkpoints_cvae_user_1\\latent_vectors_epoch_2\\class_1.pth\n",
      "Saved latent vectors for User 1, Class 2 at epoch 2 to FL_VEHICLE_CVAE_latent_test2_non_iid_2\\checkpoints_cvae_user_1\\latent_vectors_epoch_2\\class_2.pth\n",
      "Saved latent vectors for User 1, Class 3 at epoch 2 to FL_VEHICLE_CVAE_latent_test2_non_iid_2\\checkpoints_cvae_user_1\\latent_vectors_epoch_2\\class_3.pth\n",
      "Saved latent vectors for User 1, Class 4 at epoch 2 to FL_VEHICLE_CVAE_latent_test2_non_iid_2\\checkpoints_cvae_user_1\\latent_vectors_epoch_2\\class_4.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Subset, Dataset, ConcatDataset\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import kagglehub\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Start total script timer\n",
    "total_start_time = time.time()\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = \"FL_VEHICLE_CVAE_latent_test2_non_iid_2\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "RESIZE = 128\n",
    "original_dim = RESIZE * RESIZE * 3\n",
    "intermediate_dim = 512\n",
    "latent_dim = 256\n",
    "num_classes = 5\n",
    "batch_size = 8\n",
    "epochs = 2  # Changed from 3000 to 2\n",
    "learning_rate = 1e-4\n",
    "beta_start = 1\n",
    "beta_end = 10\n",
    "device = torch.device('cuda:1' if torch.cuda.device_count() > 1 else 'cuda')\n",
    "\n",
    "# Print GPU information\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "\n",
    "# Download Vehicle Type Image Dataset from Kaggle\n",
    "try:\n",
    "    path = kagglehub.dataset_download(\"sujaykapadnis/vehicle-type-image-dataset\")\n",
    "    print(\"Path to dataset files:\", path)\n",
    "    dataset_path = path\n",
    "except Exception as e:\n",
    "    print(f\"Failed to download dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Transform for CVAE (normalize to [-1, 1])\n",
    "cvae_input_transform = transforms.Compose([\n",
    "    transforms.Resize((RESIZE, RESIZE)),\n",
    "    transforms.ToTensor(),  # [0, 1]\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # [-1, 1]\n",
    "])\n",
    "\n",
    "# Debug dataset directory structure\n",
    "print(\"Inspecting dataset path:\", dataset_path)\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    print(f\"Root: {root}\")\n",
    "    print(f\"Dirs: {dirs}\")\n",
    "    print(f\"Files (first 5): {files[:5]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Custom Dataset for the Vehicle Type Dataset\n",
    "class VehicleTypeDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_names = []\n",
    "        self.class_to_idx = {}\n",
    "\n",
    "        print(f\"Searching for images in {root_dir}\")\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            image_files = [f for f in files if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "            if image_files:\n",
    "                class_name = os.path.basename(root)\n",
    "                if class_name not in self.class_to_idx:\n",
    "                    self.class_names.append(class_name)\n",
    "                    self.class_to_idx[class_name] = len(self.class_names) - 1\n",
    "                for img_file in image_files:\n",
    "                    img_path = os.path.join(root, img_file)\n",
    "                    try:\n",
    "                        Image.open(img_path).verify()\n",
    "                        self.images.append(img_path)\n",
    "                        self.labels.append(self.class_to_idx[class_name])\n",
    "                    except:\n",
    "                        print(f\"Skipping corrupted image: {img_path}\")\n",
    "\n",
    "        if len(self.class_names) != num_classes:\n",
    "            raise ValueError(f\"Expected {num_classes} classes, found {len(self.class_names)}\")\n",
    "        print(f\"Found {len(self.images)} images across {len(self.class_names)} classes.\")\n",
    "        print(f\"Classes: {self.class_names}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Load the dataset\n",
    "dataset = VehicleTypeDataset(root_dir=dataset_path, transform=cvae_input_transform)\n",
    "\n",
    "# Update label_dim\n",
    "label_dim = len(dataset.class_names)\n",
    "print(f\"Number of classes (label_dim): {label_dim}\")\n",
    "\n",
    "# Step 1: Split dataset into train, validation, and test sets per class\n",
    "validation_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "train_ratio = 0.8\n",
    "\n",
    "class_datasets = [[] for _ in range(label_dim)]\n",
    "for idx in range(len(dataset)):\n",
    "    label = dataset.labels[idx]\n",
    "    class_datasets[label].append(idx)\n",
    "\n",
    "train_indices_per_class = []\n",
    "val_indices_per_class = []\n",
    "test_indices_per_class = []\n",
    "\n",
    "for class_idx in range(label_dim):\n",
    "    indices = class_datasets[class_idx]\n",
    "    total_samples = len(indices)\n",
    "    num_train = int(total_samples * train_ratio)\n",
    "    num_val = int(total_samples * validation_ratio)\n",
    "    num_test = total_samples - num_train - num_val\n",
    "\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_indices = indices[:num_train]\n",
    "    val_indices = indices[num_train:num_train + num_val]\n",
    "    test_indices = indices[num_train + num_val:]\n",
    "\n",
    "    train_indices_per_class.append(train_indices)\n",
    "    val_indices_per_class.append(val_indices)\n",
    "    test_indices_per_class.append(test_indices)\n",
    "\n",
    "    print(f\"Class {class_idx}: Train={len(train_indices)}, Val={len(val_indices)}, Test={len(test_indices)}\")\n",
    "\n",
    "# Verify no overlap\n",
    "for class_idx in range(label_dim):\n",
    "    train_set = set(train_indices_per_class[class_idx])\n",
    "    val_set = set(val_indices_per_class[class_idx])\n",
    "    test_set = set(test_indices_per_class[class_idx])\n",
    "    assert len(train_set.intersection(val_set)) == 0, f\"Overlap between train and val for class {class_idx}\"\n",
    "    assert len(train_set.intersection(test_set)) == 0, f\"Overlap between train and test for class {class_idx}\"\n",
    "    assert len(val_set.intersection(test_set)) == 0, f\"Overlap between val and test for class {class_idx}\"\n",
    "\n",
    "# Create training and validation datasets\n",
    "train_dataset = Subset(dataset, [idx for class_indices in train_indices_per_class for idx in class_indices])\n",
    "val_dataset = Subset(dataset, [idx for class_indices in val_indices_per_class for idx in class_indices])\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Step 1.5: Ensure 500 samples per class by generating synthetic samples for underrepresented classes\n",
    "target_samples_per_class = 500\n",
    "train_class_datasets = []\n",
    "cvae_per_class = {}\n",
    "\n",
    "# Weight initialization\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# Encoder (Convolutional)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, latent_dim, num_classes):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 1024, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_hidden = nn.Linear(1024 * 2 * 2 + num_classes, hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.encoder_cnn(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x_with_y = torch.cat([x, y], dim=-1)\n",
    "        h = F.relu(self.fc_hidden(x_with_y))\n",
    "        h = self.dropout(h)\n",
    "        z_mean = self.fc_mean(h)\n",
    "        z_logvar = self.fc_logvar(h)\n",
    "        z_logvar = torch.clamp(z_logvar, min=-10, max=10)\n",
    "        return z_mean, z_logvar\n",
    "\n",
    "# Decoder (Convolutional)\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, num_classes):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim + num_classes, hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc_to_cnn = nn.Linear(hidden_dim, 1024 * 2 * 2)\n",
    "        self.decoder_cnn = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 1024, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 512, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        z_with_y = torch.cat([z, y], dim=-1)\n",
    "        h = F.relu(self.fc(z_with_y))\n",
    "        h = self.dropout(h)\n",
    "        h = F.relu(self.fc_to_cnn(h))\n",
    "        h = h.view(-1, 1024, 2, 2)\n",
    "        x_reconstructed = self.decoder_cnn(h)\n",
    "        x_reconstructed = torch.clamp(x_reconstructed, min=-1, max=1)\n",
    "        return x_reconstructed\n",
    "\n",
    "# Conditional VAE\n",
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def reparameterize(self, z_mean, z_logvar):\n",
    "        std = torch.exp(0.5 * z_logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return z_mean + eps * std\n",
    "\n",
    "    def forward(self, data, y):\n",
    "        z_mean, z_logvar = self.encoder(data, y)\n",
    "        z = self.reparameterize(z_mean, z_logvar)\n",
    "        x_reconstructed = self.decoder(z, y)\n",
    "        return x_reconstructed, z_mean, z_logvar\n",
    "\n",
    "# Loss Function\n",
    "def cvae_loss(data, x_reconstructed, z_mean, z_logvar, beta=1.0):\n",
    "    batch_size = data.size(0)\n",
    "    mse_loss = F.mse_loss(x_reconstructed, data, reduction='sum') / batch_size\n",
    "    kl_loss = -0.5 * torch.sum(1 + z_logvar - z_mean.pow(2) - z_logvar.exp()) / batch_size\n",
    "    return mse_loss + beta * kl_loss\n",
    "\n",
    "# Function to format time\n",
    "def format_time(seconds):\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = seconds % 60\n",
    "    if hours > 0:\n",
    "        return f\"{hours}h {minutes}m {secs:.2f}s\"\n",
    "    elif minutes > 0:\n",
    "        return f\"{minutes}m {secs:.2f}s\"\n",
    "    else:\n",
    "        return f\"{secs:.2f}s\"\n",
    "\n",
    "# Generate synthetic samples for classes with fewer than 500 samples\n",
    "for class_idx in range(label_dim):\n",
    "    class_indices = train_indices_per_class[class_idx]\n",
    "    num_real_samples = len(class_indices)\n",
    "    print(f\"Class {class_idx} has {num_real_samples} real samples before augmentation.\")\n",
    "\n",
    "    if num_real_samples >= target_samples_per_class:\n",
    "        class_indices = np.random.choice(class_indices, target_samples_per_class, replace=False).tolist()\n",
    "        class_dataset = Subset(dataset, class_indices)\n",
    "        train_class_datasets.append(class_dataset)\n",
    "        print(f\"Class {class_idx} subsampled to {len(class_dataset)} samples.\")\n",
    "        continue\n",
    "\n",
    "    num_synthetic_needed = target_samples_per_class - num_real_samples\n",
    "    print(f\"Generating {num_synthetic_needed} synthetic samples for Class {class_idx} to reach {target_samples_per_class} samples.\")\n",
    "\n",
    "    class_dataset = Subset(dataset, class_indices)\n",
    "    class_loader = DataLoader(class_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True, drop_last=True)\n",
    "\n",
    "    encoder = Encoder(intermediate_dim, latent_dim, num_classes).to(device)\n",
    "    decoder = Decoder(latent_dim, intermediate_dim, num_classes).to(device)\n",
    "    cvae = ConditionalVAE(encoder, decoder).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(cvae.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.5)\n",
    "\n",
    "    class_checkpoint_dir = os.path.join(output_dir, f'checkpoints_cvae_class_{class_idx}')\n",
    "    os.makedirs(class_checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    cvae.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        beta = beta_start + (beta_end - beta_start) * ((epoch - 1) / (epochs - 1)) if epochs > 1 else beta_end\n",
    "\n",
    "        train_loss = 0\n",
    "        batches_processed = 0\n",
    "\n",
    "        for batch_idx, (data, labels) in enumerate(class_loader):\n",
    "            try:\n",
    "                data = data.to(device)\n",
    "                y = F.one_hot(labels, num_classes=num_classes).float().to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                x_recon, z_mean, z_logvar = cvae(data, y)\n",
    "                loss = cvae_loss(data, x_recon, z_mean, z_logvar, beta)\n",
    "\n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    print(f\"NaN/Inf loss detected at epoch {epoch}, batch {batch_idx + 1} for Class {class_idx}\")\n",
    "                    continue\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(cvae.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                batches_processed += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error at epoch {epoch}, batch {batch_idx + 1} for Class {class_idx}: {e}\")\n",
    "                if \"out of memory\" in str(e).lower():\n",
    "                    print(\"Out of memory error detected. Clearing cache...\")\n",
    "                    if device.type == 'cuda':\n",
    "                        torch.cuda.empty_cache()\n",
    "                continue\n",
    "\n",
    "        avg_train_loss = train_loss / batches_processed if batches_processed > 0 else float('inf')\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        cvae.eval()\n",
    "        val_loss = 0\n",
    "        val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in val_loader:\n",
    "                data = data.to(device)\n",
    "                y = F.one_hot(labels, num_classes=num_classes).float().to(device)\n",
    "                x_recon, z_mean, z_logvar = cvae(data, y)\n",
    "                loss = cvae_loss(data, x_recon, z_mean, z_logvar, beta=1.0)\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "\n",
    "        avg_val_loss = val_loss / val_batches if val_batches > 0 else 0\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"Class {class_idx}, Epoch {epoch}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Time: {format_time(epoch_time)}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Save checkpoint after epoch 1 and at the final epoch\n",
    "        if epoch == 1 or epoch == epochs:\n",
    "            checkpoint_path = os.path.join(class_checkpoint_dir, f'cvae_epoch_{epoch}.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': cvae.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Checkpoint saved for Class {class_idx} at epoch {epoch} to {checkpoint_path}\")\n",
    "\n",
    "            latent_dir = os.path.join(class_checkpoint_dir, f'latent_vectors_epoch_{epoch}')\n",
    "            os.makedirs(latent_dir, exist_ok=True)\n",
    "\n",
    "            cvae.eval()\n",
    "            with torch.no_grad():\n",
    "                latent_vectors = {'z_mean': [], 'z_logvar': [], 'labels': []}\n",
    "                for data, labels in class_loader:\n",
    "                    data = data.to(device)\n",
    "                    y = F.one_hot(labels, num_classes=num_classes).float().to(device)\n",
    "                    z_mean, z_logvar = cvae.encoder(data, y)\n",
    "                    for i in range(len(labels)):\n",
    "                        latent_vectors['z_mean'].append(z_mean[i].cpu())\n",
    "                        latent_vectors['z_logvar'].append(z_logvar[i].cpu())\n",
    "                        latent_vectors['labels'].append(labels[i].item())\n",
    "\n",
    "                if latent_vectors['z_mean']:\n",
    "                    z_mean = torch.stack(latent_vectors['z_mean'])\n",
    "                    z_logvar = torch.stack(latent_vectors['z_logvar'])\n",
    "                    labels = torch.tensor(latent_vectors['labels'])\n",
    "                    save_path = os.path.join(latent_dir, f'class_{class_idx}.pth')\n",
    "                    torch.save({\n",
    "                        'z_mean': z_mean,\n",
    "                        'z_logvar': z_logvar,\n",
    "                        'labels': labels\n",
    "                    }, save_path)\n",
    "                    print(f\"Saved latent vectors for Class {class_idx} at epoch {epoch} to {save_path}\")\n",
    "\n",
    "    cvae_per_class[class_idx] = cvae\n",
    "\n",
    "    synthetic_dir = os.path.join(output_dir, f'synthetic_class_{class_idx}')\n",
    "    os.makedirs(synthetic_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Generating {num_synthetic_needed} synthetic images for Class {class_idx}\")\n",
    "    synthetic_images = []\n",
    "    latent_path = os.path.join(class_checkpoint_dir, f'latent_vectors_epoch_{epochs}', f'class_{class_idx}.pth')\n",
    "    latent_data = torch.load(latent_path, weights_only=False)\n",
    "    z_mean_all = latent_data['z_mean'].to(device)\n",
    "    z_logvar_all = latent_data['z_logvar'].to(device)\n",
    "\n",
    "    cvae.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_synthetic_needed):\n",
    "            z = cvae.reparameterize(z_mean_all[i % len(z_mean_all)].unsqueeze(0), \n",
    "                                    z_logvar_all[i % len(z_mean_all)].unsqueeze(0))\n",
    "            y = F.one_hot(torch.tensor([class_idx]), num_classes=label_dim).float().to(device)\n",
    "            synthetic_img = cvae.decoder(z, y).cpu()\n",
    "            synthetic_images.append(synthetic_img)\n",
    "\n",
    "    for idx, img in enumerate(synthetic_images):\n",
    "        img_path = os.path.join(synthetic_dir, f'image_{idx + 1}.png')\n",
    "        try:\n",
    "            img = img.view(3, RESIZE, RESIZE)\n",
    "            img = img * 0.5 + 0.5\n",
    "            img = img.clamp(0, 1)\n",
    "            img = transforms.ToPILImage()(img)\n",
    "            img.save(img_path)\n",
    "            if (idx + 1) % 100 == 0 or idx == 0:\n",
    "                print(f\"Saved {idx + 1} synthetic images for Class {class_idx}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving image {img_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    class SyntheticDataset(Dataset):\n",
    "        def __init__(self, class_label, root_dir, transform=None):\n",
    "            self.class_label = class_label\n",
    "            self.root_dir = root_dir\n",
    "            self.transform = transform\n",
    "            self.image_files = sorted([f for f in os.listdir(root_dir) if f.endswith('.png')])\n",
    "            if len(self.image_files) == 0:\n",
    "                raise ValueError(f\"No images found in {root_dir}\")\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.image_files)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, self.class_label\n",
    "\n",
    "    synthetic_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    synthetic_dataset = SyntheticDataset(class_idx, synthetic_dir, transform=synthetic_transform)\n",
    "\n",
    "    combined_dataset = ConcatDataset([class_dataset, synthetic_dataset])\n",
    "    combined_indices = list(range(len(combined_dataset)))\n",
    "    if len(combined_indices) > target_samples_per_class:\n",
    "        combined_indices = np.random.choice(combined_indices, target_samples_per_class, replace=False).tolist()\n",
    "    final_class_dataset = Subset(combined_dataset, combined_indices)\n",
    "    train_class_datasets.append(final_class_dataset)\n",
    "    print(f\"Class {class_idx} final dataset length: {len(final_class_dataset)}\")\n",
    "\n",
    "# Step 1.6: Split train_class_datasets like the first code\n",
    "# Function to split a dataset into two parts\n",
    "def split_dataset(dataset, split_ratio):\n",
    "    train_size = int(np.round(split_ratio * len(dataset)))\n",
    "    remaining_size = len(dataset) - train_size\n",
    "    train_dataset, remaining_dataset = torch.utils.data.random_split(dataset, [train_size, remaining_size])\n",
    "    return train_dataset, remaining_dataset\n",
    "\n",
    "# Split each class dataset into two halves (50/50)\n",
    "split_ratio = 0.5\n",
    "split_datasets = []\n",
    "train_class_datasets1 = []\n",
    "train_class_datasets2 = []\n",
    "\n",
    "for class_dataset in train_class_datasets:\n",
    "    train_class_dataset1, train_class_dataset2 = split_dataset(class_dataset, split_ratio)\n",
    "    split_datasets.append((train_class_dataset1, train_class_dataset2))\n",
    "    train_class_datasets1.append(train_class_dataset1)\n",
    "    train_class_datasets2.append(train_class_dataset2)\n",
    "\n",
    "for i, (train_class_dataset1, train_class_dataset2) in enumerate(split_datasets):\n",
    "    print(f\"Class {i}:\")\n",
    "    print(f\"  Number of samples in train_class_datasets1: {len(train_class_dataset1)}\")\n",
    "    print(f\"  Number of samples in train_class_datasets2: {len(train_class_dataset2)}\")\n",
    "\n",
    "# Further split train_class_datasets2 into 70% and 30% parts\n",
    "split_ratio = 0.7\n",
    "split_datasets2 = []\n",
    "train_class_datasets2_part1 = []\n",
    "train_class_datasets2_part2 = []\n",
    "\n",
    "for class_dataset in train_class_datasets2:\n",
    "    part1_dataset, part2_dataset = split_dataset(class_dataset, split_ratio)\n",
    "    split_datasets2.append((part1_dataset, part2_dataset))\n",
    "    train_class_datasets2_part1.append(part1_dataset)\n",
    "    train_class_datasets2_part2.append(part2_dataset)\n",
    "\n",
    "for i, (part1_dataset, part2_dataset) in enumerate(split_datasets2):\n",
    "    print(f\"Class {i}:\")\n",
    "    print(f\"  Number of samples in train_class_datasets2_part1 (70%): {len(part1_dataset)}\")\n",
    "    print(f\"  Number of samples in train_class_datasets2_part2 (30%): {len(part2_dataset)}\")\n",
    "\n",
    "# Step 2: Define user datasets as per the first code\n",
    "Num_users = 5\n",
    "user_data = []\n",
    "user_classes = {}  # For synthetic data generation\n",
    "\n",
    "# Assign datasets to users\n",
    "user_data.append(ConcatDataset([train_class_datasets1[0], train_class_datasets2[1], train_class_datasets2_part2[2], train_class_datasets2_part2[3], train_class_datasets2_part2[4]]))\n",
    "user_classes[0] = [0, 1, 2, 3, 4]\n",
    "\n",
    "user_data.append(ConcatDataset([train_class_datasets1[1], train_class_datasets2_part1[2]]))\n",
    "user_classes[1] = [1, 2]\n",
    "\n",
    "user_data.append(ConcatDataset([train_class_datasets1[2], train_class_datasets2_part1[3]]))\n",
    "user_classes[2] = [2, 3]\n",
    "\n",
    "user_data.append(ConcatDataset([train_class_datasets1[3], train_class_datasets2_part1[4]]))\n",
    "user_classes[3] = [3, 4]\n",
    "\n",
    "user_data.append(ConcatDataset([train_class_datasets1[4]]))\n",
    "user_classes[4] = [4]\n",
    "\n",
    "for i, user_dataset in enumerate(user_data):\n",
    "    print(f\"User {i + 1}:\")\n",
    "    print(f\"Number of samples in the user dataset: {len(user_dataset)}\")\n",
    "\n",
    "cvae_users = {}\n",
    "train_losses_users = {}\n",
    "val_losses_users = {}\n",
    "\n",
    "for user_idx in range(Num_users):\n",
    "    user_start_time = time.time()\n",
    "\n",
    "    user_dataset = user_data[user_idx]\n",
    "    print(f\"User {user_idx + 1} dataset length: {len(user_dataset)}\")\n",
    "\n",
    "    try:\n",
    "        for i in range(min(5, len(user_dataset))):\n",
    "            sample, label = user_dataset[i]\n",
    "            print(f\"User {user_idx + 1}, Sample {i}: Label={label}, Data shape={sample.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing samples for User {user_idx + 1}: {e}\")\n",
    "        raise\n",
    "\n",
    "    user_loader = DataLoader(user_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True, drop_last=True)\n",
    "\n",
    "    encoder = Encoder(intermediate_dim, latent_dim, num_classes).to(device)\n",
    "    decoder = Decoder(latent_dim, intermediate_dim, num_classes).to(device)\n",
    "    cvae = ConditionalVAE(encoder, decoder).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(cvae.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.5)\n",
    "\n",
    "    checkpoint_dir = os.path.join(output_dir, f'checkpoints_cvae_user_{user_idx + 1}')\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    cvae.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        beta = beta_start + (beta_end - beta_start) * ((epoch - 1) / (epochs - 1)) if epochs > 1 else beta_end\n",
    "\n",
    "        train_loss = 0\n",
    "        batches_processed = 0\n",
    "\n",
    "        for batch_idx, (data, labels) in enumerate(user_loader):\n",
    "            try:\n",
    "                data = data.to(device)\n",
    "                y = F.one_hot(labels, num_classes=num_classes).float().to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                x_recon, z_mean, z_logvar = cvae(data, y)\n",
    "                loss = cvae_loss(data, x_recon, z_mean, z_logvar, beta)\n",
    "\n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    print(f\"NaN/Inf loss detected at epoch {epoch}, batch {batch_idx + 1} for User {user_idx + 1}\")\n",
    "                    continue\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(cvae.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                batches_processed += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error at epoch {epoch}, batch {batch_idx + 1} for User {user_idx + 1}: {e}\")\n",
    "                if \"out of memory\" in str(e).lower():\n",
    "                    print(\"Out of memory error detected. Clearing cache...\")\n",
    "                    if device.type == 'cuda':\n",
    "                        torch.cuda.empty_cache()\n",
    "                continue\n",
    "\n",
    "        avg_train_loss = train_loss / batches_processed if batches_processed > 0 else float('inf')\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        cvae.eval()\n",
    "        val_loss = 0\n",
    "        val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in val_loader:\n",
    "                data = data.to(device)\n",
    "                y = F.one_hot(labels, num_classes=num_classes).float().to(device)\n",
    "                x_recon, z_mean, z_logvar = cvae(data, y)\n",
    "                loss = cvae_loss(data, x_recon, z_mean, z_logvar, beta=1.0)\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "\n",
    "        avg_val_loss = val_loss / val_batches if val_batches > 0 else 0\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"User {user_idx + 1}, Epoch {epoch}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Time: {format_time(epoch_time)}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Save checkpoint after epoch 1 and at the final epoch\n",
    "        if epoch == 1 or epoch == epochs:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'cvae_epoch_{epoch}.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': cvae.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Checkpoint saved for User {user_idx + 1} at epoch {epoch} to {checkpoint_path}\")\n",
    "\n",
    "            decoder_dir = os.path.join(checkpoint_dir, 'decoder')\n",
    "            os.makedirs(decoder_dir, exist_ok=True)\n",
    "            decoder_path = os.path.join(decoder_dir, f'decoder_epoch_{epoch}.pth')\n",
    "            torch.save(cvae.decoder.state_dict(), decoder_path)\n",
    "            print(f\"Decoder saved for User {user_idx + 1} at epoch {epoch} to {decoder_path}\")\n",
    "\n",
    "            latent_dir = os.path.join(checkpoint_dir, f'latent_vectors_epoch_{epoch}')\n",
    "            os.makedirs(latent_dir, exist_ok=True)\n",
    "\n",
    "            cvae.eval()\n",
    "            with torch.no_grad():\n",
    "                latent_vectors = {cls: {'z_mean': [], 'z_logvar': [], 'labels': []} for cls in user_classes[user_idx]}\n",
    "                for data, labels in user_loader:\n",
    "                    data = data.to(device)\n",
    "                    y = F.one_hot(labels, num_classes=num_classes).float().to(device)\n",
    "                    z_mean, z_logvar = cvae.encoder(data, y)\n",
    "                    for i, label in enumerate(labels):\n",
    "                        latent_vectors[label.item()]['z_mean'].append(z_mean[i].cpu())\n",
    "                        latent_vectors[label.item()]['z_logvar'].append(z_logvar[i].cpu())\n",
    "                        latent_vectors[label.item()]['labels'].append(label.item())\n",
    "\n",
    "                for cls in user_classes[user_idx]:\n",
    "                    if latent_vectors[cls]['z_mean']:\n",
    "                        z_mean = torch.stack(latent_vectors[cls]['z_mean'])\n",
    "                        z_logvar = torch.stack(latent_vectors[cls]['z_logvar'])\n",
    "                        labels = torch.tensor(latent_vectors[cls]['labels'])\n",
    "                        save_path = os.path.join(latent_dir, f'class_{cls}.pth')\n",
    "                        torch.save({\n",
    "                            'z_mean': z_mean,\n",
    "                            'z_logvar': z_logvar,\n",
    "                            'labels': labels\n",
    "                        }, save_path)\n",
    "                        print(f\"Saved latent vectors for User {user_idx + 1}, Class {cls} at epoch {epoch} to {save_path}\")\n",
    "\n",
    "    train_losses_users[user_idx] = train_losses\n",
    "    val_losses_users[user_idx] = val_losses\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'User {user_idx + 1} CVAE Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    loss_plot_path = os.path.join(checkpoint_dir, 'loss_plot.png')\n",
    "    plt.savefig(loss_plot_path)\n",
    "    plt.close()\n",
    "    print(f\"Loss plot saved for User {user_idx + 1} to {loss_plot_path}\")\n",
    "\n",
    "    cvae_users[user_idx] = cvae\n",
    "\n",
    "    user_time = time.time() - user_start_time\n",
    "    print(f\"Total time for User {user_idx + 1} CVAE training: {format_time(user_time)}\\n\")\n",
    "\n",
    "# Step 3: Share latent vectors and decoder parameters to generate synthetic data\n",
    "class_to_users = {cls: [] for cls in range(label_dim)}\n",
    "for user_idx, classes in user_classes.items():\n",
    "    for cls in classes:\n",
    "        class_to_users[cls].append(user_idx)\n",
    "\n",
    "sharing_scheme = {}\n",
    "for cls in range(label_dim):\n",
    "    target_users = [user_idx for user_idx in range(Num_users) if cls not in user_classes[user_idx]]\n",
    "    if target_users and class_to_users[cls]:\n",
    "        source_user = class_to_users[cls][0]\n",
    "        sharing_scheme[f'class_{cls}'] = {\n",
    "            'source_user': source_user,\n",
    "            'target_users': target_users,\n",
    "            'share_decoder': True\n",
    "        }\n",
    "\n",
    "synthetic_datasets = [[] for _ in range(Num_users)]\n",
    "num_synthetic_per_class_generate = 1000\n",
    "num_synthetic_per_class_select = 500\n",
    "\n",
    "for class_key, scheme in sharing_scheme.items():\n",
    "    class_id = int(class_key.split('_')[1])\n",
    "    source_user = scheme['source_user']\n",
    "    target_users = scheme['target_users']\n",
    "\n",
    "    latent_dir = os.path.join(output_dir, f'checkpoints_cvae_user_{source_user+1}', f'latent_vectors_epoch_{epochs}')\n",
    "    latent_path = os.path.join(latent_dir, f'class_{class_id}.pth')\n",
    "    latent_data = torch.load(latent_path, weights_only=False)\n",
    "    print(f\"Loaded latent data for User {source_user+1}, Class {class_id}: z_mean shape={latent_data['z_mean'].shape}\")\n",
    "    z_mean_all = latent_data['z_mean'].to(device)\n",
    "    z_logvar_all = latent_data['z_logvar'].to(device)\n",
    "\n",
    "    decoder_dir = os.path.join(output_dir, f'checkpoints_cvae_user_{source_user+1}', 'decoder')\n",
    "    decoder_path = os.path.join(decoder_dir, f'decoder_epoch_{epochs}.pth')\n",
    "    print(f\"Loading decoder for User {source_user + 1}, Class {class_id}\")\n",
    "\n",
    "    shared_cvae = ConditionalVAE(Encoder(intermediate_dim, latent_dim, num_classes), Decoder(latent_dim, intermediate_dim, num_classes)).to(device)\n",
    "\n",
    "    if scheme['share_decoder']:\n",
    "        decoder_params = torch.load(decoder_path, weights_only=False)\n",
    "        shared_cvae.decoder.load_state_dict(decoder_params)\n",
    "        print(f\"Loaded decoder parameters: {decoder_path}\")\n",
    "    else:\n",
    "        print(f\"Warning: No decoder shared for user {source_user + 1}, Class {class_id}. Using random decoder.\")\n",
    "\n",
    "    for user_idx in target_users:\n",
    "        synthetic_dir = os.path.join(output_dir, f'synthetic_user_{user_idx + 1}', f'class_{class_id}')\n",
    "        os.makedirs(synthetic_dir, exist_ok=True)\n",
    "\n",
    "        print(f\"Generating {num_synthetic_per_class_generate} synthetic images for User {user_idx + 1}, Class {class_id}\")\n",
    "        synthetic_images = []\n",
    "        mean_intensities = []\n",
    "\n",
    "        shared_cvae.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(num_synthetic_per_class_generate):\n",
    "                z = shared_cvae.reparameterize(z_mean_all[i % len(z_mean_all)].unsqueeze(0), \n",
    "                                               z_logvar_all[i % len(z_mean_all)].unsqueeze(0))\n",
    "                y = F.one_hot(torch.tensor([class_id]), num_classes=label_dim).float().to(device)\n",
    "                synthetic_img = shared_cvae.decoder(z, y).cpu()\n",
    "                mean_intensity = synthetic_img.mean().item()\n",
    "                synthetic_images.append(synthetic_img)\n",
    "                mean_intensities.append(mean_intensity)\n",
    "\n",
    "                if (i + 1) % 200 == 0:\n",
    "                    print(f\"Generated {i + 1} images for User {user_idx + 1}, Class {class_id}\")\n",
    "\n",
    "        print(f\"Selecting top {num_synthetic_per_class_select} images for User {user_idx + 1}, Class {class_id}\")\n",
    "        sorted_indices = np.argsort(mean_intensities)[::-1]\n",
    "        selected_indices = sorted_indices[:num_synthetic_per_class_select]\n",
    "\n",
    "        for idx, img_idx in enumerate(selected_indices):\n",
    "            img_path = os.path.join(synthetic_dir, f'image_{idx + 1}.png')\n",
    "            try:\n",
    "                img = synthetic_images[img_idx].view(3, RESIZE, RESIZE)\n",
    "                img = img * 0.5 + 0.5\n",
    "                img = img.clamp(0, 1)\n",
    "                img = transforms.ToPILImage()(img)\n",
    "                img.save(img_path)\n",
    "                if (idx + 1) % 100 == 0 or idx == 0:\n",
    "                    print(f\"Saved {idx + 1} selected images for User {user_idx + 1}, Class {class_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving image {img_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"Completed generating and selecting {num_synthetic_per_class_select} images for User {user_idx + 1}, Class {class_id}\")\n",
    "\n",
    "        class SyntheticDataset(Dataset):\n",
    "            def __init__(self, class_label, root_dir, transform=None):\n",
    "                self.class_label = class_label\n",
    "                self.root_dir = root_dir\n",
    "                self.transform = transform\n",
    "                self.image_files = sorted([f for f in os.listdir(root_dir) if f.endswith('.png')])\n",
    "                if len(self.image_files) == 0:\n",
    "                    raise ValueError(f\"No images found in {root_dir}\")\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.image_files)\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "                image = Image.open(img_path).convert(\"RGB\")\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                return image, self.class_label\n",
    "\n",
    "        synthetic_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        synthetic_dataset = SyntheticDataset(class_id, synthetic_dir, transform=synthetic_transform)\n",
    "        synthetic_datasets[user_idx].append(synthetic_dataset)\n",
    "\n",
    "# Step 4: Verify the final non-IID distribution\n",
    "final_user_data = []\n",
    "for user_idx in range(Num_users):\n",
    "    real_data = user_data[user_idx]\n",
    "    if synthetic_datasets[user_idx]:\n",
    "        final_user_data.append(ConcatDataset([real_data] + synthetic_datasets[user_idx]))\n",
    "    else:\n",
    "        final_user_data.append(real_data)\n",
    "\n",
    "print(\"\\n=== Verifying Final Non-IID Data Distribution Across Users ===\")\n",
    "class_counts_per_user = []\n",
    "for user_idx in range(Num_users):\n",
    "    user_dataset = final_user_data[user_idx]\n",
    "    class_counts = [0] * label_dim\n",
    "    for idx in range(len(user_dataset)):\n",
    "        _, label = user_dataset[idx]\n",
    "        class_counts[label] += 1\n",
    "    class_counts_per_user.append(class_counts)\n",
    "    print(f\"User {user_idx + 1} (Non-IID) Class Distribution: {class_counts}\")\n",
    "    total_samples = len(user_dataset)\n",
    "    class_percentages = [count / total_samples * 100 if total_samples > 0 else 0 for count in class_counts]\n",
    "    print(f\"User {user_idx + 1} (Non-IID) Class Percentages: {[f'{p:.2f}%' for p in class_percentages]}\")\n",
    "\n",
    "total_time = time.time() - total_start_time\n",
    "print(f\"\\nTotal time for the entire script: {format_time(total_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93d87360-29a1-429e-a29f-fc369614588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##generating 1000samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c691c0f7-cfc4-4dc6-a69e-288f76fbe5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Name: NVIDIA RTX A5000\n",
      "GPU Memory Allocated: 1632.76 MB\n",
      "Path to dataset files: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Inspecting dataset path: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Root: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Dirs: ['Vehicle Type Image Dataset (Version 2) VTID2']\n",
      "Files (first 5): []\n",
      "--------------------------------------------------\n",
      "Root: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\\Vehicle Type Image Dataset (Version 2) VTID2\n",
      "Dirs: ['Hatchback', 'Other', 'Pickup', 'Seden', 'SUV']\n",
      "Files (first 5): []\n",
      "--------------------------------------------------\n",
      "Root: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\\Vehicle Type Image Dataset (Version 2) VTID2\\Hatchback\n",
      "Dirs: []\n",
      "Files (first 5): ['PHOTO_0.jpg', 'PHOTO_1.jpg', 'PHOTO_10.jpg', 'PHOTO_100.jpg', 'PHOTO_101.jpg']\n",
      "--------------------------------------------------\n",
      "Root: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\\Vehicle Type Image Dataset (Version 2) VTID2\\Other\n",
      "Dirs: []\n",
      "Files (first 5): ['PHOTO_0.jpg', 'PHOTO_1.jpg', 'PHOTO_10.jpg', 'PHOTO_100.jpg', 'PHOTO_101.jpg']\n",
      "--------------------------------------------------\n",
      "Root: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\\Vehicle Type Image Dataset (Version 2) VTID2\\Pickup\n",
      "Dirs: []\n",
      "Files (first 5): ['PHOTO_0.jpg', 'PHOTO_1.jpg', 'PHOTO_10.jpg', 'PHOTO_100(1).jpg', 'PHOTO_100.jpg']\n",
      "--------------------------------------------------\n",
      "Root: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\\Vehicle Type Image Dataset (Version 2) VTID2\\Seden\n",
      "Dirs: []\n",
      "Files (first 5): ['PHOTO_0.jpg', 'PHOTO_1.jpg', 'PHOTO_10.jpg', 'PHOTO_100.jpg', 'PHOTO_1000.jpg']\n",
      "--------------------------------------------------\n",
      "Root: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\\Vehicle Type Image Dataset (Version 2) VTID2\\SUV\n",
      "Dirs: []\n",
      "Files (first 5): ['PHOTO_0.jpg', 'PHOTO_1.jpg', 'PHOTO_10.jpg', 'PHOTO_100.jpg', 'PHOTO_101.jpg']\n",
      "--------------------------------------------------\n",
      "Searching for images in C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Found 4793 images across 5 classes.\n",
      "Classes: ['Hatchback', 'Other', 'Pickup', 'Seden', 'SUV']\n",
      "Number of classes (label_dim): 5\n",
      "Class 0: Train=481, Val=60, Test=61\n",
      "Class 1: Train=480, Val=60, Test=60\n",
      "Class 2: Train=1351, Val=168, Test=170\n",
      "Class 3: Train=977, Val=122, Test=123\n",
      "Class 4: Train=544, Val=68, Test=68\n",
      "Training samples: 3833\n",
      "Validation samples: 478\n",
      "Class 0 dataset size: 481\n",
      "Class 1 dataset size: 480\n",
      "Class 2 dataset size: 1351\n",
      "Class 3 dataset size: 977\n",
      "Class 4 dataset size: 544\n",
      "Class 0:\n",
      "  Number of samples in train_class_datasets1: 240\n",
      "  Number of samples in train_class_datasets2: 241\n",
      "Class 1:\n",
      "  Number of samples in train_class_datasets1: 240\n",
      "  Number of samples in train_class_datasets2: 240\n",
      "Class 2:\n",
      "  Number of samples in train_class_datasets1: 676\n",
      "  Number of samples in train_class_datasets2: 675\n",
      "Class 3:\n",
      "  Number of samples in train_class_datasets1: 488\n",
      "  Number of samples in train_class_datasets2: 489\n",
      "Class 4:\n",
      "  Number of samples in train_class_datasets1: 272\n",
      "  Number of samples in train_class_datasets2: 272\n",
      "Class 0:\n",
      "  Number of samples in train_class_datasets2_part1 (70%): 169\n",
      "  Number of samples in train_class_datasets2_part2 (30%): 72\n",
      "Class 1:\n",
      "  Number of samples in train_class_datasets2_part1 (70%): 168\n",
      "  Number of samples in train_class_datasets2_part2 (30%): 72\n",
      "Class 2:\n",
      "  Number of samples in train_class_datasets2_part1 (70%): 472\n",
      "  Number of samples in train_class_datasets2_part2 (30%): 203\n",
      "Class 3:\n",
      "  Number of samples in train_class_datasets2_part1 (70%): 342\n",
      "  Number of samples in train_class_datasets2_part2 (30%): 147\n",
      "Class 4:\n",
      "  Number of samples in train_class_datasets2_part1 (70%): 190\n",
      "  Number of samples in train_class_datasets2_part2 (30%): 82\n",
      "Sizes of augmented datasets:\n",
      "  Length of augmented train_class_datasets1[0]: 500\n",
      "  Length of augmented train_class_datasets1[1]: 500\n",
      "  Length of augmented train_class_datasets1[2]: 676\n",
      "  Length of augmented train_class_datasets1[3]: 500\n",
      "  Length of augmented train_class_datasets1[4]: 500\n",
      "  Length of augmented train_class_datasets2_part1[0]: 500\n",
      "  Length of augmented train_class_datasets2_part1[1]: 500\n",
      "  Length of augmented train_class_datasets2_part1[2]: 500\n",
      "  Length of augmented train_class_datasets2_part1[3]: 500\n",
      "  Length of augmented train_class_datasets2_part1[4]: 500\n",
      "  Length of augmented train_class_datasets2_part2[0]: 500\n",
      "  Length of augmented train_class_datasets2_part2[1]: 500\n",
      "  Length of augmented train_class_datasets2_part2[2]: 500\n",
      "  Length of augmented train_class_datasets2_part2[3]: 500\n",
      "  Length of augmented train_class_datasets2_part2[4]: 500\n",
      "  Length of augmented train_class_datasets2[0]: 500\n",
      "  Length of augmented train_class_datasets2[1]: 500\n",
      "  Length of augmented train_class_datasets2[2]: 675\n",
      "  Length of augmented train_class_datasets2[3]: 500\n",
      "  Length of augmented train_class_datasets2[4]: 500\n",
      "User 1:\n",
      "Number of samples in the user dataset: 2500\n",
      "User 2:\n",
      "Number of samples in the user dataset: 1000\n",
      "User 3:\n",
      "Number of samples in the user dataset: 1176\n",
      "User 4:\n",
      "Number of samples in the user dataset: 1000\n",
      "User 5:\n",
      "Number of samples in the user dataset: 500\n",
      "User 1 dataset length: 2500\n",
      "User 1, Sample 0: Label=0, Data shape=torch.Size([3, 128, 128])\n",
      "User 1, Sample 1: Label=0, Data shape=torch.Size([3, 128, 128])\n",
      "User 1, Sample 2: Label=0, Data shape=torch.Size([3, 128, 128])\n",
      "User 1, Sample 3: Label=0, Data shape=torch.Size([3, 128, 128])\n",
      "User 1, Sample 4: Label=0, Data shape=torch.Size([3, 128, 128])\n",
      "User 1, Epoch 1/2, Train Loss: 11266.1283, Val Loss: 9005.9421, Time: 15.55s\n",
      "Checkpoint saved for User 1 at epoch 1 to FL_VEHICLE_CVAE_latent_test2_noniid_2\\checkpoints_cvae_user_1\\cvae_epoch_1.pth\n",
      "Decoder saved for User 1 at epoch 1 to FL_VEHICLE_CVAE_latent_test2_noniid_2\\checkpoints_cvae_user_1\\decoder\\decoder_epoch_1.pth\n",
      "Saved latent vectors for User 1, Class 0 at epoch 1 to FL_VEHICLE_CVAE_latent_test2_noniid_2\\checkpoints_cvae_user_1\\latent_vectors_epoch_1\\class_0.pth\n",
      "Saved latent vectors for User 1, Class 1 at epoch 1 to FL_VEHICLE_CVAE_latent_test2_noniid_2\\checkpoints_cvae_user_1\\latent_vectors_epoch_1\\class_1.pth\n",
      "Saved latent vectors for User 1, Class 2 at epoch 1 to FL_VEHICLE_CVAE_latent_test2_noniid_2\\checkpoints_cvae_user_1\\latent_vectors_epoch_1\\class_2.pth\n",
      "Saved latent vectors for User 1, Class 3 at epoch 1 to FL_VEHICLE_CVAE_latent_test2_noniid_2\\checkpoints_cvae_user_1\\latent_vectors_epoch_1\\class_3.pth\n",
      "Saved latent vectors for User 1, Class 4 at epoch 1 to FL_VEHICLE_CVAE_latent_test2_noniid_2\\checkpoints_cvae_user_1\\latent_vectors_epoch_1\\class_4.pth\n",
      "User 1, Epoch 2/2, Train Loss: 9687.5397, Val Loss: 7655.7006, Time: 15.26s\n",
      "Checkpoint saved for User 1 at epoch 2 to FL_VEHICLE_CVAE_latent_test2_noniid_2\\checkpoints_cvae_user_1\\cvae_epoch_2.pth\n",
      "Decoder saved for User 1 at epoch 2 to FL_VEHICLE_CVAE_latent_test2_noniid_2\\checkpoints_cvae_user_1\\decoder\\decoder_epoch_2.pth\n",
      "Saved latent vectors for User 1, Class 0 at epoch 2 to FL_VEHICLE_CVAE_latent_test2_noniid_2\\checkpoints_cvae_user_1\\latent_vectors_epoch_2\\class_0.pth\n",
      "Saved latent vectors for User 1, Class 1 at epoch 2 to FL_VEHICLE_CVAE_latent_test2_noniid_2\\checkpoints_cvae_user_1\\latent_vectors_epoch_2\\class_1.pth\n",
      "Saved latent vectors for User 1, Class 2 at epoch 2 to FL_VEHICLE_CVAE_latent_test2_noniid_2\\checkpoints_cvae_user_1\\latent_vectors_epoch_2\\class_2.pth\n",
      "Saved latent vectors for User 1, Class 3 at epoch 2 to FL_VEHICLE_CVAE_latent_test2_noniid_2\\checkpoints_cvae_user_1\\latent_vectors_epoch_2\\class_3.pth\n",
      "Saved latent vectors for User 1, Class 4 at epoch 2 to FL_VEHICLE_CVAE_latent_test2_noniid_2\\checkpoints_cvae_user_1\\latent_vectors_epoch_2\\class_4.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Subset, Dataset, ConcatDataset\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import kagglehub\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Start total script timer\n",
    "total_start_time = time.time()\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = \"FL_VEHICLE_CVAE_latent_test2_noniid_2\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "RESIZE = 128\n",
    "original_dim = RESIZE * RESIZE * 3\n",
    "intermediate_dim = 512\n",
    "latent_dim = 256\n",
    "num_classes = 5\n",
    "batch_size = 8\n",
    "epochs = 2  # Already set to 2 for testing\n",
    "learning_rate = 1e-4\n",
    "beta_start = 1\n",
    "beta_end = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Print GPU information\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "\n",
    "# Download Vehicle Type Image Dataset from Kaggle\n",
    "try:\n",
    "    path = kagglehub.dataset_download(\"sujaykapadnis/vehicle-type-image-dataset\")\n",
    "    print(\"Path to dataset files:\", path)\n",
    "    dataset_path = path\n",
    "except Exception as e:\n",
    "    print(f\"Failed to download dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Transform for CVAE (normalize to [-1, 1])\n",
    "cvae_input_transform = transforms.Compose([\n",
    "    transforms.Resize((RESIZE, RESIZE)),\n",
    "    transforms.ToTensor(),  # [0, 1]\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # [-1, 1]\n",
    "])\n",
    "\n",
    "# Debug dataset directory structure\n",
    "print(\"Inspecting dataset path:\", dataset_path)\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    print(f\"Root: {root}\")\n",
    "    print(f\"Dirs: {dirs}\")\n",
    "    print(f\"Files (first 5): {files[:5]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Custom Dataset for the Vehicle Type Dataset\n",
    "class VehicleTypeDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_names = []\n",
    "        self.class_to_idx = {}\n",
    "\n",
    "        print(f\"Searching for images in {root_dir}\")\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            image_files = [f for f in files if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "            if image_files:\n",
    "                class_name = os.path.basename(root)\n",
    "                if class_name not in self.class_to_idx:\n",
    "                    self.class_names.append(class_name)\n",
    "                    self.class_to_idx[class_name] = len(self.class_names) - 1\n",
    "                for img_file in image_files:\n",
    "                    img_path = os.path.join(root, img_file)\n",
    "                    try:\n",
    "                        Image.open(img_path).verify()\n",
    "                        self.images.append(img_path)\n",
    "                        self.labels.append(self.class_to_idx[class_name])\n",
    "                    except:\n",
    "                        print(f\"Skipping corrupted image: {img_path}\")\n",
    "\n",
    "        if len(self.class_names) != num_classes:\n",
    "            raise ValueError(f\"Expected {num_classes} classes, found {len(self.class_names)}\")\n",
    "        print(f\"Found {len(self.images)} images across {len(self.class_names)} classes.\")\n",
    "        print(f\"Classes: {self.class_names}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Load the dataset\n",
    "dataset = VehicleTypeDataset(root_dir=dataset_path, transform=cvae_input_transform)\n",
    "\n",
    "# Update label_dim\n",
    "label_dim = len(dataset.class_names)\n",
    "print(f\"Number of classes (label_dim): {label_dim}\")\n",
    "\n",
    "# Step 1: Split dataset into train, validation, and test sets per class\n",
    "validation_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "train_ratio = 0.8\n",
    "\n",
    "class_datasets = [[] for _ in range(label_dim)]\n",
    "for idx in range(len(dataset)):\n",
    "    label = dataset.labels[idx]\n",
    "    class_datasets[label].append(idx)\n",
    "\n",
    "train_indices_per_class = []\n",
    "val_indices_per_class = []\n",
    "test_indices_per_class = []\n",
    "\n",
    "for class_idx in range(label_dim):\n",
    "    indices = class_datasets[class_idx]\n",
    "    total_samples = len(indices)\n",
    "    num_train = int(total_samples * train_ratio)\n",
    "    num_val = int(total_samples * validation_ratio)\n",
    "    num_test = total_samples - num_train - num_val\n",
    "\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_indices = indices[:num_train]\n",
    "    val_indices = indices[num_train:num_train + num_val]\n",
    "    test_indices = indices[num_train + num_val:]\n",
    "\n",
    "    train_indices_per_class.append(train_indices)\n",
    "    val_indices_per_class.append(val_indices)\n",
    "    test_indices_per_class.append(test_indices)\n",
    "\n",
    "    print(f\"Class {class_idx}: Train={len(train_indices)}, Val={len(val_indices)}, Test={len(test_indices)}\")\n",
    "\n",
    "# Verify no overlap\n",
    "for class_idx in range(label_dim):\n",
    "    train_set = set(train_indices_per_class[class_idx])\n",
    "    val_set = set(val_indices_per_class[class_idx])\n",
    "    test_set = set(test_indices_per_class[class_idx])\n",
    "    assert len(train_set.intersection(val_set)) == 0, f\"Overlap between train and val for class {class_idx}\"\n",
    "    assert len(train_set.intersection(test_set)) == 0, f\"Overlap between train and test for class {class_idx}\"\n",
    "    assert len(val_set.intersection(test_set)) == 0, f\"Overlap between val and test for class {class_idx}\"\n",
    "\n",
    "# Create training and validation datasets\n",
    "train_dataset = Subset(dataset, [idx for class_indices in train_indices_per_class for idx in class_indices])\n",
    "val_dataset = Subset(dataset, [idx for class_indices in val_indices_per_class for idx in class_indices])\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Create separate datasets for each class using the training indices\n",
    "distinct_class_datasets = []\n",
    "for class_idx in range(label_dim):\n",
    "    distinct_class_dataset = Subset(dataset, train_indices_per_class[class_idx])\n",
    "    distinct_class_datasets.append(distinct_class_dataset)\n",
    "\n",
    "# Verify the size of each class dataset\n",
    "for i, distinct_class_dataset in enumerate(distinct_class_datasets):\n",
    "    print(f\"Class {i} dataset size: {len(distinct_class_dataset)}\")\n",
    "\n",
    "# Function to split a dataset into two parts\n",
    "def split_dataset(dataset, split_ratio):\n",
    "    train_size = int(np.round(split_ratio * len(dataset)))\n",
    "    remaining_size = len(dataset) - train_size\n",
    "    train_dataset, remaining_dataset = torch.utils.data.random_split(dataset, [train_size, remaining_size])\n",
    "    return train_dataset, remaining_dataset\n",
    "\n",
    "# Split each class dataset into two halves (50/50)\n",
    "split_ratio = 0.5\n",
    "split_datasets = []\n",
    "train_class_datasets1 = []\n",
    "train_class_datasets2 = []\n",
    "\n",
    "for distinct_class_dataset in distinct_class_datasets:\n",
    "    train_class_dataset1, train_class_dataset2 = split_dataset(distinct_class_dataset, split_ratio)\n",
    "    split_datasets.append((train_class_dataset1, train_class_dataset2))\n",
    "    train_class_datasets1.append(train_class_dataset1)\n",
    "    train_class_datasets2.append(train_class_dataset2)\n",
    "\n",
    "for i, (train_class_dataset1, train_class_dataset2) in enumerate(split_datasets):\n",
    "    print(f\"Class {i}:\")\n",
    "    print(f\"  Number of samples in train_class_datasets1: {len(train_class_dataset1)}\")\n",
    "    print(f\"  Number of samples in train_class_datasets2: {len(train_class_dataset2)}\")\n",
    "\n",
    "# Further split train_class_datasets2 into 70% and 30% parts\n",
    "split_ratio = 0.7\n",
    "split_datasets2 = []\n",
    "train_class_datasets2_part1 = []\n",
    "train_class_datasets2_part2 = []\n",
    "\n",
    "for class_dataset in train_class_datasets2:\n",
    "    part1_dataset, part2_dataset = split_dataset(class_dataset, split_ratio)\n",
    "    split_datasets2.append((part1_dataset, part2_dataset))\n",
    "    train_class_datasets2_part1.append(part1_dataset)\n",
    "    train_class_datasets2_part2.append(part2_dataset)\n",
    "\n",
    "for i, (part1_dataset, part2_dataset) in enumerate(split_datasets2):\n",
    "    print(f\"Class {i}:\")\n",
    "    print(f\"  Number of samples in train_class_datasets2_part1 (70%): {len(part1_dataset)}\")\n",
    "    print(f\"  Number of samples in train_class_datasets2_part2 (30%): {len(part2_dataset)}\")\n",
    "\n",
    "# Define augmentation transforms\n",
    "augmentation_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "])\n",
    "\n",
    "# Function to apply augmentation only to PIL images\n",
    "def augment_image_if_needed(image):\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        # Denormalize from [-1, 1] to [0, 1] before converting to PIL\n",
    "        image = image * 0.5 + 0.5\n",
    "        image = transforms.ToPILImage()(image)\n",
    "    image = augmentation_transform(image)\n",
    "    # Reapply the CVAE transform to normalize back to [-1, 1]\n",
    "    image = cvae_input_transform(image)\n",
    "    return image\n",
    "\n",
    "# Function to augment the dataset to a target length\n",
    "def augment_dataset(dataset, target_length):\n",
    "    augmented_samples = []\n",
    "    current_length = len(dataset)\n",
    "    num_samples_to_augment = target_length - current_length\n",
    "    \n",
    "    if num_samples_to_augment <= 0:\n",
    "        return dataset\n",
    "    \n",
    "    for _ in range(num_samples_to_augment):\n",
    "        index = random.randint(0, current_length - 1)\n",
    "        image, label = dataset[index]\n",
    "        augmented_image = augment_image_if_needed(image)\n",
    "        augmented_samples.append((augmented_image, label))\n",
    "    \n",
    "    augmented_dataset = ConcatDataset([dataset, augmented_samples])\n",
    "    return augmented_dataset\n",
    "\n",
    "# Target length for all datasets\n",
    "lengthiest_length = 500\n",
    "\n",
    "# Augment each dataset to have exactly 500 samples\n",
    "train_class_datasets1 = [augment_dataset(dataset, lengthiest_length) for dataset in train_class_datasets1]\n",
    "train_class_datasets2_part1 = [augment_dataset(dataset, lengthiest_length) for dataset in train_class_datasets2_part1]\n",
    "train_class_datasets2_part2 = [augment_dataset(dataset, lengthiest_length) for dataset in train_class_datasets2_part2]\n",
    "train_class_datasets2 = [augment_dataset(dataset, lengthiest_length) for dataset in train_class_datasets2]\n",
    "\n",
    "# Print the sizes of the augmented datasets\n",
    "print(\"Sizes of augmented datasets:\")\n",
    "for i, dataset in enumerate(train_class_datasets1):\n",
    "    print(f\"  Length of augmented train_class_datasets1[{i}]: {len(dataset)}\")\n",
    "for i, dataset in enumerate(train_class_datasets2_part1):\n",
    "    print(f\"  Length of augmented train_class_datasets2_part1[{i}]: {len(dataset)}\")\n",
    "for i, dataset in enumerate(train_class_datasets2_part2):\n",
    "    print(f\"  Length of augmented train_class_datasets2_part2[{i}]: {len(dataset)}\")\n",
    "for i, dataset in enumerate(train_class_datasets2):\n",
    "    print(f\"  Length of augmented train_class_datasets2[{i}]: {len(dataset)}\")\n",
    "\n",
    "# Define number of users\n",
    "Num_users = 5\n",
    "\n",
    "# Assign datasets to users in a non-IID manner\n",
    "user_data = []\n",
    "user_classes = {}\n",
    "\n",
    "user_data.append(ConcatDataset([train_class_datasets1[0], train_class_datasets2[1], train_class_datasets2_part2[2], train_class_datasets2_part2[3], train_class_datasets2_part2[4]]))\n",
    "user_classes[0] = [0, 1, 2, 3, 4]\n",
    "\n",
    "user_data.append(ConcatDataset([train_class_datasets1[1], train_class_datasets2_part1[2]]))\n",
    "user_classes[1] = [1, 2]\n",
    "\n",
    "user_data.append(ConcatDataset([train_class_datasets1[2], train_class_datasets2_part1[3]]))\n",
    "user_classes[2] = [2, 3]\n",
    "\n",
    "user_data.append(ConcatDataset([train_class_datasets1[3], train_class_datasets2_part1[4]]))\n",
    "user_classes[3] = [3, 4]\n",
    "\n",
    "user_data.append(ConcatDataset([train_class_datasets1[4]]))\n",
    "user_classes[4] = [4]\n",
    "\n",
    "for i, user_dataset in enumerate(user_data):\n",
    "    print(f\"User {i + 1}:\")\n",
    "    print(f\"Number of samples in the user dataset: {len(user_dataset)}\")\n",
    "\n",
    "# Weight initialization\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# Encoder (Convolutional)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, latent_dim, num_classes):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 1024, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_hidden = nn.Linear(1024 * 2 * 2 + num_classes, hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.encoder_cnn(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x_with_y = torch.cat([x, y], dim=-1)\n",
    "        h = F.relu(self.fc_hidden(x_with_y))\n",
    "        h = self.dropout(h)\n",
    "        z_mean = self.fc_mean(h)\n",
    "        z_logvar = self.fc_logvar(h)\n",
    "        z_logvar = torch.clamp(z_logvar, min=-10, max=10)\n",
    "        return z_mean, z_logvar\n",
    "\n",
    "# Decoder (Convolutional)\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, num_classes):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim + num_classes, hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc_to_cnn = nn.Linear(hidden_dim, 1024 * 2 * 2)\n",
    "        self.decoder_cnn = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 1024, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 512, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        z_with_y = torch.cat([z, y], dim=-1)\n",
    "        h = F.relu(self.fc(z_with_y))\n",
    "        h = self.dropout(h)\n",
    "        h = F.relu(self.fc_to_cnn(h))\n",
    "        h = h.view(-1, 1024, 2, 2)\n",
    "        x_reconstructed = self.decoder_cnn(h)\n",
    "        x_reconstructed = torch.clamp(x_reconstructed, min=-1, max=1)\n",
    "        return x_reconstructed\n",
    "\n",
    "# Conditional VAE\n",
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def reparameterize(self, z_mean, z_logvar):\n",
    "        std = torch.exp(0.5 * z_logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return z_mean + eps * std\n",
    "\n",
    "    def forward(self, data, y):\n",
    "        z_mean, z_logvar = self.encoder(data, y)\n",
    "        z = self.reparameterize(z_mean, z_logvar)\n",
    "        x_reconstructed = self.decoder(z, y)\n",
    "        return x_reconstructed, z_mean, z_logvar\n",
    "\n",
    "# Loss Function\n",
    "def cvae_loss(data, x_reconstructed, z_mean, z_logvar, beta=1.0):\n",
    "    batch_size = data.size(0)\n",
    "    mse_loss = F.mse_loss(x_reconstructed, data, reduction='sum') / batch_size\n",
    "    kl_loss = -0.5 * torch.sum(1 + z_logvar - z_mean.pow(2) - z_logvar.exp()) / batch_size\n",
    "    return mse_loss + beta * kl_loss\n",
    "\n",
    "# Function to format time\n",
    "def format_time(seconds):\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = seconds % 60\n",
    "    if hours > 0:\n",
    "        return f\"{hours}h {minutes}m {secs:.2f}s\"\n",
    "    elif minutes > 0:\n",
    "        return f\"{minutes}m {secs:.2f}s\"\n",
    "    else:\n",
    "        return f\"{secs:.2f}s\"\n",
    "\n",
    "# Step 2: Train CVAEs for each user\n",
    "cvae_users = {}\n",
    "train_losses_users = {}\n",
    "val_losses_users = {}\n",
    "\n",
    "for user_idx in range(Num_users):\n",
    "    user_start_time = time.time()\n",
    "\n",
    "    user_dataset = user_data[user_idx]\n",
    "    print(f\"User {user_idx + 1} dataset length: {len(user_dataset)}\")\n",
    "\n",
    "    try:\n",
    "        for i in range(min(5, len(user_dataset))):\n",
    "            sample, label = user_dataset[i]\n",
    "            print(f\"User {user_idx + 1}, Sample {i}: Label={label}, Data shape={sample.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing samples for User {user_idx + 1}: {e}\")\n",
    "        raise\n",
    "\n",
    "    user_loader = DataLoader(user_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True, drop_last=True)\n",
    "\n",
    "    encoder = Encoder(intermediate_dim, latent_dim, num_classes).to(device)\n",
    "    decoder = Decoder(latent_dim, intermediate_dim, num_classes).to(device)\n",
    "    cvae = ConditionalVAE(encoder, decoder).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(cvae.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.5)\n",
    "\n",
    "    checkpoint_dir = os.path.join(output_dir, f'checkpoints_cvae_user_{user_idx + 1}')\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    cvae.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        beta = beta_start + (beta_end - beta_start) * ((epoch - 1) / (epochs - 1)) if epochs > 1 else beta_end\n",
    "\n",
    "        train_loss = 0\n",
    "        batches_processed = 0\n",
    "\n",
    "        for batch_idx, (data, labels) in enumerate(user_loader):\n",
    "            try:\n",
    "                data = data.to(device)\n",
    "                y = F.one_hot(labels, num_classes=num_classes).float().to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                x_recon, z_mean, z_logvar = cvae(data, y)\n",
    "                loss = cvae_loss(data, x_recon, z_mean, z_logvar, beta)\n",
    "\n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    print(f\"NaN/Inf loss detected at epoch {epoch}, batch {batch_idx + 1} for User {user_idx + 1}\")\n",
    "                    continue\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(cvae.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                batches_processed += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error at epoch {epoch}, batch {batch_idx + 1} for User {user_idx + 1}: {e}\")\n",
    "                if \"out of memory\" in str(e).lower():\n",
    "                    print(\"Out of memory error detected. Clearing cache...\")\n",
    "                    if device.type == 'cuda':\n",
    "                        torch.cuda.empty_cache()\n",
    "                continue\n",
    "\n",
    "        avg_train_loss = train_loss / batches_processed if batches_processed > 0 else float('inf')\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        cvae.eval()\n",
    "        val_loss = 0\n",
    "        val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in val_loader:\n",
    "                data = data.to(device)\n",
    "                y = F.one_hot(labels, num_classes=num_classes).float().to(device)\n",
    "                x_recon, z_mean, z_logvar = cvae(data, y)\n",
    "                loss = cvae_loss(data, x_recon, z_mean, z_logvar, beta=1.0)\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "\n",
    "        avg_val_loss = val_loss / val_batches if val_batches > 0 else 0\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"User {user_idx + 1}, Epoch {epoch}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Time: {format_time(epoch_time)}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        if epoch == 1 or epoch == epochs:  # Save after epoch 1 as requested\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'cvae_epoch_{epoch}.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': cvae.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Checkpoint saved for User {user_idx + 1} at epoch {epoch} to {checkpoint_path}\")\n",
    "\n",
    "            decoder_dir = os.path.join(checkpoint_dir, 'decoder')\n",
    "            os.makedirs(decoder_dir, exist_ok=True)\n",
    "            decoder_path = os.path.join(decoder_dir, f'decoder_epoch_{epoch}.pth')\n",
    "            torch.save(cvae.decoder.state_dict(), decoder_path)\n",
    "            print(f\"Decoder saved for User {user_idx + 1} at epoch {epoch} to {decoder_path}\")\n",
    "\n",
    "            latent_dir = os.path.join(checkpoint_dir, f'latent_vectors_epoch_{epoch}')\n",
    "            os.makedirs(latent_dir, exist_ok=True)\n",
    "\n",
    "            cvae.eval()\n",
    "            with torch.no_grad():\n",
    "                latent_vectors = {cls: {'z_mean': [], 'z_logvar': [], 'labels': []} for cls in user_classes[user_idx]}\n",
    "                for data, labels in user_loader:\n",
    "                    data = data.to(device)\n",
    "                    y = F.one_hot(labels, num_classes=num_classes).float().to(device)\n",
    "                    z_mean, z_logvar = cvae.encoder(data, y)\n",
    "                    for i, label in enumerate(labels):\n",
    "                        latent_vectors[label.item()]['z_mean'].append(z_mean[i].cpu())\n",
    "                        latent_vectors[label.item()]['z_logvar'].append(z_logvar[i].cpu())\n",
    "                        latent_vectors[label.item()]['labels'].append(label.item())\n",
    "\n",
    "                for cls in user_classes[user_idx]:\n",
    "                    if latent_vectors[cls]['z_mean']:\n",
    "                        z_mean = torch.stack(latent_vectors[cls]['z_mean'])\n",
    "                        z_logvar = torch.stack(latent_vectors[cls]['z_logvar'])\n",
    "                        labels = torch.tensor(latent_vectors[cls]['labels'])\n",
    "                        save_path = os.path.join(latent_dir, f'class_{cls}.pth')\n",
    "                        torch.save({\n",
    "                            'z_mean': z_mean,\n",
    "                            'z_logvar': z_logvar,\n",
    "                            'labels': labels\n",
    "                        }, save_path)\n",
    "                        print(f\"Saved latent vectors for User {user_idx + 1}, Class {cls} at epoch {epoch} to {save_path}\")\n",
    "\n",
    "    train_losses_users[user_idx] = train_losses\n",
    "    val_losses_users[user_idx] = val_losses\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'User {user_idx + 1} CVAE Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    loss_plot_path = os.path.join(checkpoint_dir, 'loss_plot.png')\n",
    "    plt.savefig(loss_plot_path)\n",
    "    plt.close()\n",
    "    print(f\"Loss plot saved for User {user_idx + 1} to {loss_plot_path}\")\n",
    "\n",
    "    cvae_users[user_idx] = cvae\n",
    "\n",
    "    user_time = time.time() - user_start_time\n",
    "    print(f\"Total time for User {user_idx + 1} CVAE training: {format_time(user_time)}\\n\")\n",
    "\n",
    "# Step 3: Share latent vectors and decoder parameters to generate 1000 synthetic samples per class\n",
    "class_to_users = {cls: [] for cls in range(label_dim)}\n",
    "for user_idx, classes in user_classes.items():\n",
    "    for cls in classes:\n",
    "        class_to_users[cls].append(user_idx)\n",
    "\n",
    "sharing_scheme = {}\n",
    "for cls in range(label_dim):\n",
    "    target_users = [user_idx for user_idx in range(Num_users) if cls not in user_classes[user_idx]]\n",
    "    if target_users and class_to_users[cls]:\n",
    "        source_user = class_to_users[cls][0]\n",
    "        sharing_scheme[f'class_{cls}'] = {\n",
    "            'source_user': source_user,\n",
    "            'target_users': target_users,\n",
    "            'share_decoder': True\n",
    "        }\n",
    "\n",
    "synthetic_datasets = [[] for _ in range(Num_users)]\n",
    "num_synthetic_per_class_generate = 1000  # Generate exactly 1000 samples per class\n",
    "\n",
    "for class_key, scheme in sharing_scheme.items():\n",
    "    class_id = int(class_key.split('_')[1])\n",
    "    source_user = scheme['source_user']\n",
    "    target_users = scheme['target_users']\n",
    "\n",
    "    latent_dir = os.path.join(output_dir, f'checkpoints_cvae_user_{source_user+1}', 'latent_vectors_epoch_2')\n",
    "    latent_path = os.path.join(latent_dir, f'class_{class_id}.pth')\n",
    "    latent_data = torch.load(latent_path, weights_only=False)\n",
    "    print(f\"Loaded latent data for User {source_user+1}, Class {class_id}: z_mean shape={latent_data['z_mean'].shape}\")\n",
    "    z_mean_all = latent_data['z_mean'].to(device)\n",
    "    z_logvar_all = latent_data['z_logvar'].to(device)\n",
    "\n",
    "    decoder_dir = os.path.join(output_dir, f'checkpoints_cvae_user_{source_user+1}', 'decoder')\n",
    "    decoder_path = os.path.join(decoder_dir, 'decoder_epoch_2.pth')\n",
    "    print(f\"Loading decoder for User {source_user + 1}, Class {class_id}\")\n",
    "\n",
    "    shared_cvae = ConditionalVAE(Encoder(intermediate_dim, latent_dim, num_classes), Decoder(latent_dim, intermediate_dim, num_classes)).to(device)\n",
    "\n",
    "    if scheme['share_decoder']:\n",
    "        decoder_params = torch.load(decoder_path, weights_only=False)\n",
    "        shared_cvae.decoder.load_state_dict(decoder_params)\n",
    "        print(f\"Loaded decoder parameters: {decoder_path}\")\n",
    "    else:\n",
    "        print(f\"Warning: No decoder shared for user {source_user + 1}, Class {class_id}. Using random decoder.\")\n",
    "\n",
    "    for user_idx in target_users:\n",
    "        synthetic_dir = os.path.join(output_dir, f'synthetic_user_{user_idx + 1}', f'class_{class_id}')\n",
    "        os.makedirs(synthetic_dir, exist_ok=True)\n",
    "\n",
    "        print(f\"Generating {num_synthetic_per_class_generate} synthetic images for User {user_idx + 1}, Class {class_id}\")\n",
    "        synthetic_images = []\n",
    "\n",
    "        shared_cvae.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(num_synthetic_per_class_generate):\n",
    "                z = shared_cvae.reparameterize(z_mean_all[i % len(z_mean_all)].unsqueeze(0), \n",
    "                                               z_logvar_all[i % len(z_mean_all)].unsqueeze(0))\n",
    "                y = F.one_hot(torch.tensor([class_id]), num_classes=label_dim).float().to(device)\n",
    "                synthetic_img = shared_cvae.decoder(z, y).cpu()\n",
    "                synthetic_images.append(synthetic_img)\n",
    "\n",
    "                if (i + 1) % 200 == 0:\n",
    "                    print(f\"Generated {i + 1} images for User {user_idx + 1}, Class {class_id}\")\n",
    "\n",
    "        # Save all 1000 generated images\n",
    "        for idx, img in enumerate(synthetic_images):\n",
    "            img_path = os.path.join(synthetic_dir, f'image_{idx + 1}.png')\n",
    "            try:\n",
    "                img = img.view(3, RESIZE, RESIZE)\n",
    "                img = img * 0.5 + 0.5  # Denormalize to [0, 1]\n",
    "                img = img.clamp(0, 1)\n",
    "                img = transforms.ToPILImage()(img)\n",
    "                img.save(img_path)\n",
    "                if (idx + 1) % 200 == 0 or idx == 0:\n",
    "                    print(f\"Saved {idx + 1} images for User {user_idx + 1}, Class {class_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving image {img_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"Completed generating {num_synthetic_per_class_generate} images for User {user_idx + 1}, Class {class_id}\")\n",
    "\n",
    "        class SyntheticDataset(Dataset):\n",
    "            def __init__(self, class_label, root_dir, transform=None):\n",
    "                self.class_label = class_label\n",
    "                self.root_dir = root_dir\n",
    "                self.transform = transform\n",
    "                self.image_files = sorted([f for f in os.listdir(root_dir) if f.endswith('.png')])\n",
    "                if len(self.image_files) == 0:\n",
    "                    raise ValueError(f\"No images found in {root_dir}\")\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.image_files)\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "                image = Image.open(img_path).convert(\"RGB\")\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                return image, self.class_label\n",
    "\n",
    "        synthetic_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        synthetic_dataset = SyntheticDataset(class_id, synthetic_dir, transform=synthetic_transform)\n",
    "        synthetic_datasets[user_idx].append(synthetic_dataset)\n",
    "\n",
    "# Step 4: Verify the final non-IID distribution\n",
    "final_user_data = []\n",
    "for user_idx in range(Num_users):\n",
    "    real_data = user_data[user_idx]\n",
    "    if synthetic_datasets[user_idx]:\n",
    "        final_user_data.append(ConcatDataset([real_data] + synthetic_datasets[user_idx]))\n",
    "    else:\n",
    "        final_user_data.append(real_data)\n",
    "\n",
    "print(\"\\n=== Verifying Final Non-IID Data Distribution Across Users ===\")\n",
    "class_counts_per_user = []\n",
    "for user_idx in range(Num_users):\n",
    "    user_dataset = final_user_data[user_idx]\n",
    "    class_counts = [0] * label_dim\n",
    "    for idx in range(len(user_dataset)):\n",
    "        _, label = user_dataset[idx]\n",
    "        class_counts[label] += 1\n",
    "    class_counts_per_user.append(class_counts)\n",
    "    print(f\"User {user_idx + 1} (Non-IID) Class Distribution: {class_counts}\")\n",
    "    total_samples = len(user_dataset)\n",
    "    class_percentages = [count / total_samples * 100 if total_samples > 0 else 0 for count in class_counts]\n",
    "    print(f\"User {user_idx + 1} (Non-IID) Class Percentages: {[f'{p:.2f}%' for p in class_percentages]}\")\n",
    "\n",
    "total_time = time.time() - total_start_time\n",
    "print(f\"\\nTotal time for the entire script: {format_time(total_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2586d1-9696-417f-84a2-4417e1e74194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad4ce55-96b2-4e21-b675-cead54852265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Name: NVIDIA RTX A5000\n",
      "Total GPU Memory: 24563.50 MB\n",
      "GPU Memory Allocated: 1627.25 MB\n",
      "Path to dataset files: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Inspecting dataset path: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Root: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Dirs: ['Vehicle Type Image Dataset (Version 2) VTID2']\n",
      "Files (first 5): []\n",
      "--------------------------------------------------\n",
      "Root: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\\Vehicle Type Image Dataset (Version 2) VTID2\n",
      "Dirs: ['Hatchback', 'Other', 'Pickup', 'Seden', 'SUV']\n",
      "Files (first 5): []\n",
      "--------------------------------------------------\n",
      "Root: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\\Vehicle Type Image Dataset (Version 2) VTID2\\Hatchback\n",
      "Dirs: []\n",
      "Files (first 5): ['PHOTO_0.jpg', 'PHOTO_1.jpg', 'PHOTO_10.jpg', 'PHOTO_100.jpg', 'PHOTO_101.jpg']\n",
      "--------------------------------------------------\n",
      "Root: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\\Vehicle Type Image Dataset (Version 2) VTID2\\Other\n",
      "Dirs: []\n",
      "Files (first 5): ['PHOTO_0.jpg', 'PHOTO_1.jpg', 'PHOTO_10.jpg', 'PHOTO_100.jpg', 'PHOTO_101.jpg']\n",
      "--------------------------------------------------\n",
      "Root: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\\Vehicle Type Image Dataset (Version 2) VTID2\\Pickup\n",
      "Dirs: []\n",
      "Files (first 5): ['PHOTO_0.jpg', 'PHOTO_1.jpg', 'PHOTO_10.jpg', 'PHOTO_100(1).jpg', 'PHOTO_100.jpg']\n",
      "--------------------------------------------------\n",
      "Root: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\\Vehicle Type Image Dataset (Version 2) VTID2\\Seden\n",
      "Dirs: []\n",
      "Files (first 5): ['PHOTO_0.jpg', 'PHOTO_1.jpg', 'PHOTO_10.jpg', 'PHOTO_100.jpg', 'PHOTO_1000.jpg']\n",
      "--------------------------------------------------\n",
      "Root: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\\Vehicle Type Image Dataset (Version 2) VTID2\\SUV\n",
      "Dirs: []\n",
      "Files (first 5): ['PHOTO_0.jpg', 'PHOTO_1.jpg', 'PHOTO_10.jpg', 'PHOTO_100.jpg', 'PHOTO_101.jpg']\n",
      "--------------------------------------------------\n",
      "Searching for images in C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Found 4793 images across 5 classes.\n",
      "Classes: ['Hatchback', 'Other', 'Pickup', 'Seden', 'SUV']\n",
      "Number of classes (label_dim): 5\n",
      "Class 0: Train=481, Val=60, Test=61\n",
      "Class 1: Train=480, Val=60, Test=60\n",
      "Class 2: Train=1351, Val=168, Test=170\n",
      "Class 3: Train=977, Val=122, Test=123\n",
      "Class 4: Train=544, Val=68, Test=68\n",
      "Training samples: 3833\n",
      "Validation samples: 478\n",
      "Class 0 has 481 real samples before augmentation.\n",
      "Generating 19 synthetic samples for Class 0 to reach 500 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_34692\\2054822962.py:334: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # For mixed precision training\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_34692\\2054822962.py:358: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_34692\\2054822962.py:398: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0, Epoch 1/2, Train Loss: 15891.5903, Val Loss: 10343.9302, Time: 6.05s\n",
      "Class 0, Epoch 2/2, Train Loss: 9625.8331, Val Loss: 8858.1745, Time: 5.98s\n",
      "Checkpoint saved for Class 0 at epoch 2 to FL_VEHICLE_CVAE_latent_test3_noniid_2\\checkpoints_cvae_class_0\\cvae_epoch_2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_34692\\2054822962.py:438: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved latent vectors for Class 0 at epoch 2 to FL_VEHICLE_CVAE_latent_test3_noniid_2\\checkpoints_cvae_class_0\\latent_vectors_epoch_2\\class_0.pth\n",
      "Generating 19 synthetic images for Class 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_34692\\2054822962.py:477: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1 synthetic images for Class 0\n",
      "Class 0 final dataset length: 500\n",
      "Class 1 has 480 real samples before augmentation.\n",
      "Generating 20 synthetic samples for Class 1 to reach 500 samples.\n",
      "Class 1, Epoch 1/2, Train Loss: 12518.4573, Val Loss: 11204.9463, Time: 6.67s\n",
      "Class 1, Epoch 2/2, Train Loss: 6667.1490, Val Loss: 11129.3255, Time: 6.62s\n",
      "Checkpoint saved for Class 1 at epoch 2 to FL_VEHICLE_CVAE_latent_test3_noniid_2\\checkpoints_cvae_class_1\\cvae_epoch_2.pth\n",
      "Saved latent vectors for Class 1 at epoch 2 to FL_VEHICLE_CVAE_latent_test3_noniid_2\\checkpoints_cvae_class_1\\latent_vectors_epoch_2\\class_1.pth\n",
      "Generating 20 synthetic images for Class 1\n",
      "Saved 1 synthetic images for Class 1\n",
      "Class 1 final dataset length: 500\n",
      "Class 2 has 1351 real samples before augmentation.\n",
      "Class 2 subsampled to 500 samples.\n",
      "Class 3 has 977 real samples before augmentation.\n",
      "Class 3 subsampled to 500 samples.\n",
      "Class 4 has 544 real samples before augmentation.\n",
      "Class 4 subsampled to 500 samples.\n",
      "User 1 dataset length: 2500\n",
      "User 1, Sample 0: Label=0, Data shape=torch.Size([3, 128, 128])\n",
      "User 1, Sample 1: Label=0, Data shape=torch.Size([3, 128, 128])\n",
      "User 1, Sample 2: Label=0, Data shape=torch.Size([3, 128, 128])\n",
      "User 1, Sample 3: Label=0, Data shape=torch.Size([3, 128, 128])\n",
      "User 1, Sample 4: Label=0, Data shape=torch.Size([3, 128, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_34692\\2054822962.py:573: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_34692\\2054822962.py:597: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/312 - GPU Memory Allocated: 2432.26 MB, Reserved: 2904.00 MB\n",
      "Batch 2/312 - GPU Memory Allocated: 2432.26 MB, Reserved: 2904.00 MB\n",
      "Batch 3/312 - GPU Memory Allocated: 2432.26 MB, Reserved: 2904.00 MB\n",
      "Batch 4/312 - GPU Memory Allocated: 2432.26 MB, Reserved: 2904.00 MB\n",
      "Batch 5/312 - GPU Memory Allocated: 2432.26 MB, Reserved: 2904.00 MB\n",
      "Batch 6/312 - GPU Memory Allocated: 2432.26 MB, Reserved: 2904.00 MB\n",
      "Batch 7/312 - GPU Memory Allocated: 2432.26 MB, Reserved: 2904.00 MB\n",
      "Batch 8/312 - GPU Memory Allocated: 2432.26 MB, Reserved: 2904.00 MB\n",
      "Batch 9/312 - GPU Memory Allocated: 2432.26 MB, Reserved: 2904.00 MB\n",
      "Batch 10/312 - GPU Memory Allocated: 2432.26 MB, Reserved: 2904.00 MB\n",
      "Batch 11/312 - GPU Memory Allocated: 2432.26 MB, Reserved: 2904.00 MB\n",
      "Batch 12/312 - GPU Memory Allocated: 2432.26 MB, Reserved: 2904.00 MB\n",
      "Batch 13/312 - GPU Memory Allocated: 2432.26 MB, Reserved: 2904.00 MB\n",
      "Batch 14/312 - GPU Memory Allocated: 2432.26 MB, Reserved: 2904.00 MB\n",
      "Batch 15/312 - GPU Memory Allocated: 3238.06 MB, Reserved: 3708.00 MB\n",
      "Batch 16/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 17/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 18/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 19/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 20/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 21/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 22/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 23/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 24/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 25/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 26/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 27/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 28/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 29/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 30/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 31/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 32/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 33/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 34/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 35/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 36/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 37/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 38/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 39/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 40/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 41/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 42/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 43/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 44/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 45/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 46/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 47/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 48/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 49/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 50/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 51/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 52/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 53/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 54/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 55/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 56/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 57/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 58/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 59/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 60/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 61/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 62/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 63/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 64/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 65/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 66/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 67/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 68/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 69/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 70/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 71/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 72/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 73/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 74/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 75/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 76/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 77/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 78/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 79/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 80/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 81/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 82/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 83/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 84/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 85/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 86/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 87/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 88/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 89/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 90/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 91/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 92/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 93/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 94/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 95/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 96/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 97/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 98/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 99/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 100/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 101/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 102/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 103/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 104/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 105/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 106/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 107/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 108/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 109/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 110/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 111/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 112/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 113/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 114/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 115/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 116/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 117/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 118/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 119/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 120/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 121/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 122/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 123/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 124/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 125/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 126/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 127/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 128/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 129/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 130/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 131/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 132/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 133/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 134/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 135/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 136/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 137/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 138/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 139/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 140/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 141/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 142/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 143/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 144/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 145/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 146/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 147/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 148/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 149/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 150/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 151/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 152/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 153/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 154/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 155/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 156/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 157/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 158/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 159/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 160/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 161/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 162/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 163/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 164/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 165/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 166/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 167/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 168/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 169/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 170/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 171/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 172/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 173/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 174/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 175/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 176/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 177/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 178/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 179/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 180/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 181/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 182/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 183/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 184/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 185/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 186/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 187/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 188/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 189/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 190/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 191/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 192/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 193/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 194/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 195/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 196/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 197/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 198/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 199/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 200/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 201/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 202/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 203/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 204/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 205/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 206/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 207/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 208/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 209/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 210/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 211/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 212/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 213/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 214/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 215/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 216/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 217/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 218/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 219/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 220/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 221/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 222/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 223/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 224/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 225/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 226/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 227/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 228/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 229/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 230/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 231/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 232/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 233/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 234/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 235/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 236/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 237/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 238/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 239/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 240/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 241/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 242/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 243/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 244/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 245/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 246/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 247/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 248/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 249/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 250/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 251/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 252/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 253/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 254/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 255/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 256/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 257/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 258/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 259/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 260/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 261/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 262/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 263/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 264/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 265/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 266/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 267/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 268/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 269/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 270/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 271/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 272/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 273/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 274/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 275/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 276/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 277/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 278/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 279/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 280/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 281/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 282/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 283/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 284/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 285/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 286/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 287/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 288/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 289/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 290/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 291/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 292/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 293/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 294/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 295/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 296/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 297/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 298/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 299/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 300/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 301/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 302/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 303/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 304/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 305/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 306/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 307/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 308/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 309/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 310/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 311/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n",
      "Batch 312/312 - GPU Memory Allocated: 3237.08 MB, Reserved: 3726.00 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_34692\\2054822962.py:637: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 1, Epoch 1/2, Train Loss: 9863.5816, Val Loss: 7821.8646, Time: 22.99s\n",
      "Batch 1/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 2/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 3/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 4/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 5/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 6/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 7/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 8/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 9/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 10/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 11/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 12/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 13/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 14/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 15/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 16/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 17/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 18/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 19/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 20/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 21/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 22/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 23/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 24/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 25/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 26/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 27/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 28/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 29/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 30/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 31/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 32/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 33/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 34/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 35/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 36/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 37/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 38/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 39/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 40/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 41/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 42/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 43/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 44/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 45/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 46/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 47/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 48/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 49/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 50/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 51/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 52/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 53/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 54/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 55/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 56/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 57/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 58/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 59/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 60/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 61/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 62/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 63/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 64/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 65/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 66/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 67/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 68/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 69/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 70/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 71/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 72/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 73/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 74/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 75/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 76/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 77/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 78/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 79/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 80/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 81/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 82/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 83/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 84/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 85/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 86/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 87/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 88/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 89/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 90/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 91/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 92/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 93/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 94/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 95/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 96/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 97/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 98/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 99/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 100/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 101/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 102/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 103/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 104/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 105/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 106/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 107/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 108/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 109/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 110/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 111/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 112/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 113/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 114/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 115/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 116/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 117/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 118/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 119/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 120/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 121/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 122/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 123/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 124/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 125/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 126/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 127/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 128/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 129/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 130/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 131/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 132/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 133/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 134/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 135/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 136/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 137/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 138/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 139/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 140/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 141/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 142/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 143/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 144/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 145/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 146/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 147/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 148/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 149/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 150/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 151/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 152/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 153/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 154/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 155/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 156/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 157/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 158/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 159/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 160/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 161/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 162/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 163/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 164/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 165/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 166/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 167/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 168/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 169/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 170/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 171/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 172/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 173/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 174/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 175/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 176/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 177/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 178/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 179/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 180/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 181/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 182/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 183/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 184/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 185/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 186/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 187/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 188/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 189/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 190/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 191/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 192/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 193/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 194/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 195/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 196/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 197/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 198/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 199/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 200/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 201/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 202/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 203/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 204/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 205/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 206/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 207/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 208/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 209/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 210/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 211/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 212/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 213/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 214/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 215/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 216/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 217/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 218/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 219/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 220/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 221/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 222/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 223/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 224/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 225/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 226/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 227/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 228/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 229/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 230/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 231/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 232/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 233/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 234/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 235/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 236/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 237/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 238/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 239/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 240/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 241/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 242/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 243/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 244/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 245/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 246/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 247/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 248/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 249/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 250/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 251/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 252/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 253/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 254/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 255/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 256/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 257/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 258/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 259/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 260/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 261/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 262/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 263/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 264/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 265/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 266/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 267/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 268/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 269/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 270/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 271/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 272/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 273/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 274/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 275/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 276/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 277/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 278/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 279/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 280/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 281/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 282/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 283/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 284/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 285/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 286/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 287/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 288/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 289/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 290/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 291/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 292/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 293/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 294/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 295/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 296/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 297/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 298/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 299/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 300/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 301/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 302/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 303/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 304/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 305/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 306/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 307/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 308/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 309/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 310/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 311/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "Batch 312/312 - GPU Memory Allocated: 3236.08 MB, Reserved: 3726.00 MB\n",
      "User 1, Epoch 2/2, Train Loss: 7569.4241, Val Loss: 7418.6481, Time: 22.36s\n",
      "Checkpoint saved for User 1 at epoch 2 to FL_VEHICLE_CVAE_latent_test3_noniid_2\\checkpoints_cvae_user_1\\cvae_epoch_2.pth\n",
      "Decoder saved for User 1 at epoch 2 to FL_VEHICLE_CVAE_latent_test3_noniid_2\\checkpoints_cvae_user_1\\decoder\\decoder_epoch_2.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_34692\\2054822962.py:684: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved latent vectors for User 1, Class 0 at epoch 2 to FL_VEHICLE_CVAE_latent_test3_noniid_2\\checkpoints_cvae_user_1\\latent_vectors_epoch_2\\class_0.pth\n",
      "Saved latent vectors for User 1, Class 1 at epoch 2 to FL_VEHICLE_CVAE_latent_test3_noniid_2\\checkpoints_cvae_user_1\\latent_vectors_epoch_2\\class_1.pth\n",
      "Saved latent vectors for User 1, Class 2 at epoch 2 to FL_VEHICLE_CVAE_latent_test3_noniid_2\\checkpoints_cvae_user_1\\latent_vectors_epoch_2\\class_2.pth\n",
      "Saved latent vectors for User 1, Class 3 at epoch 2 to FL_VEHICLE_CVAE_latent_test3_noniid_2\\checkpoints_cvae_user_1\\latent_vectors_epoch_2\\class_3.pth\n",
      "Saved latent vectors for User 1, Class 4 at epoch 2 to FL_VEHICLE_CVAE_latent_test3_noniid_2\\checkpoints_cvae_user_1\\latent_vectors_epoch_2\\class_4.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Subset, Dataset, ConcatDataset\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import kagglehub\n",
    "import time\n",
    "from torch.cuda.amp import GradScaler, autocast  # For mixed precision training\n",
    "\n",
    "# Start total script timer\n",
    "total_start_time = time.time()\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = \"FL_VEHICLE_CVAE_latent_test3_noniid_2\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "RESIZE = 128\n",
    "original_dim = RESIZE * RESIZE * 3\n",
    "intermediate_dim = 512\n",
    "latent_dim = 256\n",
    "num_classes = 5\n",
    "batch_size = 8  # Reduced to prevent memory issues\n",
    "epochs = 2  # Reduced for faster testing; set to 3000 for full training\n",
    "learning_rate = 1e-4\n",
    "beta_start = 1\n",
    "beta_end = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Print GPU information and check available memory\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**2  # in MB\n",
    "    print(f\"Total GPU Memory: {total_memory:.2f} MB\")\n",
    "    print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "\n",
    "# Download Vehicle Type Image Dataset from Kaggle\n",
    "try:\n",
    "    path = kagglehub.dataset_download(\"sujaykapadnis/vehicle-type-image-dataset\")\n",
    "    print(\"Path to dataset files:\", path)\n",
    "    dataset_path = path\n",
    "except Exception as e:\n",
    "    print(f\"Failed to download dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Transform for CVAE (normalize to [-1, 1])\n",
    "cvae_input_transform = transforms.Compose([\n",
    "    transforms.Resize((RESIZE, RESIZE)),\n",
    "    transforms.ToTensor(),  # [0, 1]\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # [-1, 1]\n",
    "])\n",
    "\n",
    "# Debug dataset directory structure\n",
    "print(\"Inspecting dataset path:\", dataset_path)\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    print(f\"Root: {root}\")\n",
    "    print(f\"Dirs: {dirs}\")\n",
    "    print(f\"Files (first 5): {files[:5]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Custom Dataset for the Vehicle Type Dataset\n",
    "class VehicleTypeDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_names = []\n",
    "        self.class_to_idx = {}\n",
    "\n",
    "        print(f\"Searching for images in {root_dir}\")\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            image_files = [f for f in files if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "            if image_files:\n",
    "                class_name = os.path.basename(root)\n",
    "                if class_name not in self.class_to_idx:\n",
    "                    self.class_names.append(class_name)\n",
    "                    self.class_to_idx[class_name] = len(self.class_names) - 1\n",
    "                for img_file in image_files:\n",
    "                    img_path = os.path.join(root, img_file)\n",
    "                    try:\n",
    "                        Image.open(img_path).verify()\n",
    "                        self.images.append(img_path)\n",
    "                        self.labels.append(self.class_to_idx[class_name])\n",
    "                    except:\n",
    "                        print(f\"Skipping corrupted image: {img_path}\")\n",
    "\n",
    "        if len(self.class_names) != num_classes:\n",
    "            raise ValueError(f\"Expected {num_classes} classes, found {len(self.class_names)}\")\n",
    "        print(f\"Found {len(self.images)} images across {len(self.class_names)} classes.\")\n",
    "        print(f\"Classes: {self.class_names}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Load the dataset\n",
    "dataset = VehicleTypeDataset(root_dir=dataset_path, transform=cvae_input_transform)\n",
    "\n",
    "# Update label_dim\n",
    "label_dim = len(dataset.class_names)\n",
    "print(f\"Number of classes (label_dim): {label_dim}\")\n",
    "\n",
    "# Step 1: Split dataset into train, validation, and test sets per class\n",
    "validation_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "train_ratio = 0.8\n",
    "\n",
    "class_datasets = [[] for _ in range(label_dim)]\n",
    "for idx in range(len(dataset)):\n",
    "    label = dataset.labels[idx]\n",
    "    class_datasets[label].append(idx)\n",
    "\n",
    "train_indices_per_class = []\n",
    "val_indices_per_class = []\n",
    "test_indices_per_class = []\n",
    "\n",
    "for class_idx in range(label_dim):\n",
    "    indices = class_datasets[class_idx]\n",
    "    total_samples = len(indices)\n",
    "    num_train = int(total_samples * train_ratio)\n",
    "    num_val = int(total_samples * validation_ratio)\n",
    "    num_test = total_samples - num_train - num_val\n",
    "\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_indices = indices[:num_train]\n",
    "    val_indices = indices[num_train:num_train + num_val]\n",
    "    test_indices = indices[num_train + num_val:]\n",
    "\n",
    "    train_indices_per_class.append(train_indices)\n",
    "    val_indices_per_class.append(val_indices)\n",
    "    test_indices_per_class.append(test_indices)\n",
    "\n",
    "    print(f\"Class {class_idx}: Train={len(train_indices)}, Val={len(val_indices)}, Test={len(test_indices)}\")\n",
    "\n",
    "# Verify no overlap\n",
    "for class_idx in range(label_dim):\n",
    "    train_set = set(train_indices_per_class[class_idx])\n",
    "    val_set = set(val_indices_per_class[class_idx])\n",
    "    test_set = set(test_indices_per_class[class_idx])\n",
    "    assert len(train_set.intersection(val_set)) == 0, f\"Overlap between train and val for class {class_idx}\"\n",
    "    assert len(train_set.intersection(test_set)) == 0, f\"Overlap between train and test for class {class_idx}\"\n",
    "    assert len(val_set.intersection(test_set)) == 0, f\"Overlap between val and test for class {class_idx}\"\n",
    "\n",
    "# Create training and validation datasets\n",
    "train_dataset = Subset(dataset, [idx for class_indices in train_indices_per_class for idx in class_indices])\n",
    "val_dataset = Subset(dataset, [idx for class_indices in val_indices_per_class for idx in class_indices])\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Step 1.5: Ensure 500 samples per class by generating synthetic samples for underrepresented classes\n",
    "target_samples_per_class = 500\n",
    "train_class_datasets = []\n",
    "cvae_per_class = {}\n",
    "\n",
    "# Weight initialization\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# Encoder (Convolutional)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, latent_dim, num_classes):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 1024, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_hidden = nn.Linear(1024 * 2 * 2 + num_classes, hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc_mean = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.encoder_cnn(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x_with_y = torch.cat([x, y], dim=-1)\n",
    "        h = F.relu(self.fc_hidden(x_with_y))\n",
    "        h = self.dropout(h)\n",
    "        z_mean = self.fc_mean(h)\n",
    "        z_logvar = self.fc_logvar(h)\n",
    "        z_logvar = torch.clamp(z_logvar, min=-10, max=10)\n",
    "        return z_mean, z_logvar\n",
    "\n",
    "# Decoder (Convolutional)\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, num_classes):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim + num_classes, hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc_to_cnn = nn.Linear(hidden_dim, 1024 * 2 * 2)\n",
    "        self.decoder_cnn = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024, 1024, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(1024, 512, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 512, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        z_with_y = torch.cat([z, y], dim=-1)\n",
    "        h = F.relu(self.fc(z_with_y))\n",
    "        h = self.dropout(h)\n",
    "        h = F.relu(self.fc_to_cnn(h))\n",
    "        h = h.view(-1, 1024, 2, 2)\n",
    "        x_reconstructed = self.decoder_cnn(h)\n",
    "        x_reconstructed = torch.clamp(x_reconstructed, min=-1, max=1)\n",
    "        return x_reconstructed\n",
    "\n",
    "# Conditional VAE\n",
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def reparameterize(self, z_mean, z_logvar):\n",
    "        std = torch.exp(0.5 * z_logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return z_mean + eps * std\n",
    "\n",
    "    def forward(self, data, y):\n",
    "        z_mean, z_logvar = self.encoder(data, y)\n",
    "        z = self.reparameterize(z_mean, z_logvar)\n",
    "        x_reconstructed = self.decoder(z, y)\n",
    "        return x_reconstructed, z_mean, z_logvar\n",
    "\n",
    "# Loss Function\n",
    "def cvae_loss(data, x_reconstructed, z_mean, z_logvar, beta=1.0):\n",
    "    batch_size = data.size(0)\n",
    "    mse_loss = F.mse_loss(x_reconstructed, data, reduction='sum') / batch_size\n",
    "    kl_loss = -0.5 * torch.sum(1 + z_logvar - z_mean.pow(2) - z_logvar.exp()) / batch_size\n",
    "    return mse_loss + beta * kl_loss\n",
    "\n",
    "# Function to format time\n",
    "def format_time(seconds):\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = seconds % 60\n",
    "    if hours > 0:\n",
    "        return f\"{hours}h {minutes}m {secs:.2f}s\"\n",
    "    elif minutes > 0:\n",
    "        return f\"{minutes}m {secs:.2f}s\"\n",
    "    else:\n",
    "        return f\"{secs:.2f}s\"\n",
    "\n",
    "# Generate synthetic samples for classes with fewer than 500 samples\n",
    "for class_idx in range(label_dim):\n",
    "    class_indices = train_indices_per_class[class_idx]\n",
    "    num_real_samples = len(class_indices)\n",
    "    print(f\"Class {class_idx} has {num_real_samples} real samples before augmentation.\")\n",
    "\n",
    "    if num_real_samples >= target_samples_per_class:\n",
    "        # Subsample to exactly 500 if more than 500\n",
    "        class_indices = np.random.choice(class_indices, target_samples_per_class, replace=False).tolist()\n",
    "        class_dataset = Subset(dataset, class_indices)\n",
    "        train_class_datasets.append(class_dataset)\n",
    "        print(f\"Class {class_idx} subsampled to {len(class_dataset)} samples.\")\n",
    "        continue\n",
    "\n",
    "    # If fewer than 500, train a CVAE to generate additional samples\n",
    "    num_synthetic_needed = target_samples_per_class - num_real_samples\n",
    "    print(f\"Generating {num_synthetic_needed} synthetic samples for Class {class_idx} to reach {target_samples_per_class} samples.\")\n",
    "\n",
    "    # Create dataset for this class\n",
    "    class_dataset = Subset(dataset, class_indices)\n",
    "    class_loader = DataLoader(class_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True, drop_last=True)\n",
    "\n",
    "    # Instantiate CVAE for this class\n",
    "    encoder = Encoder(intermediate_dim, latent_dim, num_classes).to(device)\n",
    "    decoder = Decoder(latent_dim, intermediate_dim, num_classes).to(device)\n",
    "    cvae = ConditionalVAE(encoder, decoder).to(device)\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = optim.Adam(cvae.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.5)\n",
    "    scaler = GradScaler()  # For mixed precision training\n",
    "\n",
    "    # Training loop for this class\n",
    "    class_checkpoint_dir = os.path.join(output_dir, f'checkpoints_cvae_class_{class_idx}')\n",
    "    os.makedirs(class_checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    cvae.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        beta = beta_start + (beta_end - beta_start) * ((epoch - 1) / (epochs - 1)) if epochs > 1 else beta_end\n",
    "\n",
    "        train_loss = 0\n",
    "        batches_processed = 0\n",
    "\n",
    "        for batch_idx, (data, labels) in enumerate(class_loader):\n",
    "            try:\n",
    "                data = data.to(device)\n",
    "                y = F.one_hot(labels, num_classes=num_classes).float().to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                with autocast():\n",
    "                    x_recon, z_mean, z_logvar = cvae(data, y)\n",
    "                    loss = cvae_loss(data, x_recon, z_mean, z_logvar, beta)\n",
    "\n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    print(f\"NaN/Inf loss detected at epoch {epoch}, batch {batch_idx + 1} for Class {class_idx}\")\n",
    "                    continue\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                torch.nn.utils.clip_grad_norm_(cvae.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                train_loss += loss.item()\n",
    "                batches_processed += 1\n",
    "\n",
    "                # Monitor GPU memory\n",
    "                if device.type == 'cuda':\n",
    "                    mem_allocated = torch.cuda.memory_allocated(0) / 1024**2\n",
    "                    mem_reserved = torch.cuda.memory_reserved(0) / 1024**2\n",
    "                    \n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error at epoch {epoch}, batch {batch_idx + 1} for Class {class_idx}: {e}\")\n",
    "                if \"out of memory\" in str(e).lower():\n",
    "                    print(\"Out of memory error detected. Clearing cache...\")\n",
    "                    if device.type == 'cuda':\n",
    "                        torch.cuda.empty_cache()\n",
    "                continue\n",
    "\n",
    "        avg_train_loss = train_loss / batches_processed if batches_processed > 0 else float('inf')\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        cvae.eval()\n",
    "        val_loss = 0\n",
    "        val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in val_loader:\n",
    "                data = data.to(device)\n",
    "                y = F.one_hot(labels, num_classes=num_classes).float().to(device)\n",
    "                with autocast():\n",
    "                    x_recon, z_mean, z_logvar = cvae(data, y)\n",
    "                    loss = cvae_loss(data, x_recon, z_mean, z_logvar, beta=1.0)\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "\n",
    "        avg_val_loss = val_loss / val_batches if val_batches > 0 else 0\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"Class {class_idx}, Epoch {epoch}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Time: {format_time(epoch_time)}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Clear GPU memory\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Save checkpoints and latent vectors at the final epoch\n",
    "        if epoch == epochs:\n",
    "            checkpoint_path = os.path.join(class_checkpoint_dir, f'cvae_epoch_{epoch}.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': cvae.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Checkpoint saved for Class {class_idx} at epoch {epoch} to {checkpoint_path}\")\n",
    "\n",
    "            # Save latent vectors with labels\n",
    "            latent_dir = os.path.join(class_checkpoint_dir, f'latent_vectors_epoch_{epoch}')\n",
    "            os.makedirs(latent_dir, exist_ok=True)\n",
    "\n",
    "            cvae.eval()\n",
    "            with torch.no_grad():\n",
    "                latent_vectors = {'z_mean': [], 'z_logvar': [], 'labels': []}\n",
    "                for data, labels in class_loader:\n",
    "                    data = data.to(device)\n",
    "                    y = F.one_hot(labels, num_classes=num_classes).float().to(device)\n",
    "                    with autocast():\n",
    "                        z_mean, z_logvar = cvae.encoder(data, y)\n",
    "                    for i in range(len(labels)):\n",
    "                        latent_vectors['z_mean'].append(z_mean[i].cpu())\n",
    "                        latent_vectors['z_logvar'].append(z_logvar[i].cpu())\n",
    "                        latent_vectors['labels'].append(labels[i].item())\n",
    "\n",
    "                if latent_vectors['z_mean']:\n",
    "                    z_mean = torch.stack(latent_vectors['z_mean'])\n",
    "                    z_logvar = torch.stack(latent_vectors['z_logvar'])\n",
    "                    labels = torch.tensor(latent_vectors['labels'])\n",
    "                    save_path = os.path.join(latent_dir, f'class_{class_idx}.pth')\n",
    "                    torch.save({\n",
    "                        'z_mean': z_mean,\n",
    "                        'z_logvar': z_logvar,\n",
    "                        'labels': labels\n",
    "                    }, save_path)\n",
    "                    print(f\"Saved latent vectors for Class {class_idx} at epoch {epoch} to {save_path}\")\n",
    "\n",
    "    # Store the trained CVAE for this class\n",
    "    cvae_per_class[class_idx] = cvae\n",
    "\n",
    "    # Generate synthetic samples to reach 500\n",
    "    synthetic_dir = os.path.join(output_dir, f'synthetic_class_{class_idx}')\n",
    "    os.makedirs(synthetic_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Generating {num_synthetic_needed} synthetic images for Class {class_idx}\")\n",
    "    synthetic_images = []\n",
    "    latent_path = os.path.join(class_checkpoint_dir, f'latent_vectors_epoch_{epochs}', f'class_{class_idx}.pth')\n",
    "    latent_data = torch.load(latent_path, weights_only=False)\n",
    "    z_mean_all = latent_data['z_mean'].to(device)\n",
    "    z_logvar_all = latent_data['z_logvar'].to(device)\n",
    "\n",
    "    cvae.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_synthetic_needed):\n",
    "            z = cvae.reparameterize(z_mean_all[i % len(z_mean_all)].unsqueeze(0), \n",
    "                                    z_logvar_all[i % len(z_mean_all)].unsqueeze(0))\n",
    "            y = F.one_hot(torch.tensor([class_idx]), num_classes=label_dim).float().to(device)\n",
    "            with autocast():\n",
    "                synthetic_img = cvae.decoder(z, y).cpu()\n",
    "            synthetic_images.append(synthetic_img)\n",
    "\n",
    "    # Save synthetic images\n",
    "    for idx, img in enumerate(synthetic_images):\n",
    "        img_path = os.path.join(synthetic_dir, f'image_{idx + 1}.png')\n",
    "        try:\n",
    "            img = img.view(3, RESIZE, RESIZE)\n",
    "            img = img * 0.5 + 0.5  # Denormalize to [0, 1]\n",
    "            img = img.clamp(0, 1)\n",
    "            img = transforms.ToPILImage()(img)\n",
    "            img.save(img_path)\n",
    "            if (idx + 1) % 100 == 0 or idx == 0:\n",
    "                print(f\"Saved {idx + 1} synthetic images for Class {class_idx}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving image {img_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Create synthetic dataset\n",
    "    class SyntheticDataset(Dataset):\n",
    "        def __init__(self, class_label, root_dir, transform=None):\n",
    "            self.class_label = class_label\n",
    "            self.root_dir = root_dir\n",
    "            self.transform = transform\n",
    "            self.image_files = sorted([f for f in os.listdir(root_dir) if f.endswith('.png')])\n",
    "            if len(self.image_files) == 0:\n",
    "                raise ValueError(f\"No images found in {root_dir}\")\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.image_files)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, self.class_label\n",
    "\n",
    "    synthetic_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    synthetic_dataset = SyntheticDataset(class_idx, synthetic_dir, transform=synthetic_transform)\n",
    "\n",
    "    # Combine real and synthetic datasets to reach exactly 500 samples\n",
    "    combined_dataset = ConcatDataset([class_dataset, synthetic_dataset])\n",
    "    combined_indices = list(range(len(combined_dataset)))\n",
    "    if len(combined_indices) > target_samples_per_class:\n",
    "        combined_indices = np.random.choice(combined_indices, target_samples_per_class, replace=False).tolist()\n",
    "    final_class_dataset = Subset(combined_dataset, combined_indices)\n",
    "    train_class_datasets.append(final_class_dataset)\n",
    "    print(f\"Class {class_idx} final dataset length: {len(final_class_dataset)}\")\n",
    "\n",
    "# Step 2: Define user classes and train CVAEs\n",
    "Num_users = 5\n",
    "user_classes = {\n",
    "    0: [0, 1, 2, 3, 4],  # User 1\n",
    "    1: [1, 2],           # User 2\n",
    "    2: [3, 4, 0],        # User 3\n",
    "    3: [1],              # User 4\n",
    "    4: [4, 0]            # User 5\n",
    "}\n",
    "\n",
    "cvae_users = {}\n",
    "train_losses_users = {}\n",
    "val_losses_users = {}\n",
    "\n",
    "for user_idx in range(Num_users):\n",
    "    # Start timer for this user's CVAE training\n",
    "    user_start_time = time.time()\n",
    "\n",
    "    # Create user dataset\n",
    "    user_dataset = ConcatDataset([train_class_datasets[i] for i in user_classes[user_idx]])\n",
    "    print(f\"User {user_idx + 1} dataset length: {len(user_dataset)}\")\n",
    "\n",
    "    # Validate indices\n",
    "    try:\n",
    "        for i in range(min(5, len(user_dataset))):\n",
    "            sample, label = user_dataset[i]\n",
    "            print(f\"User {user_idx + 1}, Sample {i}: Label={label}, Data shape={sample.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing samples for User {user_idx + 1}: {e}\")\n",
    "        raise\n",
    "\n",
    "    user_loader = DataLoader(user_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True, drop_last=True)\n",
    "\n",
    "    # Instantiate CVAE\n",
    "    encoder = Encoder(intermediate_dim, latent_dim, num_classes).to(device)\n",
    "    decoder = Decoder(latent_dim, intermediate_dim, num_classes).to(device)\n",
    "    cvae = ConditionalVAE(encoder, decoder).to(device)\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = optim.Adam(cvae.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.5)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Training loop\n",
    "    checkpoint_dir = os.path.join(output_dir, f'checkpoints_cvae_user_{user_idx + 1}')\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    cvae.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        beta = beta_start + (beta_end - beta_start) * ((epoch - 1) / (epochs - 1)) if epochs > 1 else beta_end\n",
    "\n",
    "        train_loss = 0\n",
    "        batches_processed = 0\n",
    "\n",
    "        for batch_idx, (data, labels) in enumerate(user_loader):\n",
    "            try:\n",
    "                data = data.to(device)\n",
    "                y = F.one_hot(labels, num_classes=num_classes).float().to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                with autocast():\n",
    "                    x_recon, z_mean, z_logvar = cvae(data, y)\n",
    "                    loss = cvae_loss(data, x_recon, z_mean, z_logvar, beta)\n",
    "\n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    print(f\"NaN/Inf loss detected at epoch {epoch}, batch {batch_idx + 1} for User {user_idx + 1}\")\n",
    "                    continue\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                torch.nn.utils.clip_grad_norm_(cvae.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                train_loss += loss.item()\n",
    "                batches_processed += 1\n",
    "\n",
    "                # Monitor GPU memory\n",
    "                if device.type == 'cuda':\n",
    "                    mem_allocated = torch.cuda.memory_allocated(0) / 1024**2\n",
    "                    mem_reserved = torch.cuda.memory_reserved(0) / 1024**2\n",
    "                    print(f\"Batch {batch_idx + 1}/{len(user_loader)} - GPU Memory Allocated: {mem_allocated:.2f} MB, Reserved: {mem_reserved:.2f} MB\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error at epoch {epoch}, batch {batch_idx + 1} for User {user_idx + 1}: {e}\")\n",
    "                if \"out of memory\" in str(e).lower():\n",
    "                    print(\"Out of memory error detected. Clearing cache...\")\n",
    "                    if device.type == 'cuda':\n",
    "                        torch.cuda.empty_cache()\n",
    "                continue\n",
    "\n",
    "        avg_train_loss = train_loss / batches_processed if batches_processed > 0 else float('inf')\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        cvae.eval()\n",
    "        val_loss = 0\n",
    "        val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in val_loader:\n",
    "                data = data.to(device)\n",
    "                y = F.one_hot(labels, num_classes=num_classes).float().to(device)\n",
    "                with autocast():\n",
    "                    x_recon, z_mean, z_logvar = cvae(data, y)\n",
    "                    loss = cvae_loss(data, x_recon, z_mean, z_logvar, beta=1.0)\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "\n",
    "        avg_val_loss = val_loss / val_batches if val_batches > 0 else 0\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"User {user_idx + 1}, Epoch {epoch}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Time: {format_time(epoch_time)}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Clear GPU memory\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Save checkpoints, latent vectors, and decoder parameters at the final epoch\n",
    "        if epoch == epochs:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'cvae_epoch_{epoch}.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': cvae.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Checkpoint saved for User {user_idx + 1} at epoch {epoch} to {checkpoint_path}\")\n",
    "\n",
    "            # Save decoder parameters\n",
    "            decoder_dir = os.path.join(checkpoint_dir, 'decoder')\n",
    "            os.makedirs(decoder_dir, exist_ok=True)\n",
    "            decoder_path = os.path.join(decoder_dir, f'decoder_epoch_{epoch}.pth')\n",
    "            torch.save(cvae.decoder.state_dict(), decoder_path)\n",
    "            print(f\"Decoder saved for User {user_idx + 1} at epoch {epoch} to {decoder_path}\")\n",
    "\n",
    "            # Save latent vectors with labels\n",
    "            latent_dir = os.path.join(checkpoint_dir, f'latent_vectors_epoch_{epoch}')\n",
    "            os.makedirs(latent_dir, exist_ok=True)\n",
    "\n",
    "            cvae.eval()\n",
    "            with torch.no_grad():\n",
    "                latent_vectors = {cls: {'z_mean': [], 'z_logvar': [], 'labels': []} for cls in user_classes[user_idx]}\n",
    "                for data, labels in user_loader:\n",
    "                    data = data.to(device)\n",
    "                    y = F.one_hot(labels, num_classes=num_classes).float().to(device)\n",
    "                    with autocast():\n",
    "                        z_mean, z_logvar = cvae.encoder(data, y)\n",
    "                    for i, label in enumerate(labels):\n",
    "                        latent_vectors[label.item()]['z_mean'].append(z_mean[i].cpu())\n",
    "                        latent_vectors[label.item()]['z_logvar'].append(z_logvar[i].cpu())\n",
    "                        latent_vectors[label.item()]['labels'].append(label.item())\n",
    "\n",
    "                for cls in user_classes[user_idx]:\n",
    "                    if latent_vectors[cls]['z_mean']:\n",
    "                        z_mean = torch.stack(latent_vectors[cls]['z_mean'])\n",
    "                        z_logvar = torch.stack(latent_vectors[cls]['z_logvar'])\n",
    "                        labels = torch.tensor(latent_vectors[cls]['labels'])\n",
    "                        save_path = os.path.join(latent_dir, f'class_{cls}.pth')\n",
    "                        torch.save({\n",
    "                            'z_mean': z_mean,\n",
    "                            'z_logvar': z_logvar,\n",
    "                            'labels': labels\n",
    "                        }, save_path)\n",
    "                        print(f\"Saved latent vectors for User {user_idx + 1}, Class {cls} at epoch {epoch} to {save_path}\")\n",
    "\n",
    "    # Store losses for plotting\n",
    "    train_losses_users[user_idx] = train_losses\n",
    "    val_losses_users[user_idx] = val_losses\n",
    "\n",
    "    # Plot losses\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'User {user_idx + 1} CVAE Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    loss_plot_path = os.path.join(checkpoint_dir, 'loss_plot.png')\n",
    "    plt.savefig(loss_plot_path)\n",
    "    plt.close()\n",
    "    print(f\"Loss plot saved for User {user_idx + 1} to {loss_plot_path}\")\n",
    "\n",
    "    cvae_users[user_idx] = cvae\n",
    "\n",
    "    user_time = time.time() - user_start_time\n",
    "    print(f\"Total time for User {user_idx + 1} CVAE training: {format_time(user_time)}\\n\")\n",
    "\n",
    "# Step 3: Generate 1000 new synthetic samples for each class per user and save user-wise\n",
    "num_synthetic_per_class_generate = 1000\n",
    "batch_size_synthetic = 200  # Generate in batches to manage memory\n",
    "\n",
    "for user_idx in range(Num_users):\n",
    "    user_cvae = cvae_users[user_idx]\n",
    "    classes = user_classes[user_idx]\n",
    "\n",
    "    # Load the latent vectors for the user's classes\n",
    "    latent_dir = os.path.join(output_dir, f'checkpoints_cvae_user_{user_idx + 1}', f'latent_vectors_epoch_{epochs}')\n",
    "    latent_vectors = {}\n",
    "    for cls in classes:\n",
    "        latent_path = os.path.join(latent_dir, f'class_{cls}.pth')\n",
    "        latent_data = torch.load(latent_path, weights_only=False)\n",
    "        latent_vectors[cls] = {\n",
    "            'z_mean': latent_data['z_mean'].to(device),\n",
    "            'z_logvar': latent_data['z_logvar'].to(device)\n",
    "        }\n",
    "        print(f\"Loaded latent vectors for User {user_idx + 1}, Class {cls}: z_mean shape={latent_data['z_mean'].shape}\")\n",
    "\n",
    "    # Load the decoder\n",
    "    decoder_dir = os.path.join(output_dir, f'checkpoints_cvae_user_{user_idx + 1}', 'decoder')\n",
    "    decoder_path = os.path.join(decoder_dir, f'decoder_epoch_{epochs}.pth')\n",
    "    user_cvae.decoder.load_state_dict(torch.load(decoder_path, weights_only=False))\n",
    "    print(f\"Loaded decoder for User {user_idx + 1} from {decoder_path}\")\n",
    "\n",
    "    # Generate synthetic samples for each class\n",
    "    user_cvae.eval()\n",
    "    for cls in classes:\n",
    "        class_synthetic_dir = os.path.join(output_dir, f'synthetic_user_{user_idx + 1}', f'class_{cls}')\n",
    "        latent_subdir = os.path.join(class_synthetic_dir, 'latent_vectors')\n",
    "        os.makedirs(class_synthetic_dir, exist_ok=True)\n",
    "        os.makedirs(latent_subdir, exist_ok=True)\n",
    "\n",
    "        print(f\"Generating {num_synthetic_per_class_generate} synthetic samples for User {user_idx + 1}, Class {cls}\")\n",
    "        total_generated = 0\n",
    "        z_mean_all = latent_vectors[cls]['z_mean']\n",
    "        z_logvar_all = latent_vectors[cls]['z_logvar']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            while total_generated < num_synthetic_per_class_generate:\n",
    "                remaining = min(batch_size_synthetic, num_synthetic_per_class_generate - total_generated)\n",
    "                synthetic_images = []\n",
    "                synthetic_z_means = []\n",
    "                synthetic_z_logvars = []\n",
    "\n",
    "                try:\n",
    "                    for i in range(remaining):\n",
    "                        idx = (total_generated + i) % len(z_mean_all)\n",
    "                        z_mean = z_mean_all[idx].unsqueeze(0)\n",
    "                        z_logvar = z_logvar_all[idx].unsqueeze(0)\n",
    "                        z = user_cvae.reparameterize(z_mean, z_logvar)\n",
    "                        y = F.one_hot(torch.tensor([cls]), num_classes=label_dim).float().to(device)\n",
    "                        with autocast():\n",
    "                            synthetic_img = user_cvae.decoder(z, y).cpu()\n",
    "                        synthetic_images.append(synthetic_img)\n",
    "                        synthetic_z_means.append(z_mean.cpu())\n",
    "                        synthetic_z_logvars.append(z_logvar.cpu())\n",
    "\n",
    "                    # Save the batch of synthetic images and their latent vectors\n",
    "                    for idx, (img, z_mean, z_logvar) in enumerate(zip(synthetic_images, synthetic_z_means, synthetic_z_logvars)):\n",
    "                        img_path = os.path.join(class_synthetic_dir, f'image_{total_generated + idx + 1}.png')\n",
    "                        try:\n",
    "                            img = img.view(3, RESIZE, RESIZE)\n",
    "                            img = img * 0.5 + 0.5  # Denormalize to [0, 1]\n",
    "                            img = img.clamp(0, 1)\n",
    "                            img = transforms.ToPILImage()(img)\n",
    "                            img.save(img_path)\n",
    "\n",
    "                            # Save latent vectors for this image\n",
    "                            latent_path = os.path.join(latent_subdir, f'image_{total_generated + idx + 1}_latent.pth')\n",
    "                            torch.save({\n",
    "                                'z_mean': z_mean.squeeze(0),\n",
    "                                'z_logvar': z_logvar.squeeze(0),\n",
    "                                'label': cls\n",
    "                            }, latent_path)\n",
    "\n",
    "                            if (total_generated + idx + 1) % 200 == 0 or (total_generated + idx) == 0:\n",
    "                                print(f\"Saved {total_generated + idx + 1} images for User {user_idx + 1}, Class {cls}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error saving image {img_path}: {e}\")\n",
    "                            continue\n",
    "\n",
    "                    total_generated += remaining\n",
    "                    print(f\"Generated batch of {remaining} images. Total generated: {total_generated}/{num_synthetic_per_class_generate}\")\n",
    "\n",
    "                    # Clear memory after each batch\n",
    "                    del synthetic_images, synthetic_z_means, synthetic_z_logvars\n",
    "                    if device.type == 'cuda':\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error generating synthetic images for User {user_idx + 1}, Class {cls}: {e}\")\n",
    "                    if \"out of memory\" in str(e).lower():\n",
    "                        print(\"Out of memory error during synthetic generation. Clearing cache...\")\n",
    "                        if device.type == 'cuda':\n",
    "                            torch.cuda.empty_cache()\n",
    "                    continue\n",
    "\n",
    "        # Save the decoder parameters for this class\n",
    "        decoder_class_path = os.path.join(class_synthetic_dir, 'decoder.pth')\n",
    "        torch.save(user_cvae.decoder.state_dict(), decoder_class_path)\n",
    "        print(f\"Saved decoder for User {user_idx + 1}, Class {cls} to {decoder_class_path}\")\n",
    "\n",
    "        print(f\"Completed generating {total_generated} synthetic samples for User {user_idx + 1}, Class {cls}\")\n",
    "\n",
    "# Step 4: Verify the non-IID distribution\n",
    "user_data = []\n",
    "for user_idx in range(Num_users):\n",
    "    user_data.append(ConcatDataset([train_class_datasets[i] for i in user_classes[user_idx]]))\n",
    "\n",
    "print(\"\\n=== Verifying Non-IID Data Distribution Across Users ===\")\n",
    "class_counts_per_user = []\n",
    "for user_idx in range(Num_users):\n",
    "    user_dataset = user_data[user_idx]\n",
    "    class_counts = [0] * label_dim\n",
    "    for idx in range(len(user_dataset)):\n",
    "        _, label = user_dataset[idx]\n",
    "        class_counts[label] += 1\n",
    "    class_counts_per_user.append(class_counts)\n",
    "    print(f\"User {user_idx + 1} (Non-IID) Class Distribution: {class_counts}\")\n",
    "    total_samples = len(user_dataset)\n",
    "    class_percentages = [count / total_samples * 100 if total_samples > 0 else 0 for count in class_counts]\n",
    "    print(f\"User {user_idx + 1} (Non-IID) Class Percentages: {[f'{p:.2f}%' for p in class_percentages]}\")\n",
    "\n",
    "# Calculate and print total script time\n",
    "total_time = time.time() - total_start_time\n",
    "print(f\"\\nTotal time for the entire script: {format_time(total_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59a38819-83b4-4c71-83e4-4a66c7d56090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0de310d-8fcf-47da-b7b4-ae3429cb7baf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
