{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "909ac582-78e6-4b46-a9e0-82664272337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CAVE WITH NEW DATA DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770e16e9-3ac3-411c-8395-9e672aaf6f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24bc215d-ecf6-49dd-b18d-ae09057b1f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Inspecting dataset path: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Root: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Dirs: ['Vehicle Type Image Dataset (Version 2) VTID2']\n",
      "Files (first 5): []\n",
      "--------------------------------------------------\n",
      "Root: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\\Vehicle Type Image Dataset (Version 2) VTID2\n",
      "Dirs: ['Hatchback', 'Other', 'Pickup', 'Seden', 'SUV']\n",
      "Files (first 5): []\n",
      "--------------------------------------------------\n",
      "Root: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\\Vehicle Type Image Dataset (Version 2) VTID2\\Hatchback\n",
      "Dirs: []\n",
      "Files (first 5): ['PHOTO_0.jpg', 'PHOTO_1.jpg', 'PHOTO_10.jpg', 'PHOTO_100.jpg', 'PHOTO_101.jpg']\n",
      "--------------------------------------------------\n",
      "Root: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\\Vehicle Type Image Dataset (Version 2) VTID2\\Other\n",
      "Dirs: []\n",
      "Files (first 5): ['PHOTO_0.jpg', 'PHOTO_1.jpg', 'PHOTO_10.jpg', 'PHOTO_100.jpg', 'PHOTO_101.jpg']\n",
      "--------------------------------------------------\n",
      "Root: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\\Vehicle Type Image Dataset (Version 2) VTID2\\Pickup\n",
      "Dirs: []\n",
      "Files (first 5): ['PHOTO_0.jpg', 'PHOTO_1.jpg', 'PHOTO_10.jpg', 'PHOTO_100(1).jpg', 'PHOTO_100.jpg']\n",
      "--------------------------------------------------\n",
      "Root: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\\Vehicle Type Image Dataset (Version 2) VTID2\\Seden\n",
      "Dirs: []\n",
      "Files (first 5): ['PHOTO_0.jpg', 'PHOTO_1.jpg', 'PHOTO_10.jpg', 'PHOTO_100.jpg', 'PHOTO_1000.jpg']\n",
      "--------------------------------------------------\n",
      "Root: C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\\Vehicle Type Image Dataset (Version 2) VTID2\\SUV\n",
      "Dirs: []\n",
      "Files (first 5): ['PHOTO_0.jpg', 'PHOTO_1.jpg', 'PHOTO_10.jpg', 'PHOTO_100.jpg', 'PHOTO_101.jpg']\n",
      "--------------------------------------------------\n",
      "Searching for images in C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Found 4793 images across 5 classes.\n",
      "Classes: ['Hatchback', 'Other', 'Pickup', 'Seden', 'SUV']\n",
      "Number of classes (label_dim): 5\n",
      "Class 0: Train=481, Val=60, Test=61\n",
      "Class 1: Train=480, Val=60, Test=60\n",
      "Class 2: Train=1351, Val=168, Test=170\n",
      "Class 3: Train=977, Val=122, Test=123\n",
      "Class 4: Train=544, Val=68, Test=68\n",
      "Searching for images in C:\\Users\\hp\\.cache\\kagglehub\\datasets\\sujaykapadnis\\vehicle-type-image-dataset\\versions\\1\n",
      "Found 4793 images across 5 classes.\n",
      "Classes: ['Hatchback', 'Other', 'Pickup', 'Seden', 'SUV']\n",
      "Class 0 dataset size: 481\n",
      "Class 1 dataset size: 480\n",
      "Class 2 dataset size: 1351\n",
      "Class 3 dataset size: 977\n",
      "Class 4 dataset size: 544\n",
      "Class 0:\n",
      "  Number of samples in train_class_datasets1: 240\n",
      "  Number of samples in train_class_datasets2: 241\n",
      "Class 1:\n",
      "  Number of samples in train_class_datasets1: 240\n",
      "  Number of samples in train_class_datasets2: 240\n",
      "Class 2:\n",
      "  Number of samples in train_class_datasets1: 676\n",
      "  Number of samples in train_class_datasets2: 675\n",
      "Class 3:\n",
      "  Number of samples in train_class_datasets1: 488\n",
      "  Number of samples in train_class_datasets2: 489\n",
      "Class 4:\n",
      "  Number of samples in train_class_datasets1: 272\n",
      "  Number of samples in train_class_datasets2: 272\n",
      "Class 0:\n",
      "  Number of samples in train_class_datasets2_part1 (70%): 169\n",
      "  Number of samples in train_class_datasets2_part2 (30%): 72\n",
      "Class 1:\n",
      "  Number of samples in train_class_datasets2_part1 (70%): 168\n",
      "  Number of samples in train_class_datasets2_part2 (30%): 72\n",
      "Class 2:\n",
      "  Number of samples in train_class_datasets2_part1 (70%): 472\n",
      "  Number of samples in train_class_datasets2_part2 (30%): 203\n",
      "Class 3:\n",
      "  Number of samples in train_class_datasets2_part1 (70%): 342\n",
      "  Number of samples in train_class_datasets2_part2 (30%): 147\n",
      "Class 4:\n",
      "  Number of samples in train_class_datasets2_part1 (70%): 190\n",
      "  Number of samples in train_class_datasets2_part2 (30%): 82\n",
      "Sizes of augmented datasets:\n",
      "  Length of augmented train_class_datasets1[0]: 500\n",
      "  Length of augmented train_class_datasets1[1]: 500\n",
      "  Length of augmented train_class_datasets1[2]: 676\n",
      "  Length of augmented train_class_datasets1[3]: 500\n",
      "  Length of augmented train_class_datasets1[4]: 500\n",
      "  Length of augmented train_class_datasets2_part1[0]: 500\n",
      "  Length of augmented train_class_datasets2_part1[1]: 500\n",
      "  Length of augmented train_class_datasets2_part1[2]: 500\n",
      "  Length of augmented train_class_datasets2_part1[3]: 500\n",
      "  Length of augmented train_class_datasets2_part1[4]: 500\n",
      "  Length of augmented train_class_datasets2_part2[0]: 500\n",
      "  Length of augmented train_class_datasets2_part2[1]: 500\n",
      "  Length of augmented train_class_datasets2_part2[2]: 500\n",
      "  Length of augmented train_class_datasets2_part2[3]: 500\n",
      "  Length of augmented train_class_datasets2_part2[4]: 500\n",
      "Sizes of augmented datasets:\n",
      "  Length of augmented train_class_datasets2[0]: 500\n",
      "  Length of augmented train_class_datasets2[1]: 500\n",
      "  Length of augmented train_class_datasets2[2]: 675\n",
      "  Length of augmented train_class_datasets2[3]: 500\n",
      "  Length of augmented train_class_datasets2[4]: 500\n",
      "User 1:\n",
      "Number of samples in the user dataset: 2500\n",
      "User 2:\n",
      "Number of samples in the user dataset: 1000\n",
      "User 3:\n",
      "Number of samples in the user dataset: 1176\n",
      "User 4:\n",
      "Number of samples in the user dataset: 1000\n",
      "User 5:\n",
      "Number of samples in the user dataset: 500\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'user_classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 289\u001b[0m\n\u001b[0;32m    286\u001b[0m user_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# Create user dataset\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m user_dataset \u001b[38;5;241m=\u001b[39m ConcatDataset([train_class_datasets[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m user_classes[user_idx]])\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dataset length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(user_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    292\u001b[0m \u001b[38;5;66;03m# Validate indices\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'user_classes' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset, Dataset, ConcatDataset\n",
    "import torchvision.datasets as datasets\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import kagglehub\n",
    "import time\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = \"FL_VEHICLE_NON_IID_2\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "RESIZE = 128\n",
    "original_dim = RESIZE * RESIZE * 3\n",
    "intermediate_dim = 512\n",
    "latent_dim = 256\n",
    "num_classes = 5\n",
    "batch_size = 8\n",
    "epochs = 3000  # Set to 3000 as requested\n",
    "learning_rate = 1e-4\n",
    "beta_start = 1\n",
    "beta_end = 10\n",
    "device = torch.device('cuda:1' if torch.cuda.device_count() > 1 else 'cuda')\n",
    "\n",
    "# Download Vehicle Type Image Dataset from Kaggle\n",
    "try:\n",
    "    path = kagglehub.dataset_download(\"sujaykapadnis/vehicle-type-image-dataset\")\n",
    "    print(\"Path to dataset files:\", path)\n",
    "    dataset_path = path\n",
    "except Exception as e:\n",
    "    print(f\"Failed to download dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Base transform for RGB Vehicle Type Dataset\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize to 128x128\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize RGB channels\n",
    "])\n",
    "\n",
    "# Debug dataset directory structure\n",
    "print(\"Inspecting dataset path:\", dataset_path)\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    print(f\"Root: {root}\")\n",
    "    print(f\"Dirs: {dirs}\")\n",
    "    print(f\"Files (first 5): {files[:5]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "class VehicleTypeDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_names = []\n",
    "        self.class_to_idx = {}\n",
    "\n",
    "        print(f\"Searching for images in {root_dir}\")\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            image_files = [f for f in files if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "            if image_files:\n",
    "                class_name = os.path.basename(root)\n",
    "                if class_name not in self.class_to_idx:\n",
    "                    self.class_names.append(class_name)\n",
    "                    self.class_to_idx[class_name] = len(self.class_names) - 1\n",
    "                for img_file in image_files:\n",
    "                    img_path = os.path.join(root, img_file)\n",
    "                    self.images.append(img_path)\n",
    "                    self.labels.append(self.class_to_idx[class_name])\n",
    "\n",
    "        if not self.images:\n",
    "            raise ValueError(\n",
    "                f\"No images found in {root_dir}. \"\n",
    "                \"Expected class folders containing .jpg, .png, or .jpeg images.\"\n",
    "            )\n",
    "\n",
    "        print(f\"Found {len(self.images)} images across {len(self.class_names)} classes.\")\n",
    "        print(f\"Classes: {self.class_names}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Load the Vehicle Type Dataset without transformations for splitting\n",
    "try:\n",
    "    dataset_no_transform = VehicleTypeDataset(root_dir=dataset_path, transform=None)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Update label_dim\n",
    "label_dim = len(dataset_no_transform.class_names)\n",
    "print(f\"Number of classes (label_dim): {label_dim}\")\n",
    "\n",
    "# Step 1: Split dataset into train, validation, and test sets per class\n",
    "validation_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Separate the dataset by class using labels directly\n",
    "class_datasets = [[] for _ in range(label_dim)]\n",
    "for idx in range(len(dataset_no_transform)):\n",
    "    label = dataset_no_transform.labels[idx]\n",
    "    class_datasets[label].append(idx)\n",
    "\n",
    "# Split each class into train, validation, and test sets\n",
    "train_indices_per_class = []\n",
    "val_indices_per_class = []\n",
    "test_indices_per_class = []\n",
    "\n",
    "for class_idx in range(label_dim):\n",
    "    indices = class_datasets[class_idx]\n",
    "    total_samples = len(indices)\n",
    "    num_train = int(total_samples * train_ratio)\n",
    "    num_val = int(total_samples * validation_ratio)\n",
    "    num_test = total_samples - num_train - num_val\n",
    "\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    train_indices = indices[:num_train]\n",
    "    val_indices = indices[num_train:num_train + num_val]\n",
    "    test_indices = indices[num_train + num_val:]\n",
    "\n",
    "    train_indices_per_class.append(train_indices)\n",
    "    val_indices_per_class.append(val_indices)\n",
    "    test_indices_per_class.append(test_indices)\n",
    "\n",
    "    print(f\"Class {class_idx}: Train={len(train_indices)}, Val={len(val_indices)}, Test={len(test_indices)}\")\n",
    "\n",
    "# Verify no overlap between train, val, and test\n",
    "for class_idx in range(label_dim):\n",
    "    train_set = set(train_indices_per_class[class_idx])\n",
    "    val_set = set(val_indices_per_class[class_idx])\n",
    "    test_set = set(test_indices_per_class[class_idx])\n",
    "\n",
    "    assert len(train_set.intersection(val_set)) == 0, f\"Overlap between train and val for class {class_idx}\"\n",
    "    assert len(train_set.intersection(test_set)) == 0, f\"Overlap between train and test for class {class_idx}\"\n",
    "    assert len(val_set.intersection(test_set)) == 0, f\"Overlap between val and test for class {class_idx}\"\n",
    "\n",
    "# Create a new dataset instance with transformations\n",
    "dataset = VehicleTypeDataset(root_dir=dataset_path, transform=base_transform)\n",
    "\n",
    "# Create train, val, and test datasets\n",
    "train_dataset = Subset(dataset, [idx for class_indices in train_indices_per_class for idx in class_indices])\n",
    "val_dataset = Subset(dataset, [idx for class_indices in val_indices_per_class for idx in class_indices])\n",
    "test_dataset = Subset(dataset, [idx for class_indices in test_indices_per_class for idx in class_indices])\n",
    "\n",
    "# Create separate datasets for each class using the original dataset\n",
    "distinct_class_datasets = []\n",
    "num_classes = label_dim  # Use label_dim to ensure consistency\n",
    "for class_idx in range(num_classes):\n",
    "    distinct_class_dataset = Subset(dataset, train_indices_per_class[class_idx])\n",
    "    distinct_class_datasets.append(distinct_class_dataset)\n",
    "\n",
    "# Verify the size of each class dataset\n",
    "for i, distinct_class_dataset in enumerate(distinct_class_datasets):\n",
    "    print(f\"Class {i} dataset size: {len(distinct_class_dataset)}\")\n",
    "\n",
    "# Function to split a dataset into two parts\n",
    "def split_dataset(dataset, split_ratio):\n",
    "    train_size = int(np.round(split_ratio * len(dataset)))\n",
    "    remaining_size = len(dataset) - train_size\n",
    "    train_dataset, remaining_dataset = torch.utils.data.random_split(dataset, [train_size, remaining_size])\n",
    "    return train_dataset, remaining_dataset\n",
    "\n",
    "# Split each class dataset into two halves (50/50)\n",
    "split_ratio = 0.5\n",
    "split_datasets = []\n",
    "train_class_datasets1 = []\n",
    "train_class_datasets2 = []\n",
    "\n",
    "for distinct_class_dataset in distinct_class_datasets:\n",
    "    train_class_dataset1, train_class_dataset2 = split_dataset(distinct_class_dataset, split_ratio)\n",
    "    split_datasets.append((train_class_dataset1, train_class_dataset2))\n",
    "    train_class_datasets1.append(train_class_dataset1)\n",
    "    train_class_datasets2.append(train_class_dataset2)\n",
    "\n",
    "for i, (train_class_dataset1, train_class_dataset2) in enumerate(split_datasets):\n",
    "    print(f\"Class {i}:\")\n",
    "    print(f\"  Number of samples in train_class_datasets1: {len(train_class_dataset1)}\")\n",
    "    print(f\"  Number of samples in train_class_datasets2: {len(train_class_dataset2)}\")\n",
    "\n",
    "# Further split train_class_datasets2 into 70% and 30% parts\n",
    "split_ratio = 0.7\n",
    "split_datasets2 = []\n",
    "train_class_datasets2_part1 = []\n",
    "train_class_datasets2_part2 = []\n",
    "\n",
    "for class_dataset in train_class_datasets2:\n",
    "    part1_dataset, part2_dataset = split_dataset(class_dataset, split_ratio)\n",
    "    split_datasets2.append((part1_dataset, part2_dataset))\n",
    "    train_class_datasets2_part1.append(part1_dataset)\n",
    "    train_class_datasets2_part2.append(part2_dataset)\n",
    "\n",
    "for i, (part1_dataset, part2_dataset) in enumerate(split_datasets2):\n",
    "    print(f\"Class {i}:\")\n",
    "    print(f\"  Number of samples in train_class_datasets2_part1 (70%): {len(part1_dataset)}\")\n",
    "    print(f\"  Number of samples in train_class_datasets2_part2 (30%): {len(part2_dataset)}\")\n",
    "\n",
    "# Define augmentation transforms\n",
    "augmentation_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "])\n",
    "\n",
    "# Function to apply augmentation only to PIL images\n",
    "def augment_image_if_needed(image):\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = transforms.ToPILImage()(image)\n",
    "    image = augmentation_transform(image)\n",
    "    image = base_transform(image)\n",
    "    return image\n",
    "\n",
    "# Function to augment the dataset to a target length\n",
    "def augment_dataset(dataset, target_length):\n",
    "    augmented_samples = []\n",
    "    current_length = len(dataset)\n",
    "    num_samples_to_augment = target_length - current_length\n",
    "    \n",
    "    if num_samples_to_augment <= 0:\n",
    "        return dataset\n",
    "    \n",
    "    for _ in range(num_samples_to_augment):\n",
    "        index = random.randint(0, current_length - 1)\n",
    "        image, label = dataset[index]\n",
    "        augmented_image = augment_image_if_needed(image)\n",
    "        augmented_samples.append((augmented_image, label))\n",
    "    \n",
    "    augmented_dataset = ConcatDataset([dataset, augmented_samples])\n",
    "    return augmented_dataset\n",
    "\n",
    "# Target length for all datasets\n",
    "lengthiest_length = 500\n",
    "\n",
    "# Augment each dataset to have exactly 500 samples\n",
    "train_class_datasets1 = [augment_dataset(dataset, lengthiest_length) for dataset in train_class_datasets1]\n",
    "train_class_datasets2_part1 = [augment_dataset(dataset, lengthiest_length) for dataset in train_class_datasets2_part1]\n",
    "train_class_datasets2_part2 = [augment_dataset(dataset, lengthiest_length) for dataset in train_class_datasets2_part2]\n",
    "\n",
    "# Print the sizes of the augmented datasets\n",
    "print(\"Sizes of augmented datasets:\")\n",
    "for i, dataset in enumerate(train_class_datasets1):\n",
    "    print(f\"  Length of augmented train_class_datasets1[{i}]: {len(dataset)}\")\n",
    "for i, dataset in enumerate(train_class_datasets2_part1):\n",
    "    print(f\"  Length of augmented train_class_datasets2_part1[{i}]: {len(dataset)}\")\n",
    "for i, dataset in enumerate(train_class_datasets2_part2):\n",
    "    print(f\"  Length of augmented train_class_datasets2_part2[{i}]: {len(dataset)}\")\n",
    "\n",
    "train_class_datasets2= [augment_dataset(dataset, lengthiest_length) for dataset in train_class_datasets2]\n",
    "print(\"Sizes of augmented datasets:\")\n",
    "for i, dataset in enumerate(train_class_datasets2):\n",
    "    print(f\"  Length of augmented train_class_datasets2[{i}]: {len(dataset)}\")\n",
    "\n",
    "\n",
    "Num_users=5\n",
    "    \n",
    "user_data = [] \n",
    "\n",
    "user_data.append(torch.utils.data.ConcatDataset([train_class_datasets1[0], train_class_datasets2[1],train_class_datasets2_part2[2],train_class_datasets2_part2[3],train_class_datasets2_part2[4]]))\n",
    "user_data.append(torch.utils.data.ConcatDataset([train_class_datasets1[1],train_class_datasets2_part1[2]]))\n",
    "user_data.append(torch.utils.data.ConcatDataset([train_class_datasets1[2],train_class_datasets2_part1[3]]))\n",
    "user_data.append(torch.utils.data.ConcatDataset([train_class_datasets1[3],train_class_datasets2_part1[4]]))\n",
    "user_data.append(torch.utils.data.ConcatDataset([train_class_datasets1[4]]))\n",
    "for i, user_dataset in enumerate(user_data):\n",
    "    print(f\"User {i + 1}:\")\n",
    "    print(\"Number of samples in the user dataset:\", len(user_dataset))\n",
    "\n",
    "cvae_users = {}\n",
    "train_losses_users = {}\n",
    "val_losses_users = {}\n",
    "\n",
    "for user_idx in range(Num_users):\n",
    "    # Start timer for this user's CVAE training\n",
    "    user_start_time = time.time()\n",
    "\n",
    "    # Create user dataset\n",
    "    user_dataset = ConcatDataset([train_class_datasets[i] for i in user_classes[user_idx]])\n",
    "    print(f\"User {user_idx + 1} dataset length: {len(user_dataset)}\")\n",
    "\n",
    "    # Validate indices\n",
    "    try:\n",
    "        for i in range(min(5, len(user_dataset))):\n",
    "            sample, label = user_dataset[i]\n",
    "            print(f\"User {user_idx + 1}, Sample {i}: Label={label}, Data shape={sample.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing samples for User {user_idx + 1}: {e}\")\n",
    "        raise\n",
    "\n",
    "    user_loader = DataLoader(user_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=True, drop_last=True)\n",
    "\n",
    "    # Instantiate CVAE\n",
    "    encoder = Encoder(intermediate_dim, latent_dim, num_classes).to(device)\n",
    "    decoder = Decoder(latent_dim, intermediate_dim, num_classes).to(device)\n",
    "    cvae = ConditionalVAE(encoder, decoder).to(device)\n",
    "\n",
    "    # Optimizer and scheduler\n",
    "    optimizer = optim.Adam(cvae.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.5)\n",
    "\n",
    "    # Training loop\n",
    "    checkpoint_dir = os.path.join(output_dir, f'checkpoints_cvae_user_{user_idx + 1}')\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    cvae.train()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        beta = beta_start + (beta_end - beta_start) * ((epoch - 1) / (epochs - 1)) if epochs > 1 else beta_end\n",
    "\n",
    "        train_loss = 0\n",
    "        batches_processed = 0\n",
    "\n",
    "        for batch_idx, (data, labels) in enumerate(user_loader):\n",
    "            try:\n",
    "                data = data.to(device)\n",
    "                y = F.one_hot(labels, num_classes=num_classes).float().to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                x_recon, z_mean, z_logvar = cvae(data, y)\n",
    "                loss = cvae_loss(data, x_recon, z_mean, z_logvar, beta)\n",
    "\n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    print(f\"NaN/Inf loss detected at epoch {epoch}, batch {batch_idx + 1} for User {user_idx + 1}\")\n",
    "                    continue\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(cvae.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                batches_processed += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error at epoch {epoch}, batch {batch_idx + 1} for User {user_idx + 1}: {e}\")\n",
    "                if \"out of memory\" in str(e).lower():\n",
    "                    print(\"Out of memory error detected. Clearing cache...\")\n",
    "                    if device.type == 'cuda':\n",
    "                        torch.cuda.empty_cache()\n",
    "                continue\n",
    "\n",
    "        avg_train_loss = train_loss / batches_processed if batches_processed > 0 else float('inf')\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # Validation\n",
    "        cvae.eval()\n",
    "        val_loss = 0\n",
    "        val_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in val_loader:\n",
    "                data = data.to(device)\n",
    "                y = F.one_hot(labels, num_classes=num_classes).float().to(device)\n",
    "                x_recon, z_mean, z_logvar = cvae(data, y)\n",
    "                loss = cvae_loss(data, x_recon, z_mean, z_logvar, beta=1.0)\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "\n",
    "        avg_val_loss = val_loss / val_batches if val_batches > 0 else 0\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f\"User {user_idx + 1}, Epoch {epoch}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Time: {format_time(epoch_time)}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Clear GPU memory\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Save checkpoints, latent vectors, and decoder parameters every 500 epochs or at the end\n",
    "        if epoch % 500 == 0 or epoch == epochs:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'cvae_epoch_{epoch}.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': cvae.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Checkpoint saved for User {user_idx + 1} at epoch {epoch} to {checkpoint_path}\")\n",
    "\n",
    "            # Save decoder parameters\n",
    "            decoder_dir = os.path.join(checkpoint_dir, 'decoder')\n",
    "            os.makedirs(decoder_dir, exist_ok=True)\n",
    "            decoder_path = os.path.join(decoder_dir, f'decoder_epoch_{epoch}.pth')\n",
    "            torch.save(cvae.decoder.state_dict(), decoder_path)\n",
    "            print(f\"Decoder saved for User {user_idx + 1} at epoch {epoch} to {decoder_path}\")\n",
    "\n",
    "            # Save latent vectors with labels\n",
    "            latent_dir = os.path.join(checkpoint_dir, f'latent_vectors_epoch_{epoch}')\n",
    "            os.makedirs(latent_dir, exist_ok=True)\n",
    "\n",
    "            cvae.eval()\n",
    "            with torch.no_grad():\n",
    "                latent_vectors = {cls: {'z_mean': [], 'z_logvar': [], 'labels': []} for cls in user_classes[user_idx]}\n",
    "                for data, labels in user_loader:\n",
    "                    data = data.to(device)\n",
    "                    y = F.one_hot(labels, num_classes=num_classes).float().to(device)\n",
    "                    z_mean, z_logvar = cvae.encoder(data, y)\n",
    "                    for i, label in enumerate(labels):\n",
    "                        latent_vectors[label.item()]['z_mean'].append(z_mean[i].cpu())\n",
    "                        latent_vectors[label.item()]['z_logvar'].append(z_logvar[i].cpu())\n",
    "                        latent_vectors[label.item()]['labels'].append(label.item())\n",
    "\n",
    "                for cls in user_classes[user_idx]:\n",
    "                    if latent_vectors[cls]['z_mean']:\n",
    "                        z_mean = torch.stack(latent_vectors[cls]['z_mean'])\n",
    "                        z_logvar = torch.stack(latent_vectors[cls]['z_logvar'])\n",
    "                        labels = torch.tensor(latent_vectors[cls]['labels'])\n",
    "                        save_path = os.path.join(latent_dir, f'class_{cls}.pth')\n",
    "                        torch.save({\n",
    "                            'z_mean': z_mean,\n",
    "                            'z_logvar': z_logvar,\n",
    "                            'labels': labels\n",
    "                        }, save_path)\n",
    "                        print(f\"Saved latent vectors for User {user_idx + 1}, Class {cls} at epoch {epoch} to {save_path}\")\n",
    "\n",
    "    # Store losses for plotting\n",
    "    train_losses_users[user_idx] = train_losses\n",
    "    val_losses_users[user_idx] = val_losses\n",
    "\n",
    "    # Plot losses\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'User {user_idx + 1} CVAE Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    loss_plot_path = os.path.join(checkpoint_dir, 'loss_plot.png')\n",
    "    plt.savefig(loss_plot_path)\n",
    "    plt.close()\n",
    "    print(f\"Loss plot saved for User {user_idx + 1} to {loss_plot_path}\")\n",
    "\n",
    "    cvae_users[user_idx] = cvae\n",
    "\n",
    "    user_time = time.time() - user_start_time\n",
    "    print(f\"Total time for User {user_idx + 1} CVAE training: {format_time(user_time)}\\n\")\n",
    "\n",
    "# Step 3: Share latent vectors and decoder parameters to generate synthetic data\n",
    "class_to_users = {cls: [] for cls in range(label_dim)}\n",
    "for user_idx, classes in user_classes.items():\n",
    "    for cls in classes:\n",
    "        class_to_users[cls].append(user_idx)\n",
    "\n",
    "# Define sharing scheme\n",
    "sharing_scheme = {}\n",
    "for cls in range(label_dim):\n",
    "    target_users = [user_idx for user_idx in range(Num_users) if cls not in user_classes[user_idx]]\n",
    "    if target_users and class_to_users[cls]:\n",
    "        source_user = class_to_users[cls][0]  # First user with this class\n",
    "        sharing_scheme[f'class_{cls}'] = {\n",
    "            'source_user': source_user,\n",
    "            'target_users': target_users,\n",
    "            'share_decoder': True\n",
    "        }\n",
    "\n",
    "synthetic_datasets = [[] for _ in range(Num_users)]\n",
    "num_synthetic_per_class_generate = 1000\n",
    "num_synthetic_per_class_select = 500\n",
    "\n",
    "# Use the latest latent vectors and decoder (from epoch 2)\n",
    "for class_key, scheme in sharing_scheme.items():\n",
    "    class_id = int(class_key.split('_')[1])\n",
    "    source_user = scheme['source_user']\n",
    "    target_users = scheme['target_users']\n",
    "\n",
    "    # Load the latest latent vectors (epoch 2)\n",
    "    latent_dir = os.path.join(output_dir, f'checkpoints_cvae_user_{source_user+1}', 'latent_vectors_epoch_2')\n",
    "    latent_path = os.path.join(latent_dir, f'class_{class_id}.pth')\n",
    "    latent_data = torch.load(latent_path, weights_only=False)\n",
    "    print(f\"Loaded latent data for User {source_user+1}, Class {class_id}: z_mean shape={latent_data['z_mean'].shape}\")\n",
    "    z_mean_all = latent_data['z_mean'].to(device)\n",
    "    z_logvar_all = latent_data['z_logvar'].to(device)\n",
    "\n",
    "    # Load the latest decoder (epoch 2)\n",
    "    decoder_dir = os.path.join(output_dir, f'checkpoints_cvae_user_{source_user+1}', 'decoder')\n",
    "    decoder_path = os.path.join(decoder_dir, 'decoder_epoch_2.pth')\n",
    "    print(f\"Loading decoder for User {source_user + 1}, Class {class_id}\")\n",
    "\n",
    "    # Create shared CVAE instance\n",
    "    shared_cvae = ConditionalVAE(Encoder(intermediate_dim, latent_dim, num_classes), Decoder(latent_dim, intermediate_dim, num_classes)).to(device)\n",
    "\n",
    "    if scheme['share_decoder']:\n",
    "        decoder_params = torch.load(decoder_path, weights_only=False)\n",
    "        shared_cvae.decoder.load_state_dict(decoder_params)\n",
    "        print(f\"Loaded decoder parameters: {decoder_path}\")\n",
    "    else:\n",
    "        print(f\"Warning: No decoder shared for user {source_user + 1}, Class {class_id}. Using random decoder.\")\n",
    "\n",
    "    # Generate synthetic data for all target users\n",
    "    for user_idx in target_users:\n",
    "        synthetic_dir = os.path.join(output_dir, f'synthetic_user_{user_idx + 1}', f'class_{class_id}')\n",
    "        os.makedirs(synthetic_dir, exist_ok=True)\n",
    "\n",
    "        print(f\"Generating {num_synthetic_per_class_generate} synthetic images for User {user_idx + 1}, Class {class_id}\")\n",
    "        synthetic_images = []\n",
    "        mean_intensities = []\n",
    "\n",
    "        shared_cvae.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(num_synthetic_per_class_generate):\n",
    "                z = shared_cvae.reparameterize(z_mean_all[i % len(z_mean_all)].unsqueeze(0), \n",
    "                                               z_logvar_all[i % len(z_mean_all)].unsqueeze(0))\n",
    "                y = F.one_hot(torch.tensor([class_id]), num_classes=label_dim).float().to(device)\n",
    "                synthetic_img = shared_cvae.decoder(z, y).cpu()\n",
    "                mean_intensity = synthetic_img.mean().item()\n",
    "                synthetic_images.append(synthetic_img)\n",
    "                mean_intensities.append(mean_intensity)\n",
    "\n",
    "                if (i + 1) % 200 == 0:\n",
    "                    print(f\"Generated {i + 1} images for User {user_idx + 1}, Class {class_id}\")\n",
    "\n",
    "        # Select top 500 images based on mean pixel intensity\n",
    "        print(f\"Selecting top {num_synthetic_per_class_select} images for User {user_idx + 1}, Class {class_id}\")\n",
    "        sorted_indices = np.argsort(mean_intensities)[::-1]\n",
    "        selected_indices = sorted_indices[:num_synthetic_per_class_select]\n",
    "\n",
    "        # Save selected images\n",
    "        for idx, img_idx in enumerate(selected_indices):\n",
    "            img_path = os.path.join(synthetic_dir, f'image_{idx + 1}.png')\n",
    "            try:\n",
    "                img = synthetic_images[img_idx].view(3, RESIZE, RESIZE)\n",
    "                img = img * 0.5 + 0.5  # Denormalize to [0, 1]\n",
    "                img = img.clamp(0, 1)\n",
    "                img = transforms.ToPILImage()(img)\n",
    "                img.save(img_path)\n",
    "                if (idx + 1) % 100 == 0 or idx == 0:\n",
    "                    print(f\"Saved {idx + 1} selected images for User {user_idx + 1}, Class {class_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving image {img_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"Completed generating and selecting {num_synthetic_per_class_select} images for User {user_idx + 1}, Class {class_id}\")\n",
    "\n",
    "        class SyntheticDataset(Dataset):\n",
    "            def __init__(self, class_label, root_dir, transform=None):\n",
    "                self.class_label = class_label\n",
    "                self.root_dir = root_dir\n",
    "                self.transform = transform\n",
    "                self.image_files = sorted([f for f in os.listdir(root_dir) if f.endswith('.png')])\n",
    "                if len(self.image_files) == 0:\n",
    "                    raise ValueError(f\"No images found in {root_dir}\")\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.image_files)\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "                image = Image.open(img_path).convert(\"RGB\")\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                return image, self.class_label\n",
    "\n",
    "        synthetic_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        synthetic_dataset = SyntheticDataset(class_id, synthetic_dir, transform=synthetic_transform)\n",
    "        synthetic_datasets[user_idx].append(synthetic_dataset)\n",
    "\n",
    "# Step 4: Verify the converted IID distribution\n",
    "user_data = []\n",
    "for user_idx in range(Num_users):\n",
    "    real_data = ConcatDataset([train_class_datasets[i] for i in user_classes[user_idx]])\n",
    "    if synthetic_datasets[user_idx]:\n",
    "        user_data.append(ConcatDataset([real_data] + synthetic_datasets[user_idx]))\n",
    "    else:\n",
    "        user_data.append(real_data)\n",
    "\n",
    "print(\"\\n=== Verifying Converted IID Data Distribution Across Users ===\")\n",
    "class_counts_per_user = []\n",
    "for user_idx in range(Num_users):\n",
    "    user_dataset = user_data[user_idx]\n",
    "    class_counts = [0] * label_dim\n",
    "    for idx in range(len(user_dataset)):\n",
    "        _, label = user_dataset[idx]\n",
    "        class_counts[label] += 1\n",
    "    class_counts_per_user.append(class_counts)\n",
    "    print(f\"User {user_idx + 1} (CVAE IID) Class Distribution: {class_counts}\")\n",
    "    total_samples = len(user_dataset)\n",
    "    class_percentages = [count / total_samples * 100 if total_samples > 0 else 0 for count in class_counts]\n",
    "    print(f\"User {user_idx + 1} (CVAE IID) Class Percentages: {[f'{p:.2f}%' for p in class_percentages]}\")\n",
    "\n",
    "# Calculate and print total script time\n",
    "total_time = time.time() - total_start_time\n",
    "print(f\"\\nTotal time for the entire script: {format_time(total_time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7443eb2-3b7d-4c5f-a5cb-122feef4469c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70670661-596a-4cb8-b1e3-ee566629eb6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
