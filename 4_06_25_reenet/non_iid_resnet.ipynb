{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7a2334-f585-4ca7-a814-130ab67387d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13acbf8-7e16-4e17-83b0-10cfa9c648d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, Dataset, ConcatDataset\n",
    "import torchvision.datasets as datasets\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import csv\n",
    "import random\n",
    "import kagglehub\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = \"FL_VEHICLE_NON_IID_ANOTHER_DISTRIBUTION_500AUG\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Download Vehicle Type Image Dataset from Kaggle\n",
    "try:\n",
    "    path = kagglehub.dataset_download(\"sujaykapadnis/vehicle-type-image-dataset\")\n",
    "    print(\"Path to dataset files:\", path)\n",
    "    dataset_path = path\n",
    "except Exception as e:\n",
    "    print(f\"Failed to download dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Base transform for RGB Vehicle Type Dataset\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize to 128x128\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize RGB channels\n",
    "])\n",
    "\n",
    "# Debug dataset directory structure\n",
    "print(\"Inspecting dataset path:\", dataset_path)\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "    print(f\"Root: {root}\")\n",
    "    print(f\"Dirs: {dirs}\")\n",
    "    print(f\"Files (first 5): {files[:5]}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "class VehicleTypeDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_names = []\n",
    "        self.class_to_idx = {}\n",
    "\n",
    "        print(f\"Searching for images in {root_dir}\")\n",
    "        for root, dirs, files in os.walk(root_dir):\n",
    "            image_files = [f for f in files if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "            if image_files:\n",
    "                class_name = os.path.basename(root)\n",
    "                if class_name not in self.class_to_idx:\n",
    "                    self.class_names.append(class_name)\n",
    "                    self.class_to_idx[class_name] = len(self.class_names) - 1\n",
    "                for img_file in image_files:\n",
    "                    img_path = os.path.join(root, img_file)\n",
    "                    self.images.append(img_path)\n",
    "                    self.labels.append(self.class_to_idx[class_name])\n",
    "\n",
    "        if not self.images:\n",
    "            raise ValueError(\n",
    "                f\"No images found in {root_dir}. \"\n",
    "                \"Expected class folders containing .jpg, .png, or .jpeg images.\"\n",
    "            )\n",
    "\n",
    "        print(f\"Found {len(self.images)} images across {len(self.class_names)} classes.\")\n",
    "        print(f\"Classes: {self.class_names}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Load the Vehicle Type Dataset without transformations for splitting\n",
    "try:\n",
    "    dataset_no_transform = VehicleTypeDataset(root_dir=dataset_path, transform=None)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# Update label_dim\n",
    "label_dim = len(dataset_no_transform.class_names)\n",
    "print(f\"Number of classes (label_dim): {label_dim}\")\n",
    "\n",
    "# Step 1: Split dataset into train, validation, and test sets per class\n",
    "validation_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Separate the dataset by class using labels directly\n",
    "class_datasets = [[] for _ in range(label_dim)]\n",
    "for idx in range(len(dataset_no_transform)):\n",
    "    label = dataset_no_transform.labels[idx]  # Directly access labels without loading images\n",
    "    class_datasets[label].append(idx)\n",
    "\n",
    "# Split each class into train, validation, and test sets\n",
    "train_indices_per_class = []\n",
    "val_indices_per_class = []\n",
    "test_indices_per_class = []\n",
    "\n",
    "for class_idx in range(label_dim):\n",
    "    indices = class_datasets[class_idx]\n",
    "    total_samples = len(indices)\n",
    "    num_train = int(total_samples * train_ratio)\n",
    "    num_val = int(total_samples * validation_ratio)\n",
    "    num_test = total_samples - num_train - num_val  # Ensure all samples are used\n",
    "\n",
    "    # Shuffle indices for this class\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    # Split indices\n",
    "    train_indices = indices[:num_train]\n",
    "    val_indices = indices[num_train:num_train + num_val]\n",
    "    test_indices = indices[num_train + num_val:]\n",
    "\n",
    "    train_indices_per_class.append(train_indices)\n",
    "    val_indices_per_class.append(val_indices)\n",
    "    test_indices_per_class.append(test_indices)\n",
    "\n",
    "    print(f\"Class {class_idx}: Train={len(train_indices)}, Val={len(val_indices)}, Test={len(test_indices)}\")\n",
    "\n",
    "# Verify no overlap between train, val, and test sets\n",
    "for class_idx in range(label_dim):\n",
    "    train_set = set(train_indices_per_class[class_idx])\n",
    "    val_set = set(val_indices_per_class[class_idx])\n",
    "    test_set = set(test_indices_per_class[class_idx])\n",
    "    \n",
    "    assert len(train_set.intersection(val_set)) == 0, f\"Overlap between train and val for class {class_idx}\"\n",
    "    assert len(train_set.intersection(test_set)) == 0, f\"Overlap between train and test for class {class_idx}\"\n",
    "    assert len(val_set.intersection(test_set)) == 0, f\"Overlap between val and test for class {class_idx}\"\n",
    "\n",
    "# Create a new dataset instance with transformations for training\n",
    "dataset = VehicleTypeDataset(root_dir=dataset_path, transform=base_transform)\n",
    "\n",
    "# Create val and test datasets\n",
    "val_dataset = Subset(dataset, [idx for class_indices in val_indices_per_class for idx in class_indices])\n",
    "test_dataset = Subset(dataset, [idx for class_indices in test_indices_per_class for idx in class_indices])\n",
    "\n",
    "print(f\"Validation samples: {len(val_dataset)}, Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Step 2: Define augmentation transforms\n",
    "augmentation_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Horizontal flip with 50% probability\n",
    "    transforms.RandomRotation(degrees=10),  # Rotate by up to 10 degrees\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),  # Small translation and scaling\n",
    "])\n",
    "\n",
    "# Function to apply augmentation only to PIL images\n",
    "def augment_image_if_needed(image):\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = ToPILImage()(image)\n",
    "    image = augmentation_transform(image)\n",
    "    image = base_transform(image)  # Reapply base_transform after augmentation\n",
    "    return image\n",
    "\n",
    "# Function to augment the dataset to a target length\n",
    "def augment_dataset(dataset, target_length):\n",
    "    augmented_samples = []\n",
    "    current_length = len(dataset)\n",
    "    num_samples_to_augment = target_length - current_length\n",
    "    \n",
    "    if num_samples_to_augment <= 0:\n",
    "        indices = list(range(current_length))\n",
    "        random.shuffle(indices)\n",
    "        indices = indices[:target_length]  # Downsample to target length\n",
    "        return Subset(dataset, indices)\n",
    "    \n",
    "    for _ in range(num_samples_to_augment):\n",
    "        index = random.randint(0, current_length - 1)\n",
    "        image, label = dataset[index]\n",
    "        augmented_image = augment_image_if_needed(image)\n",
    "        augmented_samples.append((augmented_image, label))\n",
    "    \n",
    "    augmented_dataset = ConcatDataset([dataset, augmented_samples])\n",
    "    return augmented_dataset\n",
    "\n",
    "# Step 3: Create separate datasets for each class and split them as per the new distribution\n",
    "distinct_class_datasets = []\n",
    "num_classes = label_dim\n",
    "for class_idx in range(num_classes):\n",
    "    distinct_class_dataset = Subset(dataset, train_indices_per_class[class_idx])\n",
    "    distinct_class_datasets.append(distinct_class_dataset)\n",
    "\n",
    "# Verify the size of each class dataset\n",
    "for i, distinct_class_dataset in enumerate(distinct_class_datasets):\n",
    "    print(f\"Class {i} dataset size: {len(distinct_class_dataset)}\")\n",
    "\n",
    "# Function to split a dataset into two parts\n",
    "def split_dataset(dataset, split_ratio):\n",
    "    train_size = int(np.round(split_ratio * len(dataset)))\n",
    "    remaining_size = len(dataset) - train_size\n",
    "    train_dataset, remaining_dataset = torch.utils.data.random_split(dataset, [train_size, remaining_size])\n",
    "    return train_dataset, remaining_dataset\n",
    "\n",
    "# Split each class dataset into two halves (50/50)\n",
    "split_ratio = 0.5\n",
    "split_datasets = []\n",
    "train_class_datasets1 = []\n",
    "train_class_datasets2 = []\n",
    "\n",
    "for distinct_class_dataset in distinct_class_datasets:\n",
    "    train_class_dataset1, train_class_dataset2 = split_dataset(distinct_class_dataset, split_ratio)\n",
    "    split_datasets.append((train_class_dataset1, train_class_dataset2))\n",
    "    train_class_datasets1.append(train_class_dataset1)\n",
    "    train_class_datasets2.append(train_class_dataset2)\n",
    "\n",
    "for i, (train_class_dataset1, train_class_dataset2) in enumerate(split_datasets):\n",
    "    print(f\"Class {i}:\")\n",
    "    print(f\"  Number of samples in train_class_datasets1: {len(train_class_dataset1)}\")\n",
    "    print(f\"  Number of samples in train_class_datasets2: {len(train_class_dataset2)}\")\n",
    "\n",
    "# Further split train_class_datasets2 into 70% and 30% parts\n",
    "split_ratio = 0.7\n",
    "split_datasets2 = []\n",
    "train_class_datasets2_part1 = []\n",
    "train_class_datasets2_part2 = []\n",
    "\n",
    "for class_dataset in train_class_datasets2:\n",
    "    part1_dataset, part2_dataset = split_dataset(class_dataset, split_ratio)\n",
    "    split_datasets2.append((part1_dataset, part2_dataset))\n",
    "    train_class_datasets2_part1.append(part1_dataset)\n",
    "    train_class_datasets2_part2.append(part2_dataset)\n",
    "\n",
    "for i, (part1_dataset, part2_dataset) in enumerate(split_datasets2):\n",
    "    print(f\"Class {i}:\")\n",
    "    print(f\"  Number of samples in train_class_datasets2_part1 (70%): {len(part1_dataset)}\")\n",
    "    print(f\"  Number of samples in train_class_datasets2_part2 (30%): {len(part2_dataset)}\")\n",
    "\n",
    "# Target length for all datasets\n",
    "lengthiest_length = 500\n",
    "\n",
    "# Augment each dataset to have exactly 500 samples\n",
    "train_class_datasets1 = [augment_dataset(dataset, lengthiest_length) for dataset in train_class_datasets1]\n",
    "train_class_datasets2_augmented = [augment_dataset(dataset, lengthiest_length) for dataset in train_class_datasets2]\n",
    "train_class_datasets2_part1 = [augment_dataset(dataset, lengthiest_length) for dataset in train_class_datasets2_part1]\n",
    "train_class_datasets2_part2 = [augment_dataset(dataset, lengthiest_length) for dataset in train_class_datasets2_part2]\n",
    "\n",
    "# Print the sizes of the augmented datasets\n",
    "print(\"Sizes of augmented datasets:\")\n",
    "for i, dataset in enumerate(train_class_datasets1):\n",
    "    print(f\"  Length of augmented train_class_datasets1[{i}]: {len(dataset)}\")\n",
    "for i, dataset in enumerate(train_class_datasets2_augmented):\n",
    "    print(f\"  Length of augmented train_class_datasets2[{i}]: {len(dataset)}\")\n",
    "for i, dataset in enumerate(train_class_datasets2_part1):\n",
    "    print(f\"  Length of augmented train_class_datasets2_part1[{i}]: {len(dataset)}\")\n",
    "for i, dataset in enumerate(train_class_datasets2_part2):\n",
    "    print(f\"  Length of augmented train_class_datasets2_part2[{i}]: {len(dataset)}\")\n",
    "\n",
    "# Step 4: Distribute datasets to users as per the new scheme\n",
    "Num_users = 5\n",
    "user_data = []\n",
    "user_indices_sets = [[] for _ in range(Num_users)]\n",
    "\n",
    "# Helper function to collect indices from a dataset (handles Subset and ConcatDataset)\n",
    "def collect_indices(dataset):\n",
    "    indices = []\n",
    "    if isinstance(dataset, Subset):\n",
    "        indices.extend(dataset.indices)\n",
    "    elif isinstance(dataset, ConcatDataset):\n",
    "        for sub_dataset in dataset.datasets:\n",
    "            indices.extend(collect_indices(sub_dataset))\n",
    "    elif isinstance(dataset, list):\n",
    "        # If dataset is a list of tuples (image, label)\n",
    "        # We can't track indices directly; this case occurs with augmented samples\n",
    "        # We'll handle this by not adding these indices to user_indices_sets\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset type: {type(dataset)}\")\n",
    "    return indices\n",
    "\n",
    "# Distribute data to users using the augmented train_class_datasets2\n",
    "user_data.append(ConcatDataset([\n",
    "    train_class_datasets1[0],\n",
    "    train_class_datasets2_augmented[1],  # Use the augmented version\n",
    "    train_class_datasets2_part2[2],\n",
    "    train_class_datasets2_part2[3],\n",
    "    train_class_datasets2_part2[4]\n",
    "]))\n",
    "user_data.append(ConcatDataset([\n",
    "    train_class_datasets1[1],\n",
    "    train_class_datasets2_part1[2]\n",
    "]))\n",
    "user_data.append(ConcatDataset([\n",
    "    train_class_datasets1[2],\n",
    "    train_class_datasets2_part1[3]\n",
    "]))\n",
    "user_data.append(ConcatDataset([\n",
    "    train_class_datasets1[3],\n",
    "    train_class_datasets2_part1[4]\n",
    "]))\n",
    "user_data.append(ConcatDataset([\n",
    "    train_class_datasets1[4]\n",
    "]))\n",
    "\n",
    "# Collect indices for overlap checking (only original indices, not augmented ones)\n",
    "for user_idx, user_dataset in enumerate(user_data):\n",
    "    for sub_dataset in user_dataset.datasets:\n",
    "        if isinstance(sub_dataset, Subset):\n",
    "            user_indices_sets[user_idx].extend(sub_dataset.indices)\n",
    "        elif isinstance(sub_dataset, ConcatDataset):\n",
    "            user_indices_sets[user_idx].extend(collect_indices(sub_dataset))\n",
    "    print(f\"User {user_idx + 1}: Total samples = {len(user_dataset)}\")\n",
    "\n",
    "# Step 4.1: Compute class distribution before and after augmentation for verification\n",
    "samples_before_after = {user_idx: {class_idx: {'before': 0, 'after': 0} for class_idx in range(label_dim)} for user_idx in range(Num_users)}\n",
    "\n",
    "# Compute \"before\" counts (original samples, excluding augmented ones)\n",
    "for user_idx in range(Num_users):\n",
    "    user_dataset = user_data[user_idx]\n",
    "    for sub_dataset in user_dataset.datasets:\n",
    "        if isinstance(sub_dataset, Subset):\n",
    "            # Count original samples\n",
    "            for idx in sub_dataset.indices:\n",
    "                label = dataset_no_transform.labels[idx]\n",
    "                samples_before_after[user_idx][label]['before'] += 1\n",
    "\n",
    "# Compute \"after\" counts (total samples, including augmented ones)\n",
    "class_counts_per_user = []\n",
    "for user_idx in range(Num_users):\n",
    "    user_dataset = user_data[user_idx]\n",
    "    class_counts = [0] * label_dim\n",
    "    for idx in range(len(user_dataset)):\n",
    "        _, label = user_dataset[idx]\n",
    "        class_counts[label] += 1\n",
    "        samples_before_after[user_idx][label]['after'] = class_counts[label]\n",
    "    class_counts_per_user.append(class_counts)\n",
    "    print(f\"User {user_idx + 1} Class Distribution (After Augmentation): {class_counts}\")\n",
    "    total_samples = len(user_dataset)\n",
    "    class_percentages = [count / total_samples * 100 if total_samples > 0 else 0 for count in class_counts]\n",
    "    print(f\"User {user_idx + 1} Class Percentages (After Augmentation): {[f'{p:.2f}%' for p in class_percentages]}\")\n",
    "\n",
    "# Step 4.2: Plot histograms for each class showing sample distribution across users (before augmentation)\n",
    "print(\"\\n=== Plotting Histograms for Each Class (Before Augmentation) ===\")\n",
    "for class_idx in range(label_dim):\n",
    "    sample_counts = [samples_before_after[user_idx][class_idx]['before'] for user_idx in range(Num_users)]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(range(Num_users), sample_counts, align='center', alpha=0.7)\n",
    "    plt.xlabel('User Index')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.title(f\"Class {class_idx} ({dataset_no_transform.class_names[class_idx]}) Sample Distribution Across Users (Before Augmentation)\")\n",
    "    plt.xticks(range(Num_users), [f\"User {i+1}\" for i in range(Num_users)])\n",
    "    plt.grid(True, axis='y')\n",
    "    histogram_path = os.path.join(output_dir, f\"class_{class_idx}_sample_distribution_histogram_before_augmentation.png\")\n",
    "    plt.savefig(histogram_path)\n",
    "    plt.show()\n",
    "    print(f\"Saved histogram for Class {class_idx} to {histogram_path}\")\n",
    "\n",
    "# Print samples before and after augmentation for each user\n",
    "print(\"\\n=== Samples Before and After Augmentation ===\")\n",
    "for user_idx in range(Num_users):\n",
    "    print(f\"\\nUser {user_idx + 1}:\")\n",
    "    for class_idx in range(label_dim):\n",
    "        before = samples_before_after[user_idx][class_idx]['before']\n",
    "        after = samples_before_after[user_idx][class_idx]['after']\n",
    "        print(f\"Class {class_idx} ({dataset_no_transform.class_names[class_idx]}): Before={before}, After={after}\")\n",
    "\n",
    "# Step 4.3: Verify non-overlapping samples across users (based on original indices)\n",
    "print(\"\\n=== Verifying Data Distribution Across Users ===\")\n",
    "for i in range(Num_users):\n",
    "    for j in range(i + 1, Num_users):\n",
    "        overlap = set(user_indices_sets[i]).intersection(set(user_indices_sets[j]))\n",
    "        print(f\"Overlap between User {i+1} and User {j+1}: {len(overlap)} samples\")\n",
    "        if len(overlap) > 0:\n",
    "            print(f\"Overlapping indices: {overlap}\")\n",
    "\n",
    "# Step 5: Visualize 5 images per class for each user in a 5x5 grid\n",
    "display_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128), interpolation=transforms.InterpolationMode.LANCZOS),  # High-quality resizing\n",
    "    transforms.RandomAdjustSharpness(sharpness_factor=2.0, p=1.0),  # Apply sharpening\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "vis_dataset = VehicleTypeDataset(root_dir=dataset_path, transform=display_transform)\n",
    "\n",
    "for user_idx in range(Num_users):\n",
    "    print(f\"\\nCollecting images for User {user_idx + 1}...\")\n",
    "    user_indices = user_indices_sets[user_idx]\n",
    "\n",
    "    # Collect indices for each class (up to 5 images per class)\n",
    "    images_per_class = {class_idx: [] for class_idx in range(label_dim)}\n",
    "    for idx in user_indices:\n",
    "        label = dataset_no_transform.labels[idx]\n",
    "        if len(images_per_class[label]) < 5:\n",
    "            images_per_class[label].append(idx)\n",
    "\n",
    "    for class_idx in range(label_dim):\n",
    "        num_images = len(images_per_class[class_idx])\n",
    "        print(f\"User {user_idx + 1}, Class {class_idx} ({dataset_no_transform.class_names[class_idx]}): {num_images} images collected\")\n",
    "        if num_images < 5:\n",
    "            print(f\"Warning: User {user_idx + 1}, Class {class_idx} has only {num_images} images (less than 5).\")\n",
    "\n",
    "    # Plot images in a 5x5 grid (5 classes, 5 images each)\n",
    "    fig, axes = plt.subplots(5, 5, figsize=(12, 12))  # 5x5 grid\n",
    "    fig.suptitle(f\"User {user_idx + 1}: 5 Images Per Class (5x5 Grid, Non-IID)\", fontsize=16)\n",
    "\n",
    "    for row in range(5):  # 5 rows for 5 classes\n",
    "        class_idx = row  # Each row corresponds to a class\n",
    "        class_name = dataset_no_transform.class_names[class_idx]\n",
    "        for col in range(5):  # 5 columns for 5 images per class\n",
    "            ax = axes[row, col]\n",
    "            if col < len(images_per_class[class_idx]):\n",
    "                vis_idx = images_per_class[class_idx][col]\n",
    "                img, _ = vis_dataset[vis_idx]  # Load the image with display_transform\n",
    "                img = img.permute(1, 2, 0).numpy()  # Convert to HWC for display\n",
    "                img = np.clip(img, 0, 1)  # Clip to [0, 1]\n",
    "                ax.imshow(img)\n",
    "                if col == 0:  # Show class name on the first image of each row\n",
    "                    ax.set_ylabel(class_name, rotation=90, labelpad=10, fontsize=10)\n",
    "            else:\n",
    "                ax.text(0.5, 0.5, 'No Image', ha='center', va='center', fontsize=8)\n",
    "            ax.axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    image_save_path = os.path.join(output_dir, f\"user_{user_idx + 1}_images_5x5_grid_non_iid.png\")\n",
    "    plt.savefig(image_save_path)\n",
    "    plt.show()  # Show the plot\n",
    "    print(f\"Saved 5x5 visualization for User {user_idx + 1} to {image_save_path}\")\n",
    "\n",
    "# ResNet50 Model for RGB\n",
    "class ResNet50Model(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet50Model, self).__init__()\n",
    "        # Load pre-trained ResNet50\n",
    "        self.resnet50 = models.resnet50(pretrained=True)\n",
    "        # Freeze all layers except the final fully connected layer (optional, can unfreeze for fine-tuning)\n",
    "        # for param in self.resnet50.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        # Modify the fully connected layer to match the number of classes\n",
    "        in_features = self.resnet50.fc.in_features\n",
    "        self.resnet50.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet50(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Federated Learning Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "global_model = CNN().to(device)\n",
    "local_models = [CNN().to(device) for _ in range(Num_users)]\n",
    "batch_size = 128\n",
    "num_local_epochs = 3\n",
    "num_rounds = 100\n",
    "\n",
    "communication_rounds = []\n",
    "global_accuracies = []\n",
    "global_losses = []\n",
    "\n",
    "for round in range(num_rounds):\n",
    "    round_start_time = time.time()\n",
    "    print(f\"\\nRound {round + 1}\")\n",
    "    \n",
    "    for user_idx in range(Num_users):\n",
    "        local_models[user_idx].load_state_dict(global_model.state_dict())\n",
    "        local_model = local_models[user_idx]\n",
    "        local_tr_data_loader = DataLoader(user_data[user_idx], batch_size=batch_size, shuffle=True)\n",
    "        local_val_data_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(local_model.parameters(), lr=0.001, momentum=0.9)\n",
    "        scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "        for epoch in range(num_local_epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            local_model.train()\n",
    "            running_loss = 0.0\n",
    "            correct_train = 0\n",
    "            total_train = 0\n",
    "            for data, target in local_tr_data_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = local_model(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = output.max(1)\n",
    "                total_train += target.size(0)\n",
    "                correct_train += predicted.eq(target).sum().item()\n",
    "\n",
    "            train_loss = running_loss / len(local_tr_data_loader) if len(local_tr_data_loader) > 0 else float('inf')\n",
    "            train_accuracy = 100 * correct_train / total_train if total_train > 0 else 0.0\n",
    "\n",
    "            local_model.eval()\n",
    "            val_loss = 0.0\n",
    "            correct_val = 0\n",
    "            total_val = 0\n",
    "            with torch.no_grad():\n",
    "                for data, target in local_val_data_loader:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    output = local_model(data)\n",
    "                    val_loss += criterion(output, target).item()\n",
    "                    _, predicted = output.max(1)\n",
    "                    total_val += target.size(0)\n",
    "                    correct_val += predicted.eq(target).sum().item()\n",
    "\n",
    "            val_loss /= len(local_val_data_loader) if len(local_val_data_loader) > 0 else 1\n",
    "            val_accuracy = 100 * correct_val / total_val if total_val > 0 else 0.0\n",
    "            \n",
    "            epoch_end_time = time.time()\n",
    "            epoch_duration = epoch_end_time - epoch_start_time\n",
    "            print(f\"User {user_idx + 1}, Epoch {epoch + 1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%, \"\n",
    "                  f\"Time Taken: {epoch_duration:.2f} seconds\")\n",
    "            scheduler.step()\n",
    "\n",
    "    # Federated Averaging\n",
    "    global_state_dict = global_model.state_dict()\n",
    "    for key in global_state_dict.keys():\n",
    "        local_state_dicts = [local_model.state_dict() for local_model in local_models]\n",
    "        global_state_dict[key] = torch.mean(torch.stack([d[key].float() for d in local_state_dicts]), dim=0)\n",
    "    global_model.load_state_dict(global_state_dict)\n",
    "\n",
    "    # Global Evaluation\n",
    "    global_model.eval()\n",
    "    global_loss = 0.0\n",
    "    global_accuracy = 0.0\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = global_model(data)\n",
    "            global_loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            global_accuracy += (predicted == labels).sum().item()\n",
    "\n",
    "    global_accuracy = 100 * global_accuracy / len(test_dataset) if len(test_dataset) > 0 else 0.0\n",
    "    global_loss /= len(test_loader) if len(test_loader) > 0 else 1\n",
    "    \n",
    "    round_end_time = time.time()\n",
    "    round_duration = round_end_time - round_start_time\n",
    "    print(f\"Round {round + 1}: Global Test Accuracy: {global_accuracy:.2f}%, Global Loss: {global_loss:.4f}, \"\n",
    "          f\"Time Taken: {round_duration:.2f} seconds\")\n",
    "    \n",
    "    communication_rounds.append(round + 1)\n",
    "    global_accuracies.append(global_accuracy)\n",
    "    global_losses.append(global_loss)\n",
    "\n",
    "# Save the global model\n",
    "model_save_path = os.path.join(output_dir, \"cnn2_128_500_rgb_sgd_global_model_non_iid_ANO_DIST.pth\")\n",
    "torch.save(global_model.state_dict(), model_save_path)\n",
    "print(f\"Global model saved to {model_save_path}\")\n",
    "\n",
    "# Save Results\n",
    "csv_file_path = os.path.join(output_dir, \"cnn2_128_500_vehicle_rgb_sgd_non_iid_ANO_DIST.csv\")\n",
    "accuracy_plot_file_path = os.path.join(output_dir, \"cnn2_128_500_vehicle_rgb_sgd_accuracy_non_iid_ANO_DIST.png\")\n",
    "loss_plot_file_path = os.path.join(output_dir, \"cnn2_128_500_vehicle_rgb_sgd_loss_non_iid_ANO_DIST.png\")\n",
    "\n",
    "with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Communication Rounds', 'Global Accuracies', 'Global Losses'])\n",
    "    writer.writerows(zip(communication_rounds, global_accuracies, global_losses))\n",
    "\n",
    "# Display and Save Global Accuracy Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(communication_rounds, global_accuracies, marker='s', linestyle='-', color='b', label='CNN2 128 500(AUG) RGB SGD Global Accuracies (Non-IID)')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Global Accuracies')\n",
    "plt.title('Global Accuracies vs. Communication Rounds (CNN2 128 RGB SGD, Non-IID)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig(accuracy_plot_file_path)\n",
    "plt.show()  # Show the accuracy plot\n",
    "print(f\"Saved accuracy plot to {accuracy_plot_file_path}\")\n",
    "\n",
    "# Display and Save Global Loss Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(communication_rounds, global_losses, marker='o', linestyle='--', color='r', label='CNN2 128 500(AUG) RGB SGD Global Losses (Non-IID)')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Global Losses')\n",
    "plt.title('Global Losses vs. Communication Rounds (CNN2 128 RGB SGD, Non-IID)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig(loss_plot_file_path)\n",
    "plt.show()  # Show the loss plot\n",
    "print(f\"Saved loss plot to {loss_plot_file_path}\")\n",
    "\n",
    "# Plot histogram for User 1's class distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(range(label_dim), weights=class_counts_per_user[0], bins=range(label_dim + 1), align='left', rwidth=0.8)\n",
    "plt.xlabel('Class Index')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title(\"User 1 Class Distribution Histogram (Non-IID)\")\n",
    "plt.xticks(range(label_dim), dataset_no_transform.class_names, rotation=45)\n",
    "plt.grid(True)\n",
    "histogram_path = os.path.join(output_dir, \"user_1_class_distribution_histogram.png\")\n",
    "plt.savefig(histogram_path)\n",
    "plt.show()\n",
    "print(f\"Saved User 1 class distribution histogram to {histogram_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
